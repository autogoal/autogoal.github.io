{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AutoGOAL \u00b6 Automatic Generation, Optimization And Artificial Learning AutoGOAL is a Python library for automatically finding the best way to solve a given task. It has been designed mainly for Automated Machine Learning (aka AutoML ) but it can be used in any scenario where you have several possible ways to solve a given task. Technically speaking, AutoGOAL is a framework for program synthesis, i.e., finding the best program to solve a given problem, provided that the user can describe the space of all possible programs. AutoGOAL provides a set of low-level components to define different spaces and efficiently search in them. In the specific context of machine learning, AutoGOAL also provides high-level components that can be used as a black-box in almost any type of problem and dataset format. \u2b50 Quickstart \u00b6 AutoGOAL is first and foremost a framework for Automated Machine Learning. As such, it comes pre-packaged with hundreds of low-level machine learning algorithms that can be automatically assembled into pipelines for different problems. The core of this functionality lies in the AutoML class. To illustrate the simplicity of its use we will load a dataset and run an automatic classifier in it. The following code will run for approximately 5 minutes on a classic dataset. from autogoal.datasets import cars from autogoal.kb import ( MatrixContinuousDense , Supervised , VectorCategorical ) from autogoal.ml import AutoML # Load dataset X , y = cars . load () # Instantiate AutoML and define input/output types automl = AutoML ( input = ( MatrixContinuousDense , Supervised [ VectorCategorical ]), output = VectorCategorical ) # Run the pipeline search process automl . fit ( X , y ) # Report the best pipeline print ( automl . best_pipeline_ ) print ( automl . best_score_ ) Sensible defaults are defined for each of the many parameters of AutoML . Make sure to read the documentation for more information. \u2699\ufe0f Installation \u00b6 The easiest way to get AutoGOAL up and running with all the dependencies is to pull the development Docker image, which is somewhat big: docker pull autogoal/autogoal Instructions for setting up Docker are available here . Once you have the development image downloaded, you can fire up a console and use AutoGOAL interactively. If you prefer to not use Docker, or you don't want all the dependencies, you can also install AutoGOAL directly with pip: pip install autogoal This will install the core library but you won't be able to use any of the underlying machine learning algorithms until you install the corresponding optional dependencies. You can install them all with: pip install autogoal [ contrib ] To fine-pick which dependencies you want, read the dependencies section . \u26a0\ufe0f NOTE : By installing through pip you will get the latest release version of AutoGOAL, while by installing through Docker, you will get the latest development version. The development version is mostly up-to-date with the main branch, hence it will probably contain more features, but also more bugs, than the release version. \ud83d\udcbb CLI \u00b6 You can use AutoGOAL directly from the CLI. To see options just type: autogoal Using the CLI you can train and use AutoML models, download datasets and inspect the contrib libraries without writing a single line of code. Read more in the CLI documentation . \ud83e\udd29 Demo \u00b6 An online demo app is available at autogoal.github.io/demo . This app showcases the main features of AutoGOAL in interactive case studies. To run the demo locally, simply type: docker run -p 8501:8501 autogoal/autogoal And navigate to localhost:8501 . \u2696\ufe0f API stability \u00b6 We make a conscious effort to maintain a consistent public API across versions, but the private API can change at any time. In general, everything you can import from autogoal without underscores is considered public. For example: # \"clean\" imports are part of the public API from autogoal import optimize from autogoal.ml import AutoML from autogoal.contrib.sklearn import find_classes # public members of public types as well automl = AutoML automl . fit ( ... ) # underscored imports are part of the private API from autogoal.ml._automl import ... from autogoal.contrib.sklearn._generated import ... # as well as private members of any type automl . _input_type ( ... ) These are our consistency rules: Major breaking changes are introduced between major version updates, e.g., x.0 and y.0 . These can be additions, removals, or modifications of any kind in any part of the API. Between minor version updates, e.g., 1.x and 1.y , you can expect to find new functionality, but anything you can use from the public API will still be there with a consistent semantic (save for bugfixes). Between micro version updates, e.g., 1.3.x and 1.3.y , the public API is frozen even for additions. The private API can be changed at all times. \u26a0\ufe0f While AutoGOAL is on public beta (versions 0.x ) the public API is considered unstable and thus everything can change. However, we try to keep breaking changes to a minimum. \ud83d\udcda Documentation \u00b6 This documentation is available online at autogoal.github.io . Check the following sections: User Guide : Step-by-step showcase of everything you need to know to use AuoGOAL. Examples : The best way to learn how to use AutoGOAL by practice. API : Details about the public API for AutoGOAL. The HTML version can be deployed offline by downloading the AutoGOAL Docker image and running: docker run -p 8000:8000 autogoal/autogoal mkdocs serve -a 0.0.0.0:8000 And navigating to localhost:8000 . \ud83d\udcc3 Publications \u00b6 If you use AutoGOAL in academic research, please cite the following paper: @article { estevez2020general , title = {General-purpose hierarchical optimisation of machine learning pipelines with grammatical evolution} , author = {Est{\\'e}vez-Velarde, Suilan and Guti{\\'e}rrez, Yoan and Almeida-Cruz, Yudivi{\\'a}n and Montoyo, Andr{\\'e}s} , journal = {Information Sciences} , year = {2020} , publisher = {Elsevier} , doi = {10.1016/j.ins.2020.07.035} } The technologies and theoretical results leading up to AutoGOAL have been presented at different venues: Optimizing Natural Language Processing Pipelines: Opinion Mining Case Study marks the inception of the idea of using evolutionary optimization with a probabilistic search space for pipeline optimization. AutoML Strategy Based on Grammatical Evolution: A Case Study about Knowledge Discovery from Text applied probabilistic grammatical evolution with a custom-made grammar in the context of entity recognition in medical text. General-purpose Hierarchical Optimisation of Machine Learning Pipelines with Grammatical Evolution presents a more uniform framework with different grammars in different problems, from tabular datasets to natural language processing. Solving Heterogeneous AutoML Problems with AutoGOAL is the first actual description of AutoGOAL as a framework, unifying the ideas presented in the previous papers. \ud83e\udd1d Contribution \u00b6 Code is licensed under MIT. Read the details in the collaboration section . This project follows the all-contributors specification. Any contribution will be given credit, from fixing typos, to reporting bugs, to implementing new core functionalities. Here are all the current contributions. \ud83d\ude4f Thanks! Suilan Estevez-Velarde \ud83d\udcbb \u26a0\ufe0f \ud83e\udd14 \ud83d\udcd6 Alejandro Piad \ud83d\udcbb \u26a0\ufe0f \ud83d\udcd6 Yudivi\u00e1n Almeida Cruz \ud83e\udd14 \ud83d\udcd6 ygutierrez \ud83e\udd14 \ud83d\udcd6 Ernesto Luis Estevanell Valladares \ud83d\udcbb \u26a0\ufe0f Alexander Gonzalez \ud83d\udcbb \u26a0\ufe0f Anshu Trivedi \ud83d\udcbb Alex Coto \ud83d\udcd6 Guillermo Blanco \ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 yacth \ud83d\udc1b \ud83d\udcbb Brandon Fergerson \ud83d\udc1b Aditya Nikhil \ud83d\udc1b","title":"Home"},{"location":"#autogoal","text":"Automatic Generation, Optimization And Artificial Learning AutoGOAL is a Python library for automatically finding the best way to solve a given task. It has been designed mainly for Automated Machine Learning (aka AutoML ) but it can be used in any scenario where you have several possible ways to solve a given task. Technically speaking, AutoGOAL is a framework for program synthesis, i.e., finding the best program to solve a given problem, provided that the user can describe the space of all possible programs. AutoGOAL provides a set of low-level components to define different spaces and efficiently search in them. In the specific context of machine learning, AutoGOAL also provides high-level components that can be used as a black-box in almost any type of problem and dataset format.","title":"AutoGOAL"},{"location":"#quickstart","text":"AutoGOAL is first and foremost a framework for Automated Machine Learning. As such, it comes pre-packaged with hundreds of low-level machine learning algorithms that can be automatically assembled into pipelines for different problems. The core of this functionality lies in the AutoML class. To illustrate the simplicity of its use we will load a dataset and run an automatic classifier in it. The following code will run for approximately 5 minutes on a classic dataset. from autogoal.datasets import cars from autogoal.kb import ( MatrixContinuousDense , Supervised , VectorCategorical ) from autogoal.ml import AutoML # Load dataset X , y = cars . load () # Instantiate AutoML and define input/output types automl = AutoML ( input = ( MatrixContinuousDense , Supervised [ VectorCategorical ]), output = VectorCategorical ) # Run the pipeline search process automl . fit ( X , y ) # Report the best pipeline print ( automl . best_pipeline_ ) print ( automl . best_score_ ) Sensible defaults are defined for each of the many parameters of AutoML . Make sure to read the documentation for more information.","title":"\u2b50 Quickstart"},{"location":"#installation","text":"The easiest way to get AutoGOAL up and running with all the dependencies is to pull the development Docker image, which is somewhat big: docker pull autogoal/autogoal Instructions for setting up Docker are available here . Once you have the development image downloaded, you can fire up a console and use AutoGOAL interactively. If you prefer to not use Docker, or you don't want all the dependencies, you can also install AutoGOAL directly with pip: pip install autogoal This will install the core library but you won't be able to use any of the underlying machine learning algorithms until you install the corresponding optional dependencies. You can install them all with: pip install autogoal [ contrib ] To fine-pick which dependencies you want, read the dependencies section . \u26a0\ufe0f NOTE : By installing through pip you will get the latest release version of AutoGOAL, while by installing through Docker, you will get the latest development version. The development version is mostly up-to-date with the main branch, hence it will probably contain more features, but also more bugs, than the release version.","title":"\u2699\ufe0f Installation"},{"location":"#cli","text":"You can use AutoGOAL directly from the CLI. To see options just type: autogoal Using the CLI you can train and use AutoML models, download datasets and inspect the contrib libraries without writing a single line of code. Read more in the CLI documentation .","title":"\ud83d\udcbb CLI"},{"location":"#demo","text":"An online demo app is available at autogoal.github.io/demo . This app showcases the main features of AutoGOAL in interactive case studies. To run the demo locally, simply type: docker run -p 8501:8501 autogoal/autogoal And navigate to localhost:8501 .","title":"\ud83e\udd29 Demo"},{"location":"#api-stability","text":"We make a conscious effort to maintain a consistent public API across versions, but the private API can change at any time. In general, everything you can import from autogoal without underscores is considered public. For example: # \"clean\" imports are part of the public API from autogoal import optimize from autogoal.ml import AutoML from autogoal.contrib.sklearn import find_classes # public members of public types as well automl = AutoML automl . fit ( ... ) # underscored imports are part of the private API from autogoal.ml._automl import ... from autogoal.contrib.sklearn._generated import ... # as well as private members of any type automl . _input_type ( ... ) These are our consistency rules: Major breaking changes are introduced between major version updates, e.g., x.0 and y.0 . These can be additions, removals, or modifications of any kind in any part of the API. Between minor version updates, e.g., 1.x and 1.y , you can expect to find new functionality, but anything you can use from the public API will still be there with a consistent semantic (save for bugfixes). Between micro version updates, e.g., 1.3.x and 1.3.y , the public API is frozen even for additions. The private API can be changed at all times. \u26a0\ufe0f While AutoGOAL is on public beta (versions 0.x ) the public API is considered unstable and thus everything can change. However, we try to keep breaking changes to a minimum.","title":"\u2696\ufe0f API stability"},{"location":"#documentation","text":"This documentation is available online at autogoal.github.io . Check the following sections: User Guide : Step-by-step showcase of everything you need to know to use AuoGOAL. Examples : The best way to learn how to use AutoGOAL by practice. API : Details about the public API for AutoGOAL. The HTML version can be deployed offline by downloading the AutoGOAL Docker image and running: docker run -p 8000:8000 autogoal/autogoal mkdocs serve -a 0.0.0.0:8000 And navigating to localhost:8000 .","title":"\ud83d\udcda Documentation"},{"location":"#publications","text":"If you use AutoGOAL in academic research, please cite the following paper: @article { estevez2020general , title = {General-purpose hierarchical optimisation of machine learning pipelines with grammatical evolution} , author = {Est{\\'e}vez-Velarde, Suilan and Guti{\\'e}rrez, Yoan and Almeida-Cruz, Yudivi{\\'a}n and Montoyo, Andr{\\'e}s} , journal = {Information Sciences} , year = {2020} , publisher = {Elsevier} , doi = {10.1016/j.ins.2020.07.035} } The technologies and theoretical results leading up to AutoGOAL have been presented at different venues: Optimizing Natural Language Processing Pipelines: Opinion Mining Case Study marks the inception of the idea of using evolutionary optimization with a probabilistic search space for pipeline optimization. AutoML Strategy Based on Grammatical Evolution: A Case Study about Knowledge Discovery from Text applied probabilistic grammatical evolution with a custom-made grammar in the context of entity recognition in medical text. General-purpose Hierarchical Optimisation of Machine Learning Pipelines with Grammatical Evolution presents a more uniform framework with different grammars in different problems, from tabular datasets to natural language processing. Solving Heterogeneous AutoML Problems with AutoGOAL is the first actual description of AutoGOAL as a framework, unifying the ideas presented in the previous papers.","title":"\ud83d\udcc3 Publications"},{"location":"#contribution","text":"Code is licensed under MIT. Read the details in the collaboration section . This project follows the all-contributors specification. Any contribution will be given credit, from fixing typos, to reporting bugs, to implementing new core functionalities. Here are all the current contributions. \ud83d\ude4f Thanks! Suilan Estevez-Velarde \ud83d\udcbb \u26a0\ufe0f \ud83e\udd14 \ud83d\udcd6 Alejandro Piad \ud83d\udcbb \u26a0\ufe0f \ud83d\udcd6 Yudivi\u00e1n Almeida Cruz \ud83e\udd14 \ud83d\udcd6 ygutierrez \ud83e\udd14 \ud83d\udcd6 Ernesto Luis Estevanell Valladares \ud83d\udcbb \u26a0\ufe0f Alexander Gonzalez \ud83d\udcbb \u26a0\ufe0f Anshu Trivedi \ud83d\udcbb Alex Coto \ud83d\udcd6 Guillermo Blanco \ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 yacth \ud83d\udc1b \ud83d\udcbb Brandon Fergerson \ud83d\udc1b Aditya Nikhil \ud83d\udc1b","title":"\ud83e\udd1d Contribution"},{"location":"cli-api/","text":"","title":"Cli api"},{"location":"cli/","text":"Command Line Interface \u00b6 AutoGOAL can be used directly from the CLI for some tasks. To see all available commands just run: autogoal With the CLI you can train and deploy AutoML models on any dataset, inspect and download datasets, and explore the contrib libraries. A quick overview is shown below. The full documentation of the CLI application is shown next.","title":"CLI"},{"location":"cli/#command-line-interface","text":"AutoGOAL can be used directly from the CLI for some tasks. To see all available commands just run: autogoal With the CLI you can train and deploy AutoML models on any dataset, inspect and download datasets, and explore the contrib libraries. A quick overview is shown below. The full documentation of the CLI application is shown next.","title":"Command Line Interface"},{"location":"contributing/","text":"Collaboration \u00b6 This project uses a novel methodology for development, in which you only need Docker installed . Fork the project, clone, and you'll find a dockerfile and a docker-compose.yml file in the project root. We provide a packaged testing environment in the form of a Docker image with all the development tools installed and configured, e.g., testing libraries, etc. There is also a makefile with all the necessary commands. The workflow is something like this: Fork and clone the project. If you have not already, consider installing Github CLI . Run make pull to get the development image or make docker if you prefer to build it from scratch (somewhat slow). Develop: If you are using Visual Studio Code with Remote Extensions (recommended) when you open the project it will recommend you to launch the remote container. This is the most comfortable environment to develop for AutoGOAL since everything is preconfigured. Otherwise, pick your poison (ehem, editor) and edit. Run make shell to open a local shell inside the development container at any moment. Run make test-core inside the development container and make sure everything passes. You can also run make test-full to run the full set of tests. This is very slow and only necessary when making a new release. You can also run make test-ci outside the development container. This will build a slim container with just the core dependencies and run the core tests. This is the most similar testing environment to the Github CI, so make sure this works as well. Remember to add new tests if necessary. Run make format to reformat all code after all tests pass. If all worked, push to your own fork and open a pull-request. Here is a quick visual summary. This project uses poetry for package management. If you need to install new dependencies, run make shell and then poetry ... inside the dockerized environment. Finally, don't forget to poetry lock and commit the changes to pyproject.toml and poetry.lock files. License \u00b6 License is MIT, so you know the drill: fork, develop, test, pull request, rinse and repeat. MIT License Copyright (c) 2019-2020 - AutoGOAL contributors Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Contributing"},{"location":"contributing/#collaboration","text":"This project uses a novel methodology for development, in which you only need Docker installed . Fork the project, clone, and you'll find a dockerfile and a docker-compose.yml file in the project root. We provide a packaged testing environment in the form of a Docker image with all the development tools installed and configured, e.g., testing libraries, etc. There is also a makefile with all the necessary commands. The workflow is something like this: Fork and clone the project. If you have not already, consider installing Github CLI . Run make pull to get the development image or make docker if you prefer to build it from scratch (somewhat slow). Develop: If you are using Visual Studio Code with Remote Extensions (recommended) when you open the project it will recommend you to launch the remote container. This is the most comfortable environment to develop for AutoGOAL since everything is preconfigured. Otherwise, pick your poison (ehem, editor) and edit. Run make shell to open a local shell inside the development container at any moment. Run make test-core inside the development container and make sure everything passes. You can also run make test-full to run the full set of tests. This is very slow and only necessary when making a new release. You can also run make test-ci outside the development container. This will build a slim container with just the core dependencies and run the core tests. This is the most similar testing environment to the Github CI, so make sure this works as well. Remember to add new tests if necessary. Run make format to reformat all code after all tests pass. If all worked, push to your own fork and open a pull-request. Here is a quick visual summary. This project uses poetry for package management. If you need to install new dependencies, run make shell and then poetry ... inside the dockerized environment. Finally, don't forget to poetry lock and commit the changes to pyproject.toml and poetry.lock files.","title":"Collaboration"},{"location":"contributing/#license","text":"License is MIT, so you know the drill: fork, develop, test, pull request, rinse and repeat. MIT License Copyright (c) 2019-2020 - AutoGOAL contributors Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"dependencies/","text":"Mandatory and optional dependencies \u00b6 Note The recommended way to use AutoGOAL is by downloading the Docker development image. Installing through pip is only recommended if you do not intend to use the machine learning dependencies. This section is not relevant if you downloaded the Docker image of AutoGOAL. AutoGOAL itself does not depends on any third-party machine learning or optimization framework. The only real mandatory dependencies are: black = \"^19.10b0\" enlighten = \"^1.4.0\" networkx = \"^2.4\" numpy = \"^1.19.2\" pandas = \"^1.1.3\" psutil = \"^5.6.7\" pydot = \"^1.4.1\" pyyaml = \"^5.2\" scipy = \"^1.5.2\" termcolor = \"^1.1.0\" toml = \"^0.10.0\" tqdm = \"^4.50.2\" typer = \"^0.3.2\" rich = \"^8.0.0\" If you simply install with: pip install autogoal Then you will be able to run almost none of the examples, since most of them use external dependencies such as keras or sklearn . If you want to install all dependencies, use: pip install autogoal [ contrib ] Currently, first level optional dependencies include: gensim = \"^3.8.1\" jupyterlab = \"^1.2.4\" keras = \"^2.3.1\" nltk = \"^3.4.5\" nx_altair = \"^0.1.4\" python-telegram-bot = \"^12.4.2\" scikit-learn = \"^0.22\" seqlearn = \"^0.2\" sklearn_crfsuite = \"^0.3.6\" spacy = \"^2.2.3\" streamlit = \"^0.59.0\" transformers = \"^2.3.0\" wikipedia = \"^1.4.0\" You can also hand-pick which of these dependencies to install. It depends on the use you want to make of AutoGOAL. What about development dependencies? \u00b6 If you want to develop for the project then you will need the development dependencies. The recommended way to do this is to use Docker and use our development image . Why not include all the dependencies? \u00b6 AutoGOAL itself does not depend on sklearn or keras , for example, and it's not necessary to have either of these frameworks to make use of AutoGOAL. You might have a completely different problem setup, using totally new frameworks, or even your own classes. Hence, it makes no sense to force users of AutoGOAL to carry with all these heavy dependencies. Likewise, you may want a different version of keras , or even one that integrates with pytorch instead of tensorflow . AutoGOAL is agnostic to the underlying classes you use to actually build pipelines. Finally, you can use AutoGOAL to optimize almost everything, not just machine learning pipelines. You can, for example, optimize the configuration parameters for a scrapy crawler, or automatically find the regular expression that best matches some patterns. These are just completely random examples to illustrate that you can use AutoGOAL in scenarios other than AutoML. Whenever you can describe the set of possible solutions to a problem as a grammar, AutoGOAL comes to your help. Then why are there optional dependencies? \u00b6 Since AutoGOAL requires annotations and a specific use of its API to perform its magic, we provide pre-defined wrappers for sklearn , keras and nltk , to ease the development in the main use case of AutoGOAL, which is AutoML. We, the developers, are ourselves researchers in the AutoML area, and as such, we use AutoGOAL for this purpose. Anytime we need a new framework in our experiments, we add the corresponding wrappers to AutoGOAL to help the next generation of machine learning researchers and practicioners. Will missing optional dependencies bite me? \u00b6 They should not. All the code inside autogoal , except for autogoal.contrib is independent of any machine learning framework. The autogoal.contrib namespace contains all the code that depends on third-party libraries, and it's composed mostly of suitably annotated wrappers for these frameworks. For example, in autogoal.contrib.keras you will find utilities for automatically creating keras-based neural networks. When you import any of the modules in autogoal.contrib.* , the first thing we do is attempt to import the corresponding dependencies and provide helpful error messages otherwise. For example, if you don't have keras installed and attempt to use: from autogoal.contrib.keras import KerasClassifier You will get the following error: ( ! ) Code in ` autogoal.contrib.keras ` requires ` keras ` installed. ( ! ) Run ` pip install -U autogoal [ keras ] ` to get it. Running pip install autogoal[keras] installs all keras-dependent third-party dependencies. Likewise, you can pip install autogoal[module] for every module in autogoal.contrib.* .","title":"Dependencies"},{"location":"dependencies/#mandatory-and-optional-dependencies","text":"Note The recommended way to use AutoGOAL is by downloading the Docker development image. Installing through pip is only recommended if you do not intend to use the machine learning dependencies. This section is not relevant if you downloaded the Docker image of AutoGOAL. AutoGOAL itself does not depends on any third-party machine learning or optimization framework. The only real mandatory dependencies are: black = \"^19.10b0\" enlighten = \"^1.4.0\" networkx = \"^2.4\" numpy = \"^1.19.2\" pandas = \"^1.1.3\" psutil = \"^5.6.7\" pydot = \"^1.4.1\" pyyaml = \"^5.2\" scipy = \"^1.5.2\" termcolor = \"^1.1.0\" toml = \"^0.10.0\" tqdm = \"^4.50.2\" typer = \"^0.3.2\" rich = \"^8.0.0\" If you simply install with: pip install autogoal Then you will be able to run almost none of the examples, since most of them use external dependencies such as keras or sklearn . If you want to install all dependencies, use: pip install autogoal [ contrib ] Currently, first level optional dependencies include: gensim = \"^3.8.1\" jupyterlab = \"^1.2.4\" keras = \"^2.3.1\" nltk = \"^3.4.5\" nx_altair = \"^0.1.4\" python-telegram-bot = \"^12.4.2\" scikit-learn = \"^0.22\" seqlearn = \"^0.2\" sklearn_crfsuite = \"^0.3.6\" spacy = \"^2.2.3\" streamlit = \"^0.59.0\" transformers = \"^2.3.0\" wikipedia = \"^1.4.0\" You can also hand-pick which of these dependencies to install. It depends on the use you want to make of AutoGOAL.","title":"Mandatory and optional dependencies"},{"location":"dependencies/#what-about-development-dependencies","text":"If you want to develop for the project then you will need the development dependencies. The recommended way to do this is to use Docker and use our development image .","title":"What about development dependencies?"},{"location":"dependencies/#why-not-include-all-the-dependencies","text":"AutoGOAL itself does not depend on sklearn or keras , for example, and it's not necessary to have either of these frameworks to make use of AutoGOAL. You might have a completely different problem setup, using totally new frameworks, or even your own classes. Hence, it makes no sense to force users of AutoGOAL to carry with all these heavy dependencies. Likewise, you may want a different version of keras , or even one that integrates with pytorch instead of tensorflow . AutoGOAL is agnostic to the underlying classes you use to actually build pipelines. Finally, you can use AutoGOAL to optimize almost everything, not just machine learning pipelines. You can, for example, optimize the configuration parameters for a scrapy crawler, or automatically find the regular expression that best matches some patterns. These are just completely random examples to illustrate that you can use AutoGOAL in scenarios other than AutoML. Whenever you can describe the set of possible solutions to a problem as a grammar, AutoGOAL comes to your help.","title":"Why not include all the dependencies?"},{"location":"dependencies/#then-why-are-there-optional-dependencies","text":"Since AutoGOAL requires annotations and a specific use of its API to perform its magic, we provide pre-defined wrappers for sklearn , keras and nltk , to ease the development in the main use case of AutoGOAL, which is AutoML. We, the developers, are ourselves researchers in the AutoML area, and as such, we use AutoGOAL for this purpose. Anytime we need a new framework in our experiments, we add the corresponding wrappers to AutoGOAL to help the next generation of machine learning researchers and practicioners.","title":"Then why are there optional dependencies?"},{"location":"dependencies/#will-missing-optional-dependencies-bite-me","text":"They should not. All the code inside autogoal , except for autogoal.contrib is independent of any machine learning framework. The autogoal.contrib namespace contains all the code that depends on third-party libraries, and it's composed mostly of suitably annotated wrappers for these frameworks. For example, in autogoal.contrib.keras you will find utilities for automatically creating keras-based neural networks. When you import any of the modules in autogoal.contrib.* , the first thing we do is attempt to import the corresponding dependencies and provide helpful error messages otherwise. For example, if you don't have keras installed and attempt to use: from autogoal.contrib.keras import KerasClassifier You will get the following error: ( ! ) Code in ` autogoal.contrib.keras ` requires ` keras ` installed. ( ! ) Run ` pip install -U autogoal [ keras ] ` to get it. Running pip install autogoal[keras] installs all keras-dependent third-party dependencies. Likewise, you can pip install autogoal[module] for every module in autogoal.contrib.* .","title":"Will missing optional dependencies bite me?"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 When executing Visual Studio Code with Remote Container you get the error \"service autogoal not found\". \u00b6 Verify that your docker installation is through the package manager (example: apt). We have noticed that installing docker via snap causes this error. We have resolved the error by reinstalling Docker.","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#when-executing-visual-studio-code-with-remote-container-you-get-the-error-service-autogoal-not-found","text":"Verify that your docker installation is through the package manager (example: apt). We have noticed that installing docker via snap causes this error. We have resolved the error by reinstalling Docker.","title":"When executing Visual Studio Code with Remote Container you get the error \"service autogoal not found\"."},{"location":"api/autogoal.__init__/","text":"Note AutoGOAL is a Python framework for the automatic optimization, generation and learning of software pipelines. A software pipeline is defined, for the purpose of AutoGOAL, as any software component, whether a class hierarchy, a set of functions, or any combination thereof, that work together to solve a specific problem. With AutoGOAL you can define a pipeline in many different ways, such that certain parts of it are configurable or tunable, and then use search algorithms to find the best way to tune or configure it for a given problem. This is the top level module for AutoGOAL, it's what you we get when we import autogoal . By default, Python won't import submodules, hence, if we want autogoal.* to work we'll have to import all submodules here manually. These four submodules include the low-level components of AutoGOAL from which we can build all the core functionality. The grammar and sampling submodules allows us to define search spaces with arbitrary structure, and automatically create instances of different types of objects by sampling from them. from autogoal import grammar from autogoal import sampling The kb submodule allows us to define algorithms based on input and output types that and combine them automatically intro pipeline graphs. from autogoal import kb The search submodule contains search strategies to optimize in different hyper-parameter spaces. from autogoal import search With these low level structures, we can build the core functionality of AutoGOAL, the AutoML algorithm. The ml submodule contains the definition of the AutoML class and related functionality. from autogoal import ml The contrib submodule contains all the juicy stuff: machine learning algorithms from several external dependencies. from autogoal import contrib The datasets submodule contains several example datasets that can be used to showcase AutoGOAL. from autogoal import datasets These modules contain additional utilities. from autogoal import utils from autogoal import logging Finally, we import the top-level optimize function. from autogoal.utils._helpers import optimize Before leaving, let's setup logging at the warning level by default. logging . setup ( level = \"WARNING\" )","title":"autogoal"},{"location":"api/autogoal.__main__/","text":"import collections import inspect import logging from os import stat from pathlib import Path from typing import List import pandas as pd import typer from rich.console import Console from rich.logging import RichHandler from rich.table import Table from autogoal.contrib import ( find_classes , status , ContribStatus , download as download_contrib , ) from autogoal.kb import VectorCategorical from autogoal.ml import AutoML from autogoal.search import RichLogger from autogoal.utils import Gb , Min from autogoal.datasets import datapath , get_datasets_list , download , dummy import autogoal.logging autogoal . logging . setup ( \"WARNING\" ) logger = autogoal . logging . logger () console = Console () app = typer . Typer ( name = \"autogoal\" ) contrib_app = typer . Typer ( name = \"contrib\" ) automl_app = typer . Typer ( name = \"ml\" ) data_app = typer . Typer ( name = \"data\" ) app . add_typer ( contrib_app ) app . add_typer ( automl_app ) app . add_typer ( data_app ) @app . callback () def main (): Note \ud83e\udd29 Manage AutoGOAL directly from the CLI. @app . command () def demo (): Note \ud83c\udf1f Launch streamlit demo. try : from streamlit.bootstrap import run run ( Path ( __file__ ) . parent / \"contrib\" / \"streamlit\" / \"demo.py\" , \"\" , \"\" ) except ImportError : console . print ( \"(!) Too run the demo you need streamlit installed.\" ) console . print ( \"(!) Fix it by running `pip install autogoal[streamlit]`.\" ) @contrib_app . callback () def contrib_main (): Note \ud83d\udd0d Inspect contrib libraries and algorithms. @contrib_app . command ( \"list\" ) def contrib_list ( verbose : bool = False , include : str = None , exclude : str = None , input : str = None , output : str = None , ): Note \u2699\ufe0f List all currently available contrib algorithms. classes = find_classes ( include = include , exclude = exclude , input = input , output = output ) classes_by_contrib = collections . defaultdict ( list ) max_cls_name_length = 0 for cls in classes : max_cls_name_length = max ( max_cls_name_length , len ( cls . __name__ )) classes_by_contrib [ str ( cls ) . split ( \".\" )[ 2 ]] . append ( cls ) typer . echo ( f \"\u2699\ufe0f Found a total of { len ( classes ) } matching algorithms.\" , color = \"blue\" ) for contrib , clss in classes_by_contrib . items (): typer . echo ( f \"\ud83d\udee0\ufe0f { contrib } : { len ( clss ) } algorithms.\" ) if verbose : for cls in clss : sig = inspect . signature ( cls . run ) typer . echo ( f \" \ud83d\udd39 { cls . __name__ . ljust ( max_cls_name_length ) } : { sig . parameters [ 'input' ] . annotation } -> { sig . return_annotation } \" ) @contrib_app . command ( \"status\" ) def contrib_status (): Note \u2714\ufe0f Shows the status of all contrib libraries. table = Table ( \"\ud83d\udee0\ufe0f Contrib\" , \"\u2714\ufe0f Status\" ) statuses = { ContribStatus . RequiresDependency : \"\ud83d\udd34 Required dependency\" , ContribStatus . RequiresDownload : \"\ud83d\udd34 Requires download\" , ContribStatus . Ready : \"\ud83d\udfe2 Ready\" , } for key , value in status () . items (): table . add_row ( key , statuses [ value ]) console . print ( table ) @contrib_app . command ( \"download\" ) def contrib_download ( contrib = typer . Argument ( ... , help = \"Name of the contrib, e.g., `sklearn` or `nltk`, or `all`.\" ) ): Note \ud83d\udcbe Download necessary contrib files. if status ()[ f \"autogoal.contrib. { contrib } \" ] == ContribStatus . Ready : console . print ( f \"\u2705 Nothing to download for contrib ` { contrib } `.\" ) elif download_contrib ( contrib ): console . print ( f \"\u2705 Succesfully downloaded files for contrib ` { contrib } `.\" ) else : console . print ( f \"\u274c Cannot download files for contrib ` { contrib } `.\" ) @automl_app . callback () def automl_callback (): Note \ud83e\udd16 Fit and predict with an AutoML model. def _load_dataset ( format , input , ignore ): if format is None : if input . suffix == \".csv\" : format = \"csv\" if input . suffix == \".json\" : format = \"json\" if format == \"csv\" : dataset = pd . read_csv ( input ) elif format == \"json\" : dataset = pd . read_json ( input ) else : raise ValueError ( \"Input format not recognized. Must be either CSV or JSON.\" ) if ignore : columns_to_ignore = [ dataset . columns [ i ] for i in ignore ] dataset = dataset . drop ( columns = columns_to_ignore ) return dataset @automl_app . command ( \"fit\" ) def automl_fit ( input : Path , output : Path = Path ( \"automl.bin\" ), target : str = None , ignore_cols : List [ int ] = typer . Option ([]), evaluation_timeout : int = 5 * Min , memory_limit : int = 4 * Gb , search_timeout : int = 60 * 60 , pop_size : int = 20 , iterations : int = 100 , random_state : int = None , format : str = None , ): Note \ud83c\udfc3 Train an AutoML instance on a dataset. try : dataset = _load_dataset ( format , input , ignore_cols ) except ValueError as e : logger . error ( f \"\u26a0\ufe0f Error: { str ( e ) } \" ) return if target is None : target = dataset . columns [ - 1 ] columns = [ c for c in dataset . columns if c != target ] X = dataset [ columns ] . values y = dataset [ target ] . values automl = AutoML ( output = VectorCategorical (), search_kwargs = dict ( evaluation_timeout = evaluation_timeout , memory_limit = memory_limit , search_timeout = search_timeout , pop_size = pop_size , ), random_state = random_state , search_iterations = iterations , ) console . print ( f \"\ud83c\udfc3 Training on { len ( dataset ) } items.\" ) automl . fit ( X , y , logger = RichLogger ()) with output . open ( \"wb\" ) as fp : automl . save ( fp ) console . print ( f \"\ud83d\udcbe Saving model to [green] { output . absolute () } [/].\" ) @automl_app . command ( \"predict\" ) def automl_predict ( input : Path , output : Path = Path ( \"output.csv\" ), model : Path = Path ( \"automl.bin\" ), ignore_cols : List [ int ] = typer . Option ([]), format : str = None , ): Note \ud83d\udd2e Predict with a previously trained AutoML instance. try : dataset = _load_dataset ( format , input , ignore_cols ) except ValueError as e : logger . error ( f \"\u26a0\ufe0f Error: { str ( e ) } \" ) return try : with model . open ( \"rb\" ) as fp : automl = AutoML . load ( fp ) except TypeError as e : logger . error ( f \"\u26a0\ufe0f Error: { str ( e ) } \" ) return console . print ( f \"\ud83d\udd2e Predicting { len ( dataset ) } items with the pipeline:\" ) console . print ( repr ( automl . best_pipeline_ )) X = dataset . values y = automl . predict ( X ) with output . open ( \"wt\" ) as fp : df = pd . DataFrame ( y , columns = [ \"y\" ]) df . to_csv ( fp ) console . print ( f \"\ud83d\udcbe Predictions saved to [blue] { output . absolute () } [/]\" ) @automl_app . command ( \"inspect\" ) def automl_inspect ( model : Path = Path ( \"automl.bin\" )): Note \ud83d\udd0d Inspect a trained AutoML model. with model . open ( \"rb\" ) as fp : automl = AutoML . load ( fp ) console . print ( f \"\ud83d\udd0d Inspecting AutoML model: [green] { model . absolute () } [/]\" ) console . print ( f \"\u2b50 Best pipeline (score= { automl . best_score_ : 0.3f } ):\" ) console . print ( repr ( automl . best_pipeline_ )) @data_app . callback () def data_callback (): Note \ud83d\udcda Download, inspect, and generate training data. @data_app . command ( \"list\" ) def data_list (): Note \ud83d\udd0d List the available datasets. datasets = get_datasets_list () table = Table ( \"\ud83d\udcda Dataset\" , \"\ud83d\udcbe\" , \"\ud83d\udd17 URL\" ) for item , url in sorted ( datasets . items (), key = lambda t : t [ 0 ]): path = datapath ( item ) if path . exists (): table . add_row ( item , \"\u2714\ufe0f\" , url ) else : table . add_row ( item , \"\" , url ) console . print ( table ) @data_app . command ( \"download\" ) def data_download ( datasets : List [ str ] = typer . Argument ( ... , help = \"Name of one or more specific datasets to download, or 'all'.\" ) ): Note \u23ec Download a dataset. Pass a name to directly download that dataset. Otherwise, this command will show an interactive menu. if \"all\" in datasets : datasets = get_datasets_list () . keys () for dataset in datasets : download ( dataset ) @data_app . command ( \"gen\" ) def data_generate (): Note \ud83c\udfb2 Generate a random dataset. if __name__ == \"__main__\" : try : app ( prog_name = \"autogoal\" ) except Exception as e : console . print ( f '\u26a0\ufe0f The command failed with message: \\n \" { str ( e ) } \".' ) if console . input ( \"\u2753 Do you want to inspect the traceback? \\[y/N] \" ) == \"y\" : logger . exception ( \"Check the traceback below.\" )","title":"cli"},{"location":"api/autogoal.contrib.__init__/","text":"def find_classes ( include = None , exclude = None , modules = None , input = None , output = None ): import inspect import re result = [] if include : include = f \".*( { include } ).*\" else : include = r \".*\" if exclude : exclude = f \".*( { exclude } ).*\" if input : input = f \".*( { input } ).*\" if output : output = f \".*( { output } ).*\" if modules is None : modules = [] try : from autogoal.contrib import sklearn modules . append ( sklearn ) except ImportError as e : pass try : from autogoal.contrib import nltk modules . append ( nltk ) except ImportError as e : pass try : from autogoal.contrib import gensim modules . append ( gensim ) except ImportError as e : pass try : from autogoal.contrib import keras modules . append ( keras ) except ImportError as e : pass try : from autogoal.contrib import transformers modules . append ( transformers ) except ImportError as e : pass try : from autogoal.contrib import spacy modules . append ( spacy ) except ImportError as e : pass try : from autogoal.contrib import wikipedia modules . append ( wikipedia ) except ImportError as e : pass from autogoal.contrib import wrappers modules . append ( wrappers ) from autogoal.contrib import regex modules . append ( regex ) for module in modules : for _ , cls in inspect . getmembers ( module , inspect . isclass ): if not hasattr ( cls , \"run\" ): continue if cls . __name__ . startswith ( \"_\" ): continue if not re . match ( include , repr ( cls )): continue if exclude is not None and re . match ( exclude , repr ( cls )): continue if not cls . __module__ . startswith ( \"autogoal.contrib\" ): continue sig = inspect . signature ( cls . run ) if input and not re . match ( input , str ( sig . parameters [ \"input\" ] . annotation )): continue if output and not re . match ( output , str ( sig . return_annotation )): continue result . append ( cls ) return result import enum class ContribStatus ( enum . Enum ): RequiresDependency = enum . auto () RequiresDownload = enum . auto () Ready = enum . auto () def status (): status = {} modules = [] try : from autogoal.contrib import sklearn modules . append ( sklearn ) except ImportError as e : status [ \"autogoal.contrib.sklearn\" ] = ContribStatus . RequiresDependency try : from autogoal.contrib import nltk modules . append ( nltk ) except ImportError as e : status [ \"autogoal.contrib.nltk\" ] = ContribStatus . RequiresDependency try : from autogoal.contrib import gensim modules . append ( gensim ) except ImportError as e : status [ \"autogoal.contrib.gensim\" ] = ContribStatus . RequiresDependency try : from autogoal.contrib import keras modules . append ( keras ) except ImportError as e : status [ \"autogoal.contrib.keras\" ] = ContribStatus . RequiresDependency try : from autogoal.contrib import transformers modules . append ( transformers ) except ImportError as e : status [ \"autogoal.contrib.transformers\" ] = ContribStatus . RequiresDependency try : from autogoal.contrib import spacy modules . append ( spacy ) except ImportError as e : status [ \"autogoal.contrib.spacy\" ] = ContribStatus . RequiresDependency try : from autogoal.contrib import wikipedia modules . append ( wikipedia ) except ImportError as e : status [ \"autogoal.contrib.wikipedia\" ] = ContribStatus . RequiresDependency modules . sort ( key = lambda m : m . __name__ ) for module in modules : if hasattr ( module , \"status\" ): status [ module . __name__ ] = module . status () else : status [ module . __name__ ] = ContribStatus . Ready return status def download ( contrib : str ): modules = {} try : from autogoal.contrib import sklearn modules [ \"sklearn\" ] = sklearn except ImportError as e : pass try : from autogoal.contrib import nltk modules [ \"nltk\" ] = nltk except ImportError as e : pass try : from autogoal.contrib import gensim modules [ \"gensim\" ] = gensim except ImportError as e : pass try : from autogoal.contrib import keras modules [ \"keras\" ] = keras except ImportError as e : pass try : from autogoal.contrib import transformers modules [ \"transformers\" ] = transformers except ImportError as e : pass try : from autogoal.contrib import spacy modules [ \"spacy\" ] = spacy except ImportError as e : pass try : from autogoal.contrib import wikipedia modules [ \"wikipedia\" ] = wikipedia except ImportError as e : pass if contrib not in modules : raise ValueError ( f \"Contrib ` { contrib } ` cannot be imported.\" ) contrib = modules [ contrib ] if not hasattr ( contrib , \"download\" ): return False return contrib . download () __all__ = [ \"find_classes\" , \"status\" , \"download\" ]","title":"autogoal.contrib"},{"location":"api/autogoal.contrib.gensim.__init__/","text":"try : import gensim assert gensim. version == \"3.8.1\" except : print ( \"(!) Code in `autogoal.contrib.gensim` requires `gensim==3.8.1`.\" ) print ( \"(!) You can install it with `pip install autogoal[gensim]`.\" ) raise from ._base import ( Word2VecEmbedding , Word2VecEmbeddingSpanish , FastTextEmbeddingSpanishSUC , FastTextEmbeddingSpanishSWBC , GloveEmbeddingSpanishSWBC , )","title":"Autogoal.contrib.gensim.  init  "},{"location":"api/autogoal.contrib.gensim._base/","text":"from autogoal.kb import AlgorithmBase from pathlib import Path import requests import shutil import numpy as np import gensim.downloader as api from autogoal.kb import Word , VectorContinuous from autogoal.utils import CacheManager , nice_repr from autogoal.datasets import download_and_save from gensim.models import KeyedVectors from gensim.models.fasttext import FastText , FastTextKeyedVectors @nice_repr class Word2VecEmbedding ( AlgorithmBase ): Note \"\"\"This class transform a word in embedding vector using Word2Vec of gensim (using glove-twitter-25 ). Notes \u00b6 On the first use the model Word2Vec of gensim will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. def __init__ ( self ): self . _model = None @property def model ( self ) -> KeyedVectors : if self . _model is None : self . _model = api . load ( \"glove-twitter-25\" ) return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use Word2Vec of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input ) except : return np . zeros ( 25 ) @nice_repr class Word2VecEmbeddingSpanish ( AlgorithmBase ): Note \"\"\"This class transform a word in embedding vector using Word2Vec of gensim (using Spanish 3B Word2Vec ). Notes \u00b6 On the first use the model Word2Vec of gensim will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. def __init__ ( self ): self . _model = None def _load_model ( self ): url = \"https://zenodo.org/record/1410403/files/keyed_vectors.zip?download=1\" path = Path ( __file__ ) . parent / \"spanish-w2v.zip\" kv = Path ( __file__ ) . parent / \"complete.kv\" if download_and_save ( url , path ): shutil . unpack_archive ( str ( path ), str ( path . parent )) return KeyedVectors . load ( str ( kv ), mmap = \"r\" ) @property def model ( self ) -> KeyedVectors : if self . _model is None : self . _model = self . _load_model () return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use Word2Vec of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input . lower ()) except KeyError : return np . zeros ( 400 ) @nice_repr class FastTextEmbeddingSpanishSUC ( AlgorithmBase ): Note \"\"\"This class transform a word in embedding vector using FastText of gensim . Notes \u00b6 On the first use the model will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. Examples \u00b6 embedder = FastTextEmbeddingSpanishSUC() embedder.run(\"algoritmo\") def __init__ ( self ): self . _model = None def _load_model ( self ): url = ( \"https://zenodo.org/record/3234051/files/embeddings-l-model.bin?download=1\" ) path = Path ( __file__ ) . parent / \"fasttext-spanish-suc.bin\" download_and_save ( url , path ) return FastText . load_fasttext_format ( str ( path )) . wv @property def model ( self ) -> FastTextKeyedVectors : if self . _model is None : self . _model = self . _load_model () return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use FastText of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input . lower ()) except KeyError : return np . zeros ( 300 ) @nice_repr class FastTextEmbeddingSpanishSWBC ( AlgorithmBase ): Note \"\"\"This class transform a word in embedding vector using FastText of gensim . Notes \u00b6 On the first use the model will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. Examples \u00b6 embedder = FastTextEmbeddingSpanishSWBC() embedder.run(\"algoritmo\") def __init__ ( self ): self . _model = None def _load_model ( self ): url = \"http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.bin\" path = Path ( __file__ ) . parent / \"fasttext-spanish-swbc.bin\" download_and_save ( url , path ) return FastText . load_fasttext_format ( str ( path )) . wv @property def model ( self ) -> FastTextKeyedVectors : if self . _model is None : self . _model = self . _load_model () return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use FastText of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input . lower ()) except KeyError : return np . zeros ( 300 ) @nice_repr class GloveEmbeddingSpanishSWBC ( AlgorithmBase ): Note \"\"\"This class transform a word in embedding vector using Glove of gensim . Notes \u00b6 On the first use the model will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. Examples \u00b6 embedder = FastTextEmbeddingSpanishSWBC() embedder.run(\"algoritmo\") def __init__ ( self ): self . _model = None def _load_model ( self ): url = \"http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i25.bin\" path = Path ( __file__ ) . parent / \"glove-spanish-swbc.bin\" download_and_save ( url , path ) return FastText . load_fasttext_format ( str ( path )) . wv @property def model ( self ) -> FastTextKeyedVectors : if self . _model is None : self . _model = self . _load_model () return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use FastText of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input . lower ()) except KeyError : return np . zeros ( 300 )","title":"Autogoal.contrib.gensim. base"},{"location":"api/autogoal.contrib.gensim._base/#notes","text":"On the first use the model Word2Vec of gensim will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. def __init__ ( self ): self . _model = None @property def model ( self ) -> KeyedVectors : if self . _model is None : self . _model = api . load ( \"glove-twitter-25\" ) return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use Word2Vec of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input ) except : return np . zeros ( 25 ) @nice_repr class Word2VecEmbeddingSpanish ( AlgorithmBase ): Note \"\"\"This class transform a word in embedding vector using Word2Vec of gensim (using Spanish 3B Word2Vec ).","title":"Notes"},{"location":"api/autogoal.contrib.gensim._base/#notes_1","text":"On the first use the model Word2Vec of gensim will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. def __init__ ( self ): self . _model = None def _load_model ( self ): url = \"https://zenodo.org/record/1410403/files/keyed_vectors.zip?download=1\" path = Path ( __file__ ) . parent / \"spanish-w2v.zip\" kv = Path ( __file__ ) . parent / \"complete.kv\" if download_and_save ( url , path ): shutil . unpack_archive ( str ( path ), str ( path . parent )) return KeyedVectors . load ( str ( kv ), mmap = \"r\" ) @property def model ( self ) -> KeyedVectors : if self . _model is None : self . _model = self . _load_model () return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use Word2Vec of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input . lower ()) except KeyError : return np . zeros ( 400 ) @nice_repr class FastTextEmbeddingSpanishSUC ( AlgorithmBase ): Note \"\"\"This class transform a word in embedding vector using FastText of gensim .","title":"Notes"},{"location":"api/autogoal.contrib.gensim._base/#notes_2","text":"On the first use the model will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you.","title":"Notes"},{"location":"api/autogoal.contrib.gensim._base/#examples","text":"embedder = FastTextEmbeddingSpanishSUC() embedder.run(\"algoritmo\") def __init__ ( self ): self . _model = None def _load_model ( self ): url = ( \"https://zenodo.org/record/3234051/files/embeddings-l-model.bin?download=1\" ) path = Path ( __file__ ) . parent / \"fasttext-spanish-suc.bin\" download_and_save ( url , path ) return FastText . load_fasttext_format ( str ( path )) . wv @property def model ( self ) -> FastTextKeyedVectors : if self . _model is None : self . _model = self . _load_model () return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use FastText of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input . lower ()) except KeyError : return np . zeros ( 300 ) @nice_repr class FastTextEmbeddingSpanishSWBC ( AlgorithmBase ): Note \"\"\"This class transform a word in embedding vector using FastText of gensim .","title":"Examples"},{"location":"api/autogoal.contrib.gensim._base/#notes_3","text":"On the first use the model will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you.","title":"Notes"},{"location":"api/autogoal.contrib.gensim._base/#examples_1","text":"embedder = FastTextEmbeddingSpanishSWBC() embedder.run(\"algoritmo\") def __init__ ( self ): self . _model = None def _load_model ( self ): url = \"http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.bin\" path = Path ( __file__ ) . parent / \"fasttext-spanish-swbc.bin\" download_and_save ( url , path ) return FastText . load_fasttext_format ( str ( path )) . wv @property def model ( self ) -> FastTextKeyedVectors : if self . _model is None : self . _model = self . _load_model () return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use FastText of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input . lower ()) except KeyError : return np . zeros ( 300 ) @nice_repr class GloveEmbeddingSpanishSWBC ( AlgorithmBase ): Note \"\"\"This class transform a word in embedding vector using Glove of gensim .","title":"Examples"},{"location":"api/autogoal.contrib.gensim._base/#notes_4","text":"On the first use the model will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you.","title":"Notes"},{"location":"api/autogoal.contrib.gensim._base/#examples_2","text":"embedder = FastTextEmbeddingSpanishSWBC() embedder.run(\"algoritmo\") def __init__ ( self ): self . _model = None def _load_model ( self ): url = \"http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i25.bin\" path = Path ( __file__ ) . parent / \"glove-spanish-swbc.bin\" download_and_save ( url , path ) return FastText . load_fasttext_format ( str ( path )) . wv @property def model ( self ) -> FastTextKeyedVectors : if self . _model is None : self . _model = self . _load_model () return self . _model def run ( self , input : Word ) -> VectorContinuous : Note \"\"\"This method use FastText of gensim for tranform a word in embedding vector. try : return self . model . get_vector ( input . lower ()) except KeyError : return np . zeros ( 300 )","title":"Examples"},{"location":"api/autogoal.contrib.gensim._data/","text":"import os from pathlib import Path CONTRIB_NAME = \"gensim\" DATA_PATH = Path . home () / \".autogoal\" / \"contrib\" / CONTRIB_NAME / \"data\" ensure data path directory creation try : os . makedirs ( DATA_PATH ) except IOError as ex : directory already exists pass def load_data ( path , name ): use_data ( path ) try : import gensim import gensim.downloader as api api . load ( name ) except : pass def use_data ( path ): os . environ [ \"GENSIM_DATA_DIR\" ] = str ( path )","title":"Autogoal.contrib.gensim. data"},{"location":"api/autogoal.contrib.keras.__init__/","text":"import os os . environ [ \"TF_CPP_MIN_LOG_LEVEL\" ] = \"3\" try : from tensorflow import keras assert keras. version == \"2.3.1\" except : print ( \"(!) Code in `autogoal.contrib.keras` requires `keras==2.3.1`.\" ) print ( \"(!) You can install it with `pip install autogoal[keras]`.\" ) raise from ._base import ( KerasClassifier , KerasSequenceClassifier , KerasSequenceTagger , KerasImageClassifier , KerasImagePreprocessor , ) from ._grammars import build_grammar","title":"Autogoal.contrib.keras.  init  "},{"location":"api/autogoal.contrib.keras._base/","text":"from autogoal.kb import AlgorithmBase , Supervised from typing import Optional import numpy as np from tensorflow.keras.callbacks import EarlyStopping , TerminateOnNaN from tensorflow.keras.layers import Dense , Input , TimeDistributed , concatenate from tensorflow.keras.models import Model from tensorflow.keras.utils import to_categorical from tensorflow.keras.preprocessing.image import ( ImageDataGenerator as _ImageDataGenerator , ) from autogoal.contrib.keras._grammars import build_grammar , generate_grammar , Modules from autogoal.grammar import ( Graph , GraphGrammar , Sampler , BooleanValue , CategoricalValue , ContinuousValue , DiscreteValue , ) from autogoal.kb import ( VectorCategorical , Seq , MatrixContinuousDense , Label , Tensor3 , Tensor4 , ) from autogoal.utils import nice_repr import abc @nice_repr class KerasNeuralNetwork ( AlgorithmBase , metaclass = abc . ABCMeta ): def __init__ ( self , grammar : GraphGrammar , optimizer : CategoricalValue ( \"sgd\" , \"adam\" , \"rmsprop\" ), epochs = 10 , early_stop = 3 , validation_split = 0.1 , ** compile_kwargs , ): self . optimizer = optimizer self . _grammar = grammar self . _epochs = epochs self . _compile_kwargs = compile_kwargs self . _model : Optional [ Model ] = None self . _mode = \"train\" self . _graph = None self . _validation_split = validation_split self . _early_stop = early_stop def train ( self ): self . _mode = \"train\" def eval ( self ): self . _mode = \"eval\" def run ( self , X , y = None ): if self . _mode == \"train\" : self . fit ( X , y ) return y if self . _mode == \"eval\" : return self . predict ( X ) assert False , \"Invalid mode %s \" % self . _mode def __repr__ ( self ): nodes = len ( self . _graph . nodes ) if self . _graph is not None else None return f \" { self . __class__ . __name__ } (nodes= { nodes } , compile_kwargs= { self . _compile_kwargs } )\" def __nice_repr_hook__ ( self , names , values ): graph = [] if self . _graph is not None : for node , _ in self . _graph . build_order (): graph . append ( node ) names . append ( \"graph\" ) values . append ( graph ) @property def model ( self ): if self . _model is None : raise TypeError ( \"You need to call `fit` first to generate the model.\" ) return self . _model def sample ( self , sampler : Sampler = None , max_iterations = 100 ): if sampler is None : sampler = Sampler () self . _graph = self . _grammar . sample ( sampler = sampler , max_iterations = max_iterations ) return self def _build_nn ( self , graph : Graph , X , y ): input_x = self . _build_input ( X ) def build_model ( layer , _ , previous_layers ): if not previous_layers : return layer ( input_x ) if len ( previous_layers ) > 1 : incoming = concatenate ( previous_layers ) else : incoming = previous_layers [ 0 ] return layer ( incoming ) output_y = graph . apply ( build_model ) or [ input_x ] final_ouput = self . _build_output ( output_y , y ) if \"optimizer\" not in self . _compile_kwargs : self . _compile_kwargs [ \"optimizer\" ] = self . optimizer self . _model = Model ( inputs = input_x , outputs = final_ouput ) self . _model . compile ( ** self . _compile_kwargs ) @abc . abstractmethod def _build_input ( self , X ): pass def _build_output ( self , outputs , y ): return outputs def fit ( self , X , y , ** kwargs ): if self . _graph is None : raise TypeError ( \"You must call `sample` to generate the internal model.\" ) self . _build_nn ( self . _graph , X , y ) self.model.summary() self . _fit_model ( X , y , ** kwargs ) def _fit_model ( self , X , y , ** kwargs ): self . model . fit ( x = X , y = y , epochs = self . _epochs , callbacks = [ EarlyStopping ( patience = self . _early_stop , restore_best_weights = True ), TerminateOnNaN (), ], validation_split = self . _validation_split , verbose = 0 , ** kwargs , ) def predict ( self , X ): return self . model . predict ( X ) class KerasClassifier ( KerasNeuralNetwork ): def __init__ ( self , optimizer : CategoricalValue ( \"sgd\" , \"adam\" , \"rmsprop\" ), grammar = None , ** kwargs , ): self . _classes = None self . _num_classes = None super () . __init__ ( grammar = grammar or self . _build_grammar (), optimizer = optimizer , ** kwargs ) def _build_grammar ( self ): return build_grammar ( features = True ) def _build_input ( self , X ): return Input ( shape = ( X . shape [ 1 ],)) def _build_output_layer ( self , y ): self . _num_classes = y . shape [ 1 ] if \"loss\" not in self . _compile_kwargs : self . _compile_kwargs [ \"loss\" ] = \"categorical_crossentropy\" self . _compile_kwargs [ \"metrics\" ] = [ \"accuracy\" ] return Dense ( units = self . _num_classes , activation = \"softmax\" ) def _build_output ( self , outputs , y ): if len ( outputs ) > 1 : outputs = concatenate ( outputs ) else : outputs = outputs [ 0 ] return self . _build_output_layer ( y )( outputs ) def fit ( self , X , y ): self . _classes = { k : v for k , v in zip ( set ( y ), range ( len ( y )))} self . _inverse_classes = { v : k for k , v in self . _classes . items ()} y = [ self . _classes [ yi ] for yi in y ] y = to_categorical ( y ) return super ( KerasClassifier , self ) . fit ( X , y ) def predict ( self , X ): if self . _classes is None : raise TypeError ( \"You must call `fit` before `predict` to learn class mappings.\" ) predictions = super ( KerasClassifier , self ) . predict ( X ) predictions = predictions . argmax ( axis =- 1 ) return [ self . _inverse_classes [ yi ] for yi in predictions ] def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return super () . run ( X , y ) @nice_repr class KerasImagePreprocessor ( _ImageDataGenerator ): Note \"\"\"Augment a dataset of images by making changes to the original training set. Applies standard dataset augmentation strategies, such as rotating, scaling and fliping the image. Uses the ImageDataGenerator class from keras. The parameter grow_size determines how many new images will be created for each original image. The remaining parameters are passed to ImageDataGenerator . def __init__ ( self , featurewise_center : BooleanValue (), samplewise_center : BooleanValue (), featurewise_std_normalization : BooleanValue (), samplewise_std_normalization : BooleanValue (), rotation_range : DiscreteValue ( 0 , 15 ), width_shift_range : ContinuousValue ( 0 , 0.25 ), height_shift_range : ContinuousValue ( 0 , 0.25 ), shear_range : ContinuousValue ( 0 , 15 ), zoom_range : ContinuousValue ( 0 , 0.25 ), horizontal_flip : BooleanValue (), vertical_flip : BooleanValue (), ): super () . __init__ ( featurewise_center = featurewise_center , samplewise_center = samplewise_center , featurewise_std_normalization = featurewise_std_normalization , samplewise_std_normalization = samplewise_std_normalization , rotation_range = rotation_range , width_shift_range = width_shift_range , height_shift_range = height_shift_range , shear_range = shear_range , zoom_range = zoom_range , horizontal_flip = horizontal_flip , vertical_flip = vertical_flip , ) class KerasImageClassifier ( KerasClassifier ): def __init__ ( self , preprocessor : KerasImagePreprocessor , optimizer : CategoricalValue ( \"sgd\" , \"adam\" , \"rmsprop\" ), ** kwargs , ): self . preprocessor = preprocessor super () . __init__ ( optimizer = optimizer , ** kwargs ) def _build_grammar ( self ): return generate_grammar ( Modules . Preprocessing . Conv2D (), Modules.Features.Dense() ) def _fit_model ( self , X , y , ** kwargs ): self . preprocessor . fit ( X ) batch_size = 64 validation_size = min ( int ( 0.25 * len ( X )), 10 * batch_size ) Xtrain , Xvalid = X [: - validation_size ], X [ - validation_size :] ytrain , yvalid = y [: - validation_size ], y [ - validation_size :] self . model . fit ( self . preprocessor . flow ( Xtrain , ytrain , batch_size = batch_size ), steps_per_epoch = len ( Xtrain ) // batch_size , epochs = self . _epochs , callbacks = [ EarlyStopping ( patience = self . _early_stop , restore_best_weights = True ), TerminateOnNaN (), ], validation_data = ( Xvalid , yvalid ), verbose = 0 , ** kwargs , ) def run ( self , X : Tensor4 , y : Supervised [ VectorCategorical ]) -> VectorCategorical : return super () . run ( X , y ) def _build_input ( self , X ): return Input ( shape = X . shape [ 1 :]) class KerasSequenceClassifier ( KerasClassifier ): def _build_grammar ( self ): return build_grammar ( preprocessing = True , reduction = True , features = True ) def _build_input ( self , X ): return Input ( shape = ( None , X . shape [ 2 ])) def run ( self , X : Tensor3 , y : Supervised [ VectorCategorical ]) -> VectorCategorical : return super () . run ( X , y ) Warning These imports are taken from https://github.com/tensorflow/addons/pull/377 since the CRF layer has not yet landed in tensorflow TODO: Make sure to replace this when tensorflow merges this commit from autogoal.contrib.keras._crf import CRF from autogoal.contrib.keras._crf import crf_loss class KerasSequenceTagger ( KerasNeuralNetwork ): def __init__ ( self , decode : CategoricalValue ( \"dense\" , \"crf\" ), optimizer : CategoricalValue ( \"sgd\" , \"adam\" , \"rmsprop\" ), grammar = None , ** kwargs , ): self . _classes = None self . _num_classes = None if decode not in [ \"dense\" , \"crf\" ]: raise ValueError ( f \"Invalid decode= { decode } \" ) self . decode = decode super () . __init__ ( grammar = grammar or self . _build_grammar (), optimizer = optimizer , ** kwargs ) def _build_grammar ( self ): from autogoal.grammar._graph import Epsilon return GraphGrammar ( Epsilon ()) def _build_input ( self , X ): return Input ( shape = ( None , X [ 0 ] . shape [ - 1 ])) def _build_output ( self , outputs , y ): if \"loss\" not in self . _compile_kwargs : self . _compile_kwargs [ \"metrics\" ] = [ \"accuracy\" ] if self . decode == \"dense\" : self . _compile_kwargs [ \"loss\" ] = \"categorical_crossentropy\" elif self . decode == \"crf\" : self . _compile_kwargs [ \"loss\" ] = crf_loss if len ( outputs ) > 1 : outputs = concatenate ( outputs ) else : outputs = outputs [ 0 ] if self . decode == \"dense\" : dense = Dense ( units = len ( self . _classes ), activation = \"softmax\" ) return TimeDistributed ( dense )( outputs ) elif self . decode == \"crf\" : crf = CRF ( units = len ( self . _classes )) return crf ( outputs ) def fit ( self , X , y ): distinct_classes = set ( x for yi in y for x in yi ) self . _classes = { k : v for k , v in zip ( distinct_classes , range ( len ( distinct_classes ))) } self . _inverse_classes = { v : k for k , v in self . _classes . items ()} y = [[ self . _classes [ x ] for x in yi ] for yi in y ] return super () . fit ( X , y ) def _encode ( self , xi , yi ): if self . decode == \"dense\" : yi = to_categorical ( yi , len ( self . _classes )) return ( np . expand_dims ( xi , axis = 0 ), np . expand_dims ( yi , axis = 0 ), ) def _generate_batches ( self , X , y ): while True : for xi , yi in zip ( X , y ): yield self . _encode ( xi , yi ) def _fit_model ( self , X , y , ** kwargs ): self . model . fit ( self . _generate_batches ( X , y ), steps_per_epoch = len ( X ), epochs = self . _epochs , callbacks = [ EarlyStopping ( patience = self . _early_stop , restore_best_weights = True , monitor = \"accuracy\" , ), TerminateOnNaN (), ], verbose = 0 , ** kwargs , ) def _decode ( self , predictions ): if self . decode == \"dense\" : predictions = [ pr . argmax ( axis =- 1 ) for pr in predictions ] return [[ self . _inverse_classes [ x ] for x in yi [ 0 ]] for yi in predictions ] elif self . decode == \"crf\" : return [[ self . _inverse_classes [ x ] for x in yi [ 0 ]] for yi in predictions ] def predict ( self , X ): if self . _classes is None : raise TypeError ( \"You must call `fit` before `predict` to learn class mappings.\" ) predictions = [ self . model . predict ( np . expand_dims ( xi , axis = 0 )) for xi in X ] return self . _decode ( predictions ) def run ( self , X : Seq [ MatrixContinuousDense ], y : Supervised [ Seq [ Seq [ Label ]]] ) -> Seq [ Seq [ Label ]]: return super () . run ( X , y )","title":"Autogoal.contrib.keras. base"},{"location":"api/autogoal.contrib.keras._crf/","text":"Copyright 2019 The TensorFlow Authors. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Orginal implementation from keras_contrib/layers/crf \u00b6 Note \"\"\"Implementing Conditional Random Field layer. Warning This code is taken verbatim from https://github.com/tensorflow/addons/pull/377 since the CRF layer has not yet landed in tensorflow TODO: Make sure to replace this when tensorflow merges this commit Files: https://github.com/tensorflow/addons/blob/e0b5e0f056d24b95546970acd4aedc3a5530d466/tensorflow_addons/layers/crf.py https://github.com/tensorflow/addons/blob/e0b5e0f056d24b95546970acd4aedc3a5530d466/tensorflow_addons/losses/crf.py from __future__ import absolute_import , division , print_function import tensorflow as tf from typeguard import typechecked from tensorflow_addons.text.crf import crf_decode , crf_log_likelihood from tensorflow_addons.utils import types class CRF ( tf . keras . layers . Layer ): Note \"\"\"Linear chain conditional random field (CRF). Examples: ```python from tensorflow_addons.layers import CRF from tensorflow_addons.losses import crf_loss model = Sequential() model.add(Embedding(3001, 300, mask_zero=True) crf = CRF(10) model.add(crf) model.compile('adam', loss=crf_loss) model.fit(x, y) ``` Arguments: units: Positive integer, dimensionality of the output space, should equal to tag num. chain_initializer: Initializer for the chain_kernel weights matrix, used for the CRF chain energy. (see initializers ). chain_regularizer: Regularizer function applied to the chain_kernel weights matrix. chain_constraint: Constraint function applied to the chain_kernel weights matrix. use_boundary: Boolean (default True), indicating if trainable start-end chain energies should be added to model. boundary_initializer: Initializer for the left_boundary , 'right_boundary' weights vectors, used for the start/left and end/right boundary energy. boundary_regularizer: Regularizer function applied to the 'left_boundary', 'right_boundary' weight vectors. boundary_constraint: Constraint function applied to the left_boundary , right_boundary weights vectors. use_kernel: Boolean (default True), indicating if apply a fully connected layer before CRF op. kernel_initializer: Initializer for the kernel weights matrix, used for the linear transformation of the inputs. kernel_regularizer: Regularizer function applied to the kernel weights matrix. kernel_constraint: Constraint function applied to the kernel weights matrix. use_bias: Boolean (default True), whether the layer uses a bias vector. bias_initializer: Initializer for the bias vector. bias_regularizer: Regularizer function applied to the bias vector. bias_constraint: Constraint function applied to the bias vector. activation: default value is 'linear', Activation function to use. Input shape: 3D tensor with shape: (batch_size, sequence_length, feature_size) . Output shape: 2D tensor (dtype: int32) with shape: (batch_size, sequence_length) . Masking: This layer supports masking (2D tensor, shape: (batch_size, sequence_length) ) for input data with a variable number of timesteps. This layer output same make tensor, NOTICE this may cause issue when you use some keras loss and metrics function which usually expect 1D mask. Loss function: Due to the TF 2.0 version support eager execution be default, there is no way can implement CRF loss as independent loss function. Thus, user should use loss method of this layer. See Examples (above) for detailed usage. References: - Conditional Random Field @typechecked def __init__ ( self , units : int , chain_initializer : types . Initializer = \"orthogonal\" , chain_regularizer : types . Regularizer = None , chain_constraint : types . Constraint = None , use_boundary : bool = True , boundary_initializer : types . Initializer = \"zeros\" , boundary_regularizer : types . Regularizer = None , boundary_constraint : types . Constraint = None , use_kernel : bool = True , kernel_initializer : types . Initializer = \"glorot_uniform\" , kernel_regularizer : types . Regularizer = None , kernel_constraint : types . Constraint = None , use_bias : bool = True , bias_initializer : types . Initializer = \"zeros\" , bias_regularizer : types . Regularizer = None , bias_constraint : types . Constraint = None , activation : types . Activation = \"linear\" , ** kwargs ): super ( CRF , self ) . __init__ ( ** kwargs ) setup mask supporting flag, used by base class (the Layer) because base class's init method will set it to False unconditionally So this assigned must be executed after call base class's init method self . supports_masking = True self . units = units # numbers of tags self . use_boundary = use_boundary self . use_bias = use_bias self . use_kernel = use_kernel self . activation = tf . keras . activations . get ( activation ) self . kernel_initializer = tf . keras . initializers . get ( kernel_initializer ) self . chain_initializer = tf . keras . initializers . get ( chain_initializer ) self . boundary_initializer = tf . keras . initializers . get ( boundary_initializer ) self . bias_initializer = tf . keras . initializers . get ( bias_initializer ) self . kernel_regularizer = tf . keras . regularizers . get ( kernel_regularizer ) self . chain_regularizer = tf . keras . regularizers . get ( chain_regularizer ) self . boundary_regularizer = tf . keras . regularizers . get ( boundary_regularizer ) self . bias_regularizer = tf . keras . regularizers . get ( bias_regularizer ) self . kernel_constraint = tf . keras . constraints . get ( kernel_constraint ) self . chain_constraint = tf . keras . constraints . get ( chain_constraint ) self . boundary_constraint = tf . keras . constraints . get ( boundary_constraint ) self . bias_constraint = tf . keras . constraints . get ( bias_constraint ) values will be assigned in method self . input_spec = None value remembered for loss/metrics function self . potentials = None self . sequence_length = None self . mask = None global variable self . chain_kernel = None self . _dense_layer = None self . left_boundary = None self . right_boundary = None def build ( self , input_shape ): input_shape = tuple ( tf . TensorShape ( input_shape ) . as_list ()) see API docs of InputSpec for more detail self . input_spec = [ tf . keras . layers . InputSpec ( shape = input_shape )] weights that work as transfer probability of each tags self . chain_kernel = self . add_weight ( shape = ( self . units , self . units ), name = \"chain_kernel\" , initializer = self . chain_initializer , regularizer = self . chain_regularizer , constraint = self . chain_constraint , ) weight of to tag probability and tag to probability if self . use_boundary : self . left_boundary = self . add_weight ( shape = ( self . units ,), name = \"left_boundary\" , initializer = self . boundary_initializer , regularizer = self . boundary_regularizer , constraint = self . boundary_constraint , ) self . right_boundary = self . add_weight ( shape = ( self . units ,), name = \"right_boundary\" , initializer = self . boundary_initializer , regularizer = self . boundary_regularizer , constraint = self . boundary_constraint , ) if self . use_kernel : self . _dense_layer = tf . keras . layers . Dense ( units = self . units , activation = self . activation , use_bias = self . use_bias , bias_initializer = self . bias_initializer , kernel_regularizer = self . kernel_regularizer , bias_regularizer = self . bias_regularizer , kernel_constraint = self . kernel_constraint , bias_constraint = self . bias_constraint , dtype = self . dtype , ) else : self . _dense_layer = lambda x : tf . cast ( x , dtype = self . dtype ) super ( CRF , self ) . build ( input_shape ) def call ( self , inputs , mask = None , ** kwargs ): mask: Tensor(shape=(batch_size, sequence_length), dtype=bool) or None if mask is not None : if tf . keras . backend . ndim ( mask ) != 2 : raise ValueError ( \"Input mask to CRF must have dim 2 if not None\" ) left padding of mask is not supported, due the underline CRF function detect it and report it to user first_mask = None if mask is not None : left_boundary_mask = self . _compute_mask_left_boundary ( mask ) first_mask = left_boundary_mask [:, 0 ] remember this value for later use self . mask = mask if first_mask is not None : no_left_padding = tf . math . reduce_all ( first_mask ) msg = \"Currently, CRF layer do not support left padding\" with tf . control_dependencies ( [ tf . debugging . assert_equal ( no_left_padding , tf . constant ( True ), message = msg ) ] ): self . potentials = self . _dense_layer ( inputs ) else : self . potentials = self . _dense_layer ( inputs ) appending boundary probability info if self . use_boundary : self . potentials = self . add_boundary_energy ( self . potentials , mask , self . left_boundary , self . right_boundary ) self . sequence_length = self . _get_sequence_length ( inputs , mask ) decoded_sequence , _ = self . get_viterbi_decoding ( self . potentials , self . sequence_length ) return decoded_sequence def _get_sequence_length ( self , input_ , mask ): Note \"\"\"Currently underline CRF fucntion (provided by tensorflow_addons.text.crf) do not support bi-direction masking (left padding / right padding), it support right padding by tell it the sequence length. this function is compute the sequence length from input and mask. if mask is not None : sequence_length = self . mask_to_sequence_length ( mask ) else : make a mask tensor from input, then used to generate sequence_length input_energy_shape = tf . shape ( input_ ) raw_input_shape = tf . slice ( input_energy_shape , [ 0 ], [ 2 ]) alt_mask = tf . ones ( raw_input_shape ) sequence_length = self . mask_to_sequence_length ( alt_mask ) return sequence_length def mask_to_sequence_length ( self , mask ): Note \"\"\"compute sequence length from mask.\"\"\" sequence_length = tf.cast(tf.reduce_sum(tf.cast(mask, tf.int8), 1), tf.int64) return sequence_length @staticmethod def _compute_mask_right_boundary(mask): shift mask to left by 1: 0011100 => 0111000 offset = 1 left_shifted_mask = tf . concat ( [ mask [:, offset :], tf . zeros_like ( mask [:, : offset ])], axis = 1 ) NOTE: below code is different from keras_contrib Original code in keras_contrib: end_mask = K.cast( K.greater(self.shift_left(mask), mask), K.floatx() ) has a bug, confirmed by the original keras_contrib maintainer Luiz Felix (github: lzfelix), 0011100 > 0111000 => 0000100 right_boundary = tf . greater ( mask , left_shifted_mask ) return right_boundary @staticmethod def _compute_mask_left_boundary ( mask ): Note \"\"\"input mask: 0011100, output left_boundary: 0010000.\"\"\" shift mask to right by 1: 0011100 => 0001110 \u00b6 offset = 1 right_shifted_mask = tf.concat( [tf.zeros_like(mask[:, :offset]), mask[:, :-offset]], axis=1 ) 0011100 > 0001110 => 0010000 \u00b6 left_boundary = tf.greater( tf.cast(mask, tf.int32), tf.cast(right_shifted_mask, tf.int32) ) left_boundary = tf.greater(mask, right_shifted_mask) \u00b6 return left_boundary def add_boundary_energy(self, potentials, mask, start, end): def expand_scalar_to_3d(x): expand tensor from shape (x, ) to (1, 1, x) \u00b6 return tf.reshape(x, (1, 1, -1)) start = expand_scalar_to_3d(start) end = expand_scalar_to_3d(end) if mask is None: potentials = tf.concat( [potentials[:, :1, :] + start, potentials[:, 1:, :]], axis=1 ) potentials = tf.concat( [potentials[:, :-1, :], potentials[:, -1:, :] + end], axis=1 ) else: mask = tf.keras.backend.expand_dims(tf.cast(mask, start.dtype), axis=-1) start_mask = tf.cast(self._compute_mask_left_boundary(mask), start.dtype) end_mask = tf.cast(self._compute_mask_right_boundary(mask), end.dtype) potentials = potentials + start_mask * start potentials = potentials + end_mask * end return potentials def get_viterbi_decoding(self, potentials, sequence_length): decode_tags: A [batch_size, max_seq_len] matrix, with dtype tf.int32 \u00b6 decode_tags, best_score = crf_decode( potentials, self.chain_kernel, sequence_length ) return decode_tags, best_score def get_config(self): used for loading model from disk \u00b6 config = { \"units\": self.units, \"use_boundary\": self.use_boundary, \"use_bias\": self.use_bias, \"use_kernel\": self.use_kernel, \"kernel_initializer\": tf.keras.initializers.serialize( self.kernel_initializer ), \"chain_initializer\": tf.keras.initializers.serialize( self.chain_initializer ), \"boundary_initializer\": tf.keras.initializers.serialize( self.boundary_initializer ), \"bias_initializer\": tf.keras.initializers.serialize(self.bias_initializer), \"activation\": tf.keras.activations.serialize(self.activation), \"kernel_regularizer\": tf.keras.regularizers.serialize( self.kernel_regularizer ), \"chain_regularizer\": tf.keras.regularizers.serialize( self.chain_regularizer ), \"boundary_regularizer\": tf.keras.regularizers.serialize( self.boundary_regularizer ), \"bias_regularizer\": tf.keras.regularizers.serialize(self.bias_regularizer), \"kernel_constraint\": tf.keras.constraints.serialize(self.kernel_constraint), \"chain_constraint\": tf.keras.constraints.serialize(self.chain_constraint), \"boundary_constraint\": tf.keras.constraints.serialize( self.boundary_constraint ), \"bias_constraint\": tf.keras.constraints.serialize(self.bias_constraint), } base_config = super(CRF, self).get_config() return dict(list(base_config.items()) + list(config.items())) def compute_output_shape(self, input_shape): output_shape = input_shape[:2] return output_shape def compute_mask(self, input_, mask=None): return mask def get_negative_log_likelihood ( self , y_true ): y_true = tf . cast ( y_true , tf . int32 ) self . sequence_length = tf . cast ( self . sequence_length , tf . int32 ) log_likelihood , _ = crf_log_likelihood ( self . potentials , y_true , self . sequence_length , self . chain_kernel ) return - log_likelihood def get_loss ( self , y_true , y_pred ): we don't use y_pred, but caller pass it anyway, ignore it return self . get_negative_log_likelihood ( y_true ) def get_accuracy ( self , y_true , y_pred ): judge = tf . cast ( tf . equal ( y_pred , y_true ), tf . keras . backend . floatx ()) if self . mask is None : return tf . reduce_mean ( judge ) else : mask = tf . cast ( self . mask , tf . keras . backend . floatx ()) return tf . reduce_sum ( judge * mask ) / tf . reduce_sum ( mask ) def __call__ ( self , inputs , * args , ** kwargs ): outputs = super ( CRF , self ) . __call__ ( inputs , * args , ** kwargs ) A hack that add _keras_history to EagerTensor, make it more like normal Tensor for tensor in tf . nest . flatten ( outputs ): if not hasattr ( tensor , \"_keras_history\" ): tensor . _keras_history = ( self , 0 , 0 ) return outputs @property def _compute_dtype ( self ): fixed output dtype from underline CRF functions return tf . int32 class ConditionalRandomFieldLoss ( object ): def __init__ ( self , name : str = \"crf_loss\" ): self . name = name def get_config ( self ): return { \"name\" : self . name } def __call__ ( self , y_true , y_pred , sample_weight = None ): crf_layer = y_pred . _keras_history [ 0 ] check if last layer is CRF if not isinstance ( crf_layer , CRF ): raise ValueError ( \"Last layer must be CRF for use {} .\" . format ( self . __class__ . __name__ ) ) loss_vector = crf_layer . get_loss ( y_true , y_pred ) return tf . keras . backend . mean ( loss_vector ) crf_loss = ConditionalRandomFieldLoss ()","title":"Autogoal.contrib.keras. crf"},{"location":"api/autogoal.contrib.keras._crf/#orginal-implementation-from-keras_contriblayerscrf","text":"Note \"\"\"Implementing Conditional Random Field layer. Warning This code is taken verbatim from https://github.com/tensorflow/addons/pull/377 since the CRF layer has not yet landed in tensorflow TODO: Make sure to replace this when tensorflow merges this commit Files: https://github.com/tensorflow/addons/blob/e0b5e0f056d24b95546970acd4aedc3a5530d466/tensorflow_addons/layers/crf.py https://github.com/tensorflow/addons/blob/e0b5e0f056d24b95546970acd4aedc3a5530d466/tensorflow_addons/losses/crf.py from __future__ import absolute_import , division , print_function import tensorflow as tf from typeguard import typechecked from tensorflow_addons.text.crf import crf_decode , crf_log_likelihood from tensorflow_addons.utils import types class CRF ( tf . keras . layers . Layer ): Note \"\"\"Linear chain conditional random field (CRF). Examples: ```python from tensorflow_addons.layers import CRF from tensorflow_addons.losses import crf_loss model = Sequential() model.add(Embedding(3001, 300, mask_zero=True) crf = CRF(10) model.add(crf) model.compile('adam', loss=crf_loss) model.fit(x, y) ``` Arguments: units: Positive integer, dimensionality of the output space, should equal to tag num. chain_initializer: Initializer for the chain_kernel weights matrix, used for the CRF chain energy. (see initializers ). chain_regularizer: Regularizer function applied to the chain_kernel weights matrix. chain_constraint: Constraint function applied to the chain_kernel weights matrix. use_boundary: Boolean (default True), indicating if trainable start-end chain energies should be added to model. boundary_initializer: Initializer for the left_boundary , 'right_boundary' weights vectors, used for the start/left and end/right boundary energy. boundary_regularizer: Regularizer function applied to the 'left_boundary', 'right_boundary' weight vectors. boundary_constraint: Constraint function applied to the left_boundary , right_boundary weights vectors. use_kernel: Boolean (default True), indicating if apply a fully connected layer before CRF op. kernel_initializer: Initializer for the kernel weights matrix, used for the linear transformation of the inputs. kernel_regularizer: Regularizer function applied to the kernel weights matrix. kernel_constraint: Constraint function applied to the kernel weights matrix. use_bias: Boolean (default True), whether the layer uses a bias vector. bias_initializer: Initializer for the bias vector. bias_regularizer: Regularizer function applied to the bias vector. bias_constraint: Constraint function applied to the bias vector. activation: default value is 'linear', Activation function to use. Input shape: 3D tensor with shape: (batch_size, sequence_length, feature_size) . Output shape: 2D tensor (dtype: int32) with shape: (batch_size, sequence_length) . Masking: This layer supports masking (2D tensor, shape: (batch_size, sequence_length) ) for input data with a variable number of timesteps. This layer output same make tensor, NOTICE this may cause issue when you use some keras loss and metrics function which usually expect 1D mask. Loss function: Due to the TF 2.0 version support eager execution be default, there is no way can implement CRF loss as independent loss function. Thus, user should use loss method of this layer. See Examples (above) for detailed usage. References: - Conditional Random Field @typechecked def __init__ ( self , units : int , chain_initializer : types . Initializer = \"orthogonal\" , chain_regularizer : types . Regularizer = None , chain_constraint : types . Constraint = None , use_boundary : bool = True , boundary_initializer : types . Initializer = \"zeros\" , boundary_regularizer : types . Regularizer = None , boundary_constraint : types . Constraint = None , use_kernel : bool = True , kernel_initializer : types . Initializer = \"glorot_uniform\" , kernel_regularizer : types . Regularizer = None , kernel_constraint : types . Constraint = None , use_bias : bool = True , bias_initializer : types . Initializer = \"zeros\" , bias_regularizer : types . Regularizer = None , bias_constraint : types . Constraint = None , activation : types . Activation = \"linear\" , ** kwargs ): super ( CRF , self ) . __init__ ( ** kwargs ) setup mask supporting flag, used by base class (the Layer) because base class's init method will set it to False unconditionally So this assigned must be executed after call base class's init method self . supports_masking = True self . units = units # numbers of tags self . use_boundary = use_boundary self . use_bias = use_bias self . use_kernel = use_kernel self . activation = tf . keras . activations . get ( activation ) self . kernel_initializer = tf . keras . initializers . get ( kernel_initializer ) self . chain_initializer = tf . keras . initializers . get ( chain_initializer ) self . boundary_initializer = tf . keras . initializers . get ( boundary_initializer ) self . bias_initializer = tf . keras . initializers . get ( bias_initializer ) self . kernel_regularizer = tf . keras . regularizers . get ( kernel_regularizer ) self . chain_regularizer = tf . keras . regularizers . get ( chain_regularizer ) self . boundary_regularizer = tf . keras . regularizers . get ( boundary_regularizer ) self . bias_regularizer = tf . keras . regularizers . get ( bias_regularizer ) self . kernel_constraint = tf . keras . constraints . get ( kernel_constraint ) self . chain_constraint = tf . keras . constraints . get ( chain_constraint ) self . boundary_constraint = tf . keras . constraints . get ( boundary_constraint ) self . bias_constraint = tf . keras . constraints . get ( bias_constraint ) values will be assigned in method self . input_spec = None value remembered for loss/metrics function self . potentials = None self . sequence_length = None self . mask = None global variable self . chain_kernel = None self . _dense_layer = None self . left_boundary = None self . right_boundary = None def build ( self , input_shape ): input_shape = tuple ( tf . TensorShape ( input_shape ) . as_list ()) see API docs of InputSpec for more detail self . input_spec = [ tf . keras . layers . InputSpec ( shape = input_shape )] weights that work as transfer probability of each tags self . chain_kernel = self . add_weight ( shape = ( self . units , self . units ), name = \"chain_kernel\" , initializer = self . chain_initializer , regularizer = self . chain_regularizer , constraint = self . chain_constraint , ) weight of to tag probability and tag to probability if self . use_boundary : self . left_boundary = self . add_weight ( shape = ( self . units ,), name = \"left_boundary\" , initializer = self . boundary_initializer , regularizer = self . boundary_regularizer , constraint = self . boundary_constraint , ) self . right_boundary = self . add_weight ( shape = ( self . units ,), name = \"right_boundary\" , initializer = self . boundary_initializer , regularizer = self . boundary_regularizer , constraint = self . boundary_constraint , ) if self . use_kernel : self . _dense_layer = tf . keras . layers . Dense ( units = self . units , activation = self . activation , use_bias = self . use_bias , bias_initializer = self . bias_initializer , kernel_regularizer = self . kernel_regularizer , bias_regularizer = self . bias_regularizer , kernel_constraint = self . kernel_constraint , bias_constraint = self . bias_constraint , dtype = self . dtype , ) else : self . _dense_layer = lambda x : tf . cast ( x , dtype = self . dtype ) super ( CRF , self ) . build ( input_shape ) def call ( self , inputs , mask = None , ** kwargs ): mask: Tensor(shape=(batch_size, sequence_length), dtype=bool) or None if mask is not None : if tf . keras . backend . ndim ( mask ) != 2 : raise ValueError ( \"Input mask to CRF must have dim 2 if not None\" ) left padding of mask is not supported, due the underline CRF function detect it and report it to user first_mask = None if mask is not None : left_boundary_mask = self . _compute_mask_left_boundary ( mask ) first_mask = left_boundary_mask [:, 0 ] remember this value for later use self . mask = mask if first_mask is not None : no_left_padding = tf . math . reduce_all ( first_mask ) msg = \"Currently, CRF layer do not support left padding\" with tf . control_dependencies ( [ tf . debugging . assert_equal ( no_left_padding , tf . constant ( True ), message = msg ) ] ): self . potentials = self . _dense_layer ( inputs ) else : self . potentials = self . _dense_layer ( inputs ) appending boundary probability info if self . use_boundary : self . potentials = self . add_boundary_energy ( self . potentials , mask , self . left_boundary , self . right_boundary ) self . sequence_length = self . _get_sequence_length ( inputs , mask ) decoded_sequence , _ = self . get_viterbi_decoding ( self . potentials , self . sequence_length ) return decoded_sequence def _get_sequence_length ( self , input_ , mask ): Note \"\"\"Currently underline CRF fucntion (provided by tensorflow_addons.text.crf) do not support bi-direction masking (left padding / right padding), it support right padding by tell it the sequence length. this function is compute the sequence length from input and mask. if mask is not None : sequence_length = self . mask_to_sequence_length ( mask ) else : make a mask tensor from input, then used to generate sequence_length input_energy_shape = tf . shape ( input_ ) raw_input_shape = tf . slice ( input_energy_shape , [ 0 ], [ 2 ]) alt_mask = tf . ones ( raw_input_shape ) sequence_length = self . mask_to_sequence_length ( alt_mask ) return sequence_length def mask_to_sequence_length ( self , mask ): Note \"\"\"compute sequence length from mask.\"\"\" sequence_length = tf.cast(tf.reduce_sum(tf.cast(mask, tf.int8), 1), tf.int64) return sequence_length @staticmethod def _compute_mask_right_boundary(mask): shift mask to left by 1: 0011100 => 0111000 offset = 1 left_shifted_mask = tf . concat ( [ mask [:, offset :], tf . zeros_like ( mask [:, : offset ])], axis = 1 ) NOTE: below code is different from keras_contrib Original code in keras_contrib: end_mask = K.cast( K.greater(self.shift_left(mask), mask), K.floatx() ) has a bug, confirmed by the original keras_contrib maintainer Luiz Felix (github: lzfelix), 0011100 > 0111000 => 0000100 right_boundary = tf . greater ( mask , left_shifted_mask ) return right_boundary @staticmethod def _compute_mask_left_boundary ( mask ): Note \"\"\"input mask: 0011100, output left_boundary: 0010000.\"\"\"","title":"Orginal implementation from keras_contrib/layers/crf"},{"location":"api/autogoal.contrib.keras._crf/#shift-mask-to-right-by-1-0011100-0001110","text":"offset = 1 right_shifted_mask = tf.concat( [tf.zeros_like(mask[:, :offset]), mask[:, :-offset]], axis=1 )","title":"shift mask to right by 1: 0011100 =&gt; 0001110"},{"location":"api/autogoal.contrib.keras._crf/#0011100-0001110-0010000","text":"left_boundary = tf.greater( tf.cast(mask, tf.int32), tf.cast(right_shifted_mask, tf.int32) )","title":"0011100 &gt; 0001110 =&gt; 0010000"},{"location":"api/autogoal.contrib.keras._crf/#left_boundary-tfgreatermask-right_shifted_mask","text":"return left_boundary def add_boundary_energy(self, potentials, mask, start, end): def expand_scalar_to_3d(x):","title":"left_boundary = tf.greater(mask, right_shifted_mask)"},{"location":"api/autogoal.contrib.keras._crf/#expand-tensor-from-shape-x-to-1-1-x","text":"return tf.reshape(x, (1, 1, -1)) start = expand_scalar_to_3d(start) end = expand_scalar_to_3d(end) if mask is None: potentials = tf.concat( [potentials[:, :1, :] + start, potentials[:, 1:, :]], axis=1 ) potentials = tf.concat( [potentials[:, :-1, :], potentials[:, -1:, :] + end], axis=1 ) else: mask = tf.keras.backend.expand_dims(tf.cast(mask, start.dtype), axis=-1) start_mask = tf.cast(self._compute_mask_left_boundary(mask), start.dtype) end_mask = tf.cast(self._compute_mask_right_boundary(mask), end.dtype) potentials = potentials + start_mask * start potentials = potentials + end_mask * end return potentials def get_viterbi_decoding(self, potentials, sequence_length):","title":"expand tensor from shape (x, ) to (1, 1, x)"},{"location":"api/autogoal.contrib.keras._crf/#decode_tags-a-batch_size-max_seq_len-matrix-with-dtype-tfint32","text":"decode_tags, best_score = crf_decode( potentials, self.chain_kernel, sequence_length ) return decode_tags, best_score def get_config(self):","title":"decode_tags: A [batch_size, max_seq_len] matrix, with dtype tf.int32"},{"location":"api/autogoal.contrib.keras._crf/#used-for-loading-model-from-disk","text":"config = { \"units\": self.units, \"use_boundary\": self.use_boundary, \"use_bias\": self.use_bias, \"use_kernel\": self.use_kernel, \"kernel_initializer\": tf.keras.initializers.serialize( self.kernel_initializer ), \"chain_initializer\": tf.keras.initializers.serialize( self.chain_initializer ), \"boundary_initializer\": tf.keras.initializers.serialize( self.boundary_initializer ), \"bias_initializer\": tf.keras.initializers.serialize(self.bias_initializer), \"activation\": tf.keras.activations.serialize(self.activation), \"kernel_regularizer\": tf.keras.regularizers.serialize( self.kernel_regularizer ), \"chain_regularizer\": tf.keras.regularizers.serialize( self.chain_regularizer ), \"boundary_regularizer\": tf.keras.regularizers.serialize( self.boundary_regularizer ), \"bias_regularizer\": tf.keras.regularizers.serialize(self.bias_regularizer), \"kernel_constraint\": tf.keras.constraints.serialize(self.kernel_constraint), \"chain_constraint\": tf.keras.constraints.serialize(self.chain_constraint), \"boundary_constraint\": tf.keras.constraints.serialize( self.boundary_constraint ), \"bias_constraint\": tf.keras.constraints.serialize(self.bias_constraint), } base_config = super(CRF, self).get_config() return dict(list(base_config.items()) + list(config.items())) def compute_output_shape(self, input_shape): output_shape = input_shape[:2] return output_shape def compute_mask(self, input_, mask=None): return mask def get_negative_log_likelihood ( self , y_true ): y_true = tf . cast ( y_true , tf . int32 ) self . sequence_length = tf . cast ( self . sequence_length , tf . int32 ) log_likelihood , _ = crf_log_likelihood ( self . potentials , y_true , self . sequence_length , self . chain_kernel ) return - log_likelihood def get_loss ( self , y_true , y_pred ): we don't use y_pred, but caller pass it anyway, ignore it return self . get_negative_log_likelihood ( y_true ) def get_accuracy ( self , y_true , y_pred ): judge = tf . cast ( tf . equal ( y_pred , y_true ), tf . keras . backend . floatx ()) if self . mask is None : return tf . reduce_mean ( judge ) else : mask = tf . cast ( self . mask , tf . keras . backend . floatx ()) return tf . reduce_sum ( judge * mask ) / tf . reduce_sum ( mask ) def __call__ ( self , inputs , * args , ** kwargs ): outputs = super ( CRF , self ) . __call__ ( inputs , * args , ** kwargs ) A hack that add _keras_history to EagerTensor, make it more like normal Tensor for tensor in tf . nest . flatten ( outputs ): if not hasattr ( tensor , \"_keras_history\" ): tensor . _keras_history = ( self , 0 , 0 ) return outputs @property def _compute_dtype ( self ): fixed output dtype from underline CRF functions return tf . int32 class ConditionalRandomFieldLoss ( object ): def __init__ ( self , name : str = \"crf_loss\" ): self . name = name def get_config ( self ): return { \"name\" : self . name } def __call__ ( self , y_true , y_pred , sample_weight = None ): crf_layer = y_pred . _keras_history [ 0 ] check if last layer is CRF if not isinstance ( crf_layer , CRF ): raise ValueError ( \"Last layer must be CRF for use {} .\" . format ( self . __class__ . __name__ ) ) loss_vector = crf_layer . get_loss ( y_true , y_pred ) return tf . keras . backend . mean ( loss_vector ) crf_loss = ConditionalRandomFieldLoss ()","title":"used for loading model from disk"},{"location":"api/autogoal.contrib.keras._generated/","text":"from tensorflow.keras.layers import Conv1D as _Conv1D from tensorflow.keras.layers import Conv2D as _Conv2D from tensorflow.keras.layers import MaxPooling2D as _MaxPooling2D from tensorflow.keras.layers import Dense as _Dense from tensorflow.keras.layers import Embedding as _Embedding from tensorflow.keras.layers import LSTM as _LSTM from tensorflow.keras.layers import Bidirectional from tensorflow.keras.layers import Dropout as _Dropout from tensorflow.keras.layers import BatchNormalization as _BatchNormalization from tensorflow.keras.layers import TimeDistributed as _TimeDistributed from tensorflow.keras.layers import Activation as _Activation from tensorflow.keras.layers import Flatten as _Flatten from tensorflow.keras.layers import Reshape as _Reshape from tensorflow.keras import regularizers from autogoal.grammar import ( BooleanValue , CategoricalValue , DiscreteValue , ContinuousValue , ) from autogoal.utils import nice_repr @nice_repr class Seq2SeqLSTM ( _LSTM ): def __init__ ( self , units : DiscreteValue ( 32 , 1024 ), activation_fn : CategoricalValue ( \"tanh\" , \"sigmoid\" , \"relu\" , \"linear\" ), recurrent_activation_fn : CategoricalValue ( \"tanh\" , \"sigmoid\" , \"relu\" , \"linear\" ), dropout : ContinuousValue ( 0 , 0.5 ), recurrent_dropout : ContinuousValue ( 0 , 0.5 ), ): super () . __init__ ( units = units , activation = activation_fn , recurrent_activation = recurrent_activation_fn , dropout = dropout , recurrent_dropout = recurrent_dropout , return_sequences = True , ) self . activation_fn = activation_fn self . recurrent_activation_fn = recurrent_activation_fn @nice_repr class Seq2VecLSTM ( _LSTM ): def __init__ ( self , units : DiscreteValue ( 32 , 1024 ), activation_fn : CategoricalValue ( \"tanh\" , \"sigmoid\" , \"relu\" , \"linear\" ), recurrent_activation_fn : CategoricalValue ( \"tanh\" , \"sigmoid\" , \"relu\" , \"linear\" ), dropout : ContinuousValue ( 0 , 0.5 ), recurrent_dropout : ContinuousValue ( 0 , 0.5 ), ): super () . __init__ ( units = units , activation = activation_fn , recurrent_activation = recurrent_activation_fn , dropout = dropout , recurrent_dropout = recurrent_dropout , return_sequences = False , ) self . activation_fn = activation_fn self . recurrent_activation_fn = recurrent_activation_fn @nice_repr class Seq2SeqBiLSTM ( Bidirectional ): def __init__ ( self , merge_mode : CategoricalValue ( \"sum\" , \"mul\" , \"concat\" , \"ave\" ), units : DiscreteValue ( 32 , 1024 ), activation_fn : CategoricalValue ( \"tanh\" , \"sigmoid\" , \"relu\" , \"linear\" ), recurrent_activation_fn : CategoricalValue ( \"tanh\" , \"sigmoid\" , \"relu\" , \"linear\" ), dropout : ContinuousValue ( 0 , 0.5 ), recurrent_dropout : ContinuousValue ( 0 , 0.5 ), ): super () . __init__ ( layer = _LSTM ( units = units , activation = activation_fn , recurrent_activation = recurrent_activation_fn , dropout = dropout , recurrent_dropout = recurrent_dropout , return_sequences = True , ), merge_mode = merge_mode , ) self . activation_fn = activation_fn self . recurrent_activation_fn = recurrent_activation_fn @nice_repr class Seq2VecBiLSTM ( Bidirectional ): def __init__ ( self , merge_mode : CategoricalValue ( \"sum\" , \"mul\" , \"concat\" , \"ave\" ), units : DiscreteValue ( 32 , 1024 ), activation_fn : CategoricalValue ( \"tanh\" , \"sigmoid\" , \"relu\" , \"linear\" ), recurrent_activation_fn : CategoricalValue ( \"tanh\" , \"sigmoid\" , \"relu\" , \"linear\" ), dropout : ContinuousValue ( 0 , 0.5 ), recurrent_dropout : ContinuousValue ( 0 , 0.5 ), ): super () . __init__ ( layer = _LSTM ( units = units , activation = activation_fn , recurrent_activation = recurrent_activation_fn , dropout = dropout , recurrent_dropout = recurrent_dropout , return_sequences = False , ), merge_mode = merge_mode , ) self . activation_fn = activation_fn self . recurrent_activation_fn = recurrent_activation_fn @nice_repr class Reshape2D ( _Reshape ): def __init__ ( self ): super () . __init__ ( target_shape = ( - 1 , 1 )) @nice_repr class Embedding ( _Embedding ): def __init__ ( self , output_dim : DiscreteValue ( 32 , 128 )): super () . __init__ ( input_dim = 1000 , output_dim = output_dim ) @nice_repr class Dense ( _Dense ): def __init__ ( self , units : DiscreteValue ( 128 , 1024 ), ** kwargs ): super () . __init__ ( units = units , ** kwargs ) @nice_repr class Conv1D ( _Conv1D ): def __init__ ( self , filters : DiscreteValue ( 2 , 8 ), kernel_size : CategoricalValue ( 3 , 5 , 7 ) ): super () . __init__ ( filters = 2 ** filters , kernel_size = kernel_size , padding = \"causal\" ) @nice_repr class Conv2D ( _Conv2D ): def __init__ ( self , filters : DiscreteValue ( 2 , 8 ), kernel_size : CategoricalValue ( 3 , 5 , 7 ), l1 : ContinuousValue ( 0 , 1e-3 ), l2 : ContinuousValue ( 0 , 1e-3 ), ): self . l1 = l1 self . l2 = l2 super () . __init__ ( filters = 2 ** filters , kernel_size = ( kernel_size , kernel_size ), kernel_regularizer = regularizers . l1_l2 ( l1 = l1 , l2 = l2 ), padding = \"same\" , data_format = \"channels_last\" , ) @nice_repr class MaxPooling2D ( _MaxPooling2D ): def __init__ ( self ): super () . __init__ ( data_format = \"channels_last\" , padding = \"same\" , ) @nice_repr class TimeDistributed ( _TimeDistributed ): def __init__ ( self , layer : Dense ): super () . __init__ ( layer ) @nice_repr class Dropout ( _Dropout ): def __init__ ( self , rate : ContinuousValue ( 0 , 0.5 )): super () . __init__ ( rate = rate ) @nice_repr class BatchNormalization ( _BatchNormalization ): def __init__ ( self ): super () . __init__ () @nice_repr class Activation ( _Activation ): def __init__ ( self , function : CategoricalValue ( \"elu\" , \"selu\" , \"relu\" , \"tanh\" , \"sigmoid\" , \"hard_sigmoid\" , \"exponential\" , \"linear\" , ), ): self . function = function super () . __init__ ( activation = function ) @nice_repr class Flatten ( _Flatten ): def __init__ ( self ): super () . __init__ ()","title":"Autogoal.contrib.keras. generated"},{"location":"api/autogoal.contrib.keras._grammars/","text":"from autogoal.contrib.keras._generated import * from autogoal.grammar import GraphGrammar , Path , Block , CfgInitializer , Epsilon class Module : def make_top_level ( self , top_level : list ): pass def add_productions ( self , grammar : GraphGrammar ): raise NotImplementedError () class Modules : class Preprocessing : class Recurrent ( Module ): def make_top_level ( self , top_level ): if \"PreprocessingModule\" not in top_level : top_level . append ( \"PreprocessingModule\" ) def add_productions ( self , grammar : GraphGrammar ): grammar . add ( \"PreprocessingModule\" , \"RecurrentModule\" ) grammar . add ( \"RecurrentModule\" , Path ( \"RecurrentCell\" , \"RecurrentModule\" )) grammar . add ( \"RecurrentModule\" , \"RecurrentCell\" ) grammar . add ( \"RecurrentCell\" , Seq2SeqLSTM ) grammar . add ( \"RecurrentCell\" , Seq2SeqBiLSTM ) class Conv2D ( Module ): def make_top_level ( self , top_level ): if \"PreprocessingModule\" not in top_level : top_level . append ( \"PreprocessingModule\" ) def add_productions ( self , grammar : GraphGrammar ): grammar . add ( \"PreprocessingModule\" , Path ( \"Conv2DModule\" , Flatten )) grammar . add ( \"Conv2DModule\" , Path ( \"Conv2DBlock\" , \"Conv2DModule\" )) grammar . add ( \"Conv2DModule\" , \"Conv2DBlock\" ) grammar . add ( \"Conv2DBlock\" , Path ( \"Conv2DCells\" , MaxPooling2D )) grammar . add ( \"Conv2DBlock\" , Path ( \"Conv2DCells\" , MaxPooling2D , Dropout )) grammar . add ( \"Conv2DCells\" , Path ( \"Conv2DCell\" , \"Conv2DCells\" )) grammar . add ( \"Conv2DCells\" , Path ( \"Conv2DCell\" )) grammar . add ( \"Conv2DCell\" , Path ( Conv2D , Activation , BatchNormalization )) grammar . add ( \"Conv2DCell\" , Path ( Conv2D , Activation )) class Features : class Dense ( Module ): def make_top_level ( self , top_level ): if \"FeaturesModule\" not in top_level : top_level . append ( \"FeaturesModule\" ) def add_productions ( self , grammar ): grammar . add ( \"FeaturesModule\" , Path ( \"DenseModule\" , \"FeaturesModule\" )) grammar . add ( \"FeaturesModule\" , Epsilon ()) grammar . add ( \"DenseModule\" , Block ( \"DenseCell\" , \"DenseModule\" )) grammar . add ( \"DenseModule\" , Path ( \"DenseCell\" , \"DenseModule\" )) grammar . add ( \"DenseModule\" , Epsilon ()) grammar . add ( \"DenseCell\" , Path ( Dense , Activation , Dropout )) grammar . add ( \"DenseCell\" , Path ( Dense , Activation )) def generate_grammar ( * modules ): top_level = [] for mod in modules : mod . make_top_level ( top_level ) grammar = GraphGrammar ( start = Path ( * top_level ), initializer = CfgInitializer ()) for mod in modules : mod . add_productions ( grammar ) return grammar def build_grammar ( preprocessing = False , preprocessing_recurrent = True , preprocessing_conv = False , reduction = False , reduction_recurrent = True , reduction_conv = False , features = False , features_time_distributed = False , ): modules = [] if preprocessing : modules . append ( \"PreprocessingModule\" ) if reduction : modules . append ( \"ReductionModule\" ) if features : modules . append ( \"FeaturesModule\" ) if features_time_distributed : modules . append ( \"FeaturesTimeDistributedModule\" ) if preprocessing_recurrent and preprocessing_conv : raise ValueError ( \"Cannot combine recurrent with convolutional preprocessing.\" ) if reduction_recurrent and reduction_conv : raise ValueError ( \"Cannot combine recurrent with convolutional reduction modules.\" ) if ( reduction or features ) and features_time_distributed : raise ValueError ( \"Cannot combine time-distributed modules with flat modules.\" ) if not modules : raise ValueError ( \"At least one module must be activated.\" ) grammar = GraphGrammar ( start = Path ( * modules ), initializer = CfgInitializer ()) if preprocessing_recurrent : grammar . add ( \"PreprocessingModule\" , \"PreprocessingModuleR\" ) grammar . add ( \"PreprocessingModuleR\" , Path ( \"Recurrent\" , \"PreprocessingModuleR\" )) grammar . add ( \"PreprocessingModuleR\" , Epsilon ()) grammar . add ( \"Recurrent\" , Seq2SeqLSTM ) grammar . add ( \"Recurrent\" , Seq2SeqBiLSTM ) if preprocessing_conv : grammar . add ( \"PreprocessingModule\" , Path ( \"Convolutional\" , \"PreprocessingModuleC\" ) ) grammar . add ( \"PreprocessingModuleC\" , Path ( \"Convolutional\" , \"PreprocessingModuleC\" ) ) grammar . add ( \"PreprocessingModuleC\" , Epsilon ()) grammar . add ( \"Convolutional\" , Path ( Conv2D , MaxPooling2D )) grammar . add ( \"Convolutional\" , Path ( Conv2D , MaxPooling2D , Dropout )) if reduction_recurrent : grammar . add ( \"ReductionModule\" , Seq2VecLSTM ) grammar . add ( \"ReductionModule\" , Seq2VecBiLSTM ) if reduction_conv : grammar . add ( \"ReductionModule\" , Flatten ) if features : grammar . add ( \"FeaturesModule\" , Path ( \"DenseLayer\" , \"FeaturesModule\" )) grammar . add ( \"FeaturesModule\" , Epsilon ()) grammar . add ( \"DenseLayer\" , Block ( Dense , \"DenseLayer\" )) grammar . add ( \"DenseLayer\" , Path ( Dense , \"DenseLayer\" )) grammar . add ( \"DenseLayer\" , Path ( Dense , Dropout , \"DenseLayer\" )) grammar . add ( \"DenseLayer\" , Epsilon ()) if features_time_distributed : grammar . add ( \"FeaturesTimeDistributedModule\" , Path ( \"TDLayer\" , \"FeaturesTimeDistributedModule\" ), ) grammar . add ( \"FeaturesTimeDistributedModule\" , Epsilon ()) grammar . add ( \"TDLayer\" , Block ( TimeDistributed , \"TDLayer\" )) grammar . add ( \"TDLayer\" , Path ( TimeDistributed , \"TDLayer\" )) grammar . add ( \"TDLayer\" , Epsilon ()) return grammar","title":"Autogoal.contrib.keras. grammars"},{"location":"api/autogoal.contrib.nltk.__init__/","text":"try : import nltk except : print ( \"(!) Code in `autogoal.contrib.nltk` requires `nltk`.\" ) print ( \"(!) You can install it with `pip install autogoal[nltk]`.\" ) raise from autogoal.contrib.nltk._generated import * from autogoal.contrib.nltk._manual import * import os from pathlib import Path CONTRIB_NAME = \"nltk\" DATA_PATH = Path . home () / \".autogoal\" / \"contrib\" / CONTRIB_NAME / \"data\" if DATA_PATH not in nltk . data . path : nltk . data . path . insert ( 0 , str ( DATA_PATH )) def download (): os . makedirs ( DATA_PATH , exist_ok = True ) return nltk . download ( info_or_id = [ \"wordnet\" , \"sentiwordnet\" , \"averaged_perceptron_tagger\" , \"rslp\" , \"stopwords\" , ], download_dir = DATA_PATH , ) def status (): from autogoal.contrib import ContribStatus try : from nltk.corpus import wordnet from nltk.corpus import sentiwordnet from nltk.corpus import stopwords from nltk.stem import RSLPStemmer st = RSLPStemmer () from nltk.tag import PerceptronTagger tagger = PerceptronTagger () except LookupError : return ContribStatus . RequiresDownload return ContribStatus . Ready","title":"Autogoal.contrib.nltk.  init  "},{"location":"api/autogoal.contrib.nltk._builder/","text":"from autogoal.kb import AlgorithmBase import gensim import nltk import black import textwrap import datetime import inspect import re import numpy as np import warnings import abc import importlib import enlighten from pathlib import Path from autogoal.kb import * from autogoal.grammar import ( DiscreteValue , ContinuousValue , CategoricalValue , BooleanValue , ) from autogoal.contrib.sklearn._builder import SklearnWrapper from ._utils import ( _is_algorithm , get_input_output , is_algorithm , _is_tagger , is_pretrained_tagger , ) languages = [ \"arabic\" , \"danish\" , \"dutch\" , \"english\" , \"finnish\" , \"french\" , \"german\" , \"hungarian\" , \"italian\" , \"norwegian\" , \"portuguese\" , \"romanian\" , \"russian\" , \"spanish\" , \"swedish\" , ] languages_re = re . compile ( \"|\" . join ( languages )) class NltkTokenizer ( AlgorithmBase ): def run ( self , input ): return self . tokenize ( input ) @abc . abstractmethod def tokenize ( self , X ): pass class NltkStemmer ( AlgorithmBase ): def run ( self , input ): return self . stem ( input ) @abc . abstractmethod def stem ( self , X ): pass class NltkLemmatizer ( AlgorithmBase ): def run ( self , input ): return self . lemmatize ( input ) @abc . abstractmethod def lemmatize ( self , X ): pass class NltkClusterer ( SklearnWrapper ): def _train ( self , input ): X , y = input self . cluster ( X ) return [ self . classify ( x ) for x in X ] def _eval ( self , input ): X , y = input return X , [ self . classify ( x ) for x in X ] class NltkTagger ( SklearnWrapper ): def _train ( self , X , y ): tagged_sentences = [ list ( zip ( words , tags )) for words , tags in zip ( X , y )] self . _instance = self . tagger ( train = tagged_sentences ) return X , y def _eval ( self , X , y = None ): return [ [ tag for _ , tag in sentence ] for sentence in self . _instance . tag_sents ( X ) ] class NltkTrainedTagger ( SklearnWrapper ): def _train ( self , X , y ): X expected to be a tokenized sentence return self . tag ( X ), y def _eval ( self , X , y = None ): X expected to be a tokenized sentence return self . tag ( X ) base_classes = { \"sent_tokenizer\" : \"NltkTokenizer\" , \"word_tokenizer\" : \"NltkTokenizer\" , \"lemmatizer\" : \"NltkLemmatizer\" , \"stemmer\" : \"NltkStemmer\" , \"word_embbeder\" : \"SklearnWrapper\" , \"doc_embbeder\" : \"SklearnWrapper\" , \"tagger\" : \"NltkTagger\" , \"trained_tagger\" : \"NltkTrainedTagger\" , } GENERATION_RULES = dict ( SnowballStemmer = dict ( assume = True , assume_input = Word , assume_output = Stem ), ) def build_nltk_wrappers (): imports = _walk ( nltk ) imports += _walk ( nltk . cluster ) imports += _walk ( gensim . models ) imports += _walk(nltk.chunk.named_entity) imports += _walk ( nltk . tag ) manager = enlighten . get_manager () counter = manager . counter ( total = len ( imports ), unit = \"classes\" ) path = Path ( __file__ ) . parent / \"_generated.py\" imports = set ( imports ) with open ( path , \"w\" ) as fp : fp . write ( textwrap . dedent ( f \"\"\" AUTOGENERATED ON {datetime.datetime.now()} from autogoal.grammar import Continuous , Discrete , Categorical , Boolean from autogoal.contrib.nltk._builder import NltkStemmer , NltkTokenizer , NltkLemmatizer , NltkTagger , NltkTrainedTagger from autogoal.kb import * from autogoal.utils import nice_repr from numpy import inf , nan Note ) ) for cls in imports: counter.update() _write_class(cls, fp) black.reformat_one( path, True, black.WriteBack.YES, black.FileMode(), black.Report() ) counter.close() manager.stop() def _write_class(cls, fp): try: args = _get_args(cls) except Exception as e: warnings.warn(\"Error to generate wrapper for %s : %s\" % (cls. name , e)) return rules = GENERATION_RULES.get(cls. name ) assumed = False if rules: if rules.get(\"assume\"): assumed = True inputs = rules.get(\"assume_input\") outputs = rules.get(\"assume_output\") if not assumed: inputs, outputs = get_input_output(cls) if not inputs: warnings.warn(\"Cannot find correct types for %r\" % cls) return s = \" \" * 4 args_str = f\",\\n{s * 4}\".join(f\"{key}: {value}\" for key, value in args.items()) init_str = f\",\\n{s * 5}\".join(f\"{key}={key}\" for key in args) self_str = f\"\\n{s * 5}\".join(f\"self.{key}={key}\" for key in args) values_str = f\",\".join(f\"{key}={key}\" for key in args) input_str, output_str = repr(inputs), repr(outputs) base_class = base_classes.get(is_algorithm(cls), None) # set correct base class if not base_class: warnings.warn(\"Error to generate wrapper for %s\" % cls. name ) return print(cls) if is_tagger(cls) and not is_pretrained_tagger(cls)[0]: fp.write( textwrap.dedent( f\"\"\" from {cls.__module__} import {cls.__name__} as {cls. name } @nice_repr class {cls. name }({base_class}): def init ( self, {args_str} ): {self_str} self.tagger = _{cls. name } self.values = dict({values_str}) {base_class}. init (self) def run(self, X: {input_str}, y:Supervised[{output_str}]) -> {output_str}: return {base_class}.run(self, X, y) ) ) else : fp . write ( textwrap . dedent ( f \"\"\" from { cls . __module__ } import { cls . __name__ } as _ { cls . __name__ } @nice_repr class { cls . __name__ } (_ { cls . __name__ } , { base_class } ): def __init__( self, { args_str } ): { self_str } { base_class } .__init__(self) _ { cls . __name__ } .__init__( self, { init_str } ) def run(self, input: { input_str } ) -> { output_str } : return { base_class } .run(self, input) Note ) ) fp.flush() def write_tagger(cls, fp, args_str, init_str, input_str, output_str): fp.write( textwrap.dedent( f\"\"\" from {cls.__module__} import {cls.__name__} as {cls. name } @nice_repr class {cls. name }( {cls.__name__}, {base_class}): def __init__( self, {args_str} ): self.tagger = {cls. name } {base_class}. init (self) _{cls. name }. init ( self {init_str} ) def run(self, input: {input_str}) -> {output_str}: return {base_class}.run(self, input) ) ) fp . flush () def _walk ( module , name = \"nltk\" ): imports = [] def _walk_p ( module , name = \"nltk\" ): all_elements = dir ( module ) for elem in all_elements : if elem == \"exceptions\" : continue name = name + \".\" + elem try : obj = getattr ( module , elem ) if isinstance ( obj , type ): ignore nltk interfaces if name . endswith ( \"I\" ): continue if not _is_algorithm ( obj ): continue imports . append ( obj ) _walk_p(obj, name) If not module do not walk in it except Exception as e : pass try : inner_module = importlib . import_module ( name ) _walk_p ( inner_module , name ) except : pass _walk_p ( module , name ) imports . sort ( key = lambda c : ( c . __module__ , c . __name__ )) return imports def _find_parameter_values ( parameter , cls ): documentation = [] lines = cls . __doc__ . split ( \" \\n \" ) while lines : l = lines . pop ( 0 ) if l . strip () . startswith ( parameter ): documentation . append ( l ) tabs = l . index ( parameter ) break while lines : l = lines . pop ( 0 ) if not l . strip (): continue if l . startswith ( \" \" * ( tabs + 1 )): documentation . append ( l ) else : break options = set ( re . findall ( r \"'(\\w+)'\" , \" \" . join ( documentation ))) valid = [] invalid = [] skip = set ([ \"deprecated\" , \"auto_deprecated\" , \"precomputed\" ]) for opt in options : opt = opt . lower () if opt in skip : continue try : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) cls ( ** { parameter : opt }) . fit ( np . ones (( 10 , 10 )), [ True ] * 5 + [ False ] * 5 ) valid . append ( opt ) except Exception as e : invalid . append ( opt ) return sorted ( valid ) def _find_language_values ( cls ): global languages_re documentation = cls . __doc__ return set ( languages_re . findall ( str . lower ( documentation ))) def _get_args ( cls ): full_specs = inspect . getfullargspec ( cls . __init__ ) args = full_specs . args specs = full_specs . defaults if not args or not specs : return {} non_kwargs = [ arg for arg in args [: - len ( specs ) :] if arg != \"self\" ] args = args [ - len ( specs ) :] args_map = { k : v for k , v in zip ( args , specs )} drop_args = [ \"url\" , \"n_jobs\" , \"max_iter\" , \"class_weight\" , \"warm_start\" , \"copy_X\" , \"copy_x\" , \"copy\" , \"eps\" , \"ignore_stopwords\" , \"verbose\" , \"load\" , ] for arg in drop_args : args_map . pop ( arg , None ) result = {} for arg , value in args_map . items (): values = _get_arg_values ( arg , value , cls ) if not values : continue result [ arg ] = values for arg in non_kwargs : special handling of language if str . lower ( arg ) == \"language\" : values = _find_language_values ( cls ) if values : result [ arg ] = CategoricalValue ( * values ) continue if str . lower ( arg ) == \"train\" and _is_tagger ( cls ): continue raise Exception ( \"No values found for positional argument %s \" % ( arg )) return result def _get_arg_values ( arg , value , cls ): if isinstance ( value , bool ): return BooleanValue () if isinstance ( value , int ): return DiscreteValue ( * _get_integer_values ( arg , value , cls )) if isinstance ( value , float ): return ContinuousValue ( * _get_float_values ( arg , value , cls )) if isinstance ( value , str ): values = _find_parameter_values ( arg , cls ) return CategoricalValue ( * values ) if values else None return None def _get_integer_values ( arg , value , cls ): if value == 0 : min_val = - 100 max_val = 100 else : min_val = value // 2 max_val = 2 * value return min_val , max_val def _get_float_values ( arg , value , cls ): if value == 0 : min_val = - 1 max_val = 1 elif 0 < value <= 0.1 : min_val = value / 100 max_val = 1 elif 0 < value <= 1 : min_val = 1e-6 max_val = 1 else : min_val = value / 2 max_val = 2 * value return min_val , max_val if __name__ == \"__main__\" : build_nltk_wrappers ()","title":"Autogoal.contrib.nltk. builder"},{"location":"api/autogoal.contrib.nltk._generated/","text":"AUTOGENERATED ON 2020-02-01 12:18:14.877438 from autogoal.grammar import ( ContinuousValue , DiscreteValue , CategoricalValue , BooleanValue , ) from autogoal.contrib.nltk._builder import ( NltkStemmer , NltkTokenizer , NltkLemmatizer , NltkTagger , NltkTrainedTagger , ) from autogoal.kb import Word , Stem , Sentence , Seq , Postag , Document , algorithm from autogoal.kb import Supervised from autogoal.utils import nice_repr from numpy import inf , nan from nltk.stem.cistem import Cistem as _Cistem @nice_repr class Cistem ( _Cistem , NltkStemmer ): def __init__ ( self , case_insensitive : BooleanValue ()): self . case_insensitive = case_insensitive NltkStemmer . __init__ ( self ) _Cistem . __init__ ( self , case_insensitive = case_insensitive ) def run ( self , input : Word ) -> Stem : return NltkStemmer . run ( self , input ) from nltk.stem.isri import ISRIStemmer as _ISRIStemmer @nice_repr class ISRIStemmer ( _ISRIStemmer , NltkStemmer ): def __init__ ( self ,): NltkStemmer . __init__ ( self ) _ISRIStemmer . __init__ ( self ,) def run ( self , input : Word ) -> Stem : return NltkStemmer . run ( self , input ) from nltk.stem.lancaster import LancasterStemmer as _LancasterStemmer @nice_repr class LancasterStemmer ( _LancasterStemmer , NltkStemmer ): def __init__ ( self , strip_prefix_flag : BooleanValue ()): self . strip_prefix_flag = strip_prefix_flag NltkStemmer . __init__ ( self ) _LancasterStemmer . __init__ ( self , strip_prefix_flag = strip_prefix_flag ) def run ( self , input : Word ) -> Stem : return NltkStemmer . run ( self , input ) from nltk.stem.porter import PorterStemmer as _PorterStemmer @nice_repr class PorterStemmer ( _PorterStemmer , NltkStemmer ): def __init__ ( self ,): NltkStemmer . __init__ ( self ) _PorterStemmer . __init__ ( self ,) def run ( self , input : Word ) -> Stem : return NltkStemmer . run ( self , input ) from nltk.stem.rslp import RSLPStemmer as _RSLPStemmer @nice_repr class RSLPStemmer ( _RSLPStemmer , NltkStemmer ): def __init__ ( self ,): NltkStemmer . __init__ ( self ) _RSLPStemmer . __init__ ( self ,) def run ( self , input : Word ) -> Stem : return NltkStemmer . run ( self , input ) from nltk.stem.snowball import SnowballStemmer as _SnowballStemmer @nice_repr class SnowballStemmer ( _SnowballStemmer , NltkStemmer ): def __init__ ( self , language : CategoricalValue ( \"italian\" , \"portuguese\" , \"hungarian\" , \"english\" , \"german\" , \"arabic\" , \"danish\" , \"norwegian\" , \"finnish\" , \"dutch\" , \"romanian\" , \"russian\" , \"swedish\" , \"spanish\" , \"french\" , ), ): self . language = language NltkStemmer . __init__ ( self ) _SnowballStemmer . __init__ ( self , language = language ) def run ( self , input : Word ) -> Stem : return NltkStemmer . run ( self , input ) from nltk.stem.wordnet import WordNetLemmatizer as _WordNetLemmatizer @nice_repr class WordNetLemmatizer ( _WordNetLemmatizer , NltkLemmatizer ): def __init__ ( self ,): NltkLemmatizer . __init__ ( self ) _WordNetLemmatizer . __init__ ( self ,) def run ( self , input : Word ) -> Stem : return NltkLemmatizer . run ( self , input ) from nltk.tag.perceptron import PerceptronTagger as _PerceptronTagger @nice_repr class PerceptronTagger ( _PerceptronTagger , NltkTrainedTagger ): def __init__ ( self ,): NltkTrainedTagger . __init__ ( self ) _PerceptronTagger . __init__ ( self ,) def run ( self , input : Seq [ Word ]) -> Seq [ Postag ]: return NltkTrainedTagger . run ( self , input ) from nltk.tag.sequential import AffixTagger as _AffixTagger @nice_repr class AffixTagger ( NltkTagger ): def __init__ ( self , affix_length : DiscreteValue ( min = 2 , max = 6 ), min_stem_length : DiscreteValue ( min = 1 , max = 4 ), cutoff : DiscreteValue ( min = 0 , max = 10 ), backoff : algorithm ( Seq [ Seq [ Word ]], Supervised [ Seq [ Seq [ Postag ]]], Seq [ Seq [ Postag ]] ), ): self . affix_length = affix_length self . min_stem_length = min_stem_length self . cutoff = cutoff self . backoff = backoff self . tagger = _AffixTagger self . values = dict ( affix_length = affix_length , min_stem_length = min_stem_length , cutoff = cutoff , backoff = backoff , ) NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Postag ]]] ) -> Seq [ Seq [ Postag ]]: return NltkTagger . run ( self , input , y ) from nltk.tag.sequential import BigramTagger as _BigramTagger @nice_repr class BigramTagger ( NltkTagger ): def __init__ ( self , cutoff : DiscreteValue ( min = 0 , max = 10 ), ): self . cutoff = cutoff self . tagger = _BigramTagger self . values = dict ( cutoff = cutoff ) NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Postag ]]] ) -> Seq [ Seq [ Postag ]]: return NltkTagger . run ( self , input , y ) from nltk.tag.sequential import ClassifierBasedPOSTagger as _ClassifierBasedPOSTagger @nice_repr class ClassifierBasedPOSTagger ( NltkTagger ): def __init__ ( self ,): self . tagger = _ClassifierBasedPOSTagger self . values = dict () NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Postag ]]] ) -> Seq [ Seq [ Postag ]]: return NltkTagger . run ( self , input , y ) from nltk.tag.sequential import TrigramTagger as _TrigramTagger @nice_repr class TrigramTagger ( NltkTagger ): def __init__ ( self , cutoff : DiscreteValue ( min = 0 , max = 10 ), ): self . cutoff = cutoff self . tagger = _TrigramTagger self . values = dict ( cutoff = cutoff ) NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Postag ]]] ) -> Seq [ Seq [ Postag ]]: return NltkTagger . run ( self , input , y ) from nltk.tag.sequential import UnigramTagger as _UnigramTagger @nice_repr class UnigramTagger ( NltkTagger ): def __init__ ( self , cutoff : DiscreteValue ( min = 0 , max = 10 ), ): self . cutoff = cutoff self . tagger = _UnigramTagger self . values = dict ( cutoff = cutoff ) NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Postag ]]] ) -> Seq [ Seq [ Postag ]]: return NltkTagger . run ( self , input , y ) from nltk.tag.tnt import TnT as _TnT @nice_repr class TnT ( _TnT , NltkTrainedTagger ): def __init__ ( self , Trained : BooleanValue (), N : DiscreteValue ( min = 500 , max = 2000 ), C : BooleanValue (), ): self . Trained = Trained self . N = N self . C = C NltkTrainedTagger . __init__ ( self ) _TnT . __init__ ( self , Trained = Trained , N = N , C = C ) def run ( self , input : Seq [ Word ]) -> Seq [ Postag ]: return NltkTrainedTagger . run ( self , input ) from nltk.tokenize.casual import TweetTokenizer as _TweetTokenizer @nice_repr class TweetTokenizer ( _TweetTokenizer , NltkTokenizer ): def __init__ ( self , preserve_case : BooleanValue (), reduce_len : BooleanValue (), strip_handles : BooleanValue (), ): self . preserve_case = preserve_case self . reduce_len = reduce_len self . strip_handles = strip_handles NltkTokenizer . __init__ ( self ) _TweetTokenizer . __init__ ( self , preserve_case = preserve_case , reduce_len = reduce_len , strip_handles = strip_handles , ) def run ( self , input : Sentence ) -> Seq [ Word ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.mwe import MWETokenizer as _MWETokenizer @nice_repr class MWETokenizer ( _MWETokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _MWETokenizer . __init__ ( self ,) def run ( self , input : Sentence ) -> Seq [ Word ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.punkt import PunktSentenceTokenizer as _PunktSentenceTokenizer @nice_repr class PunktSentenceTokenizer ( _PunktSentenceTokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _PunktSentenceTokenizer . __init__ ( self ,) def run ( self , input : Document ) -> Seq [ Sentence ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.regexp import BlanklineTokenizer as _BlanklineTokenizer @nice_repr class BlanklineTokenizer ( _BlanklineTokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _BlanklineTokenizer . __init__ ( self ,) def run ( self , input : Document ) -> Seq [ Sentence ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.regexp import WhitespaceTokenizer as _WhitespaceTokenizer @nice_repr class WhitespaceTokenizer ( _WhitespaceTokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _WhitespaceTokenizer . __init__ ( self ,) def run ( self , input : Sentence ) -> Seq [ Word ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.regexp import WordPunctTokenizer as _WordPunctTokenizer @nice_repr class WordPunctTokenizer ( _WordPunctTokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _WordPunctTokenizer . __init__ ( self ,) def run ( self , input : Sentence ) -> Seq [ Word ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.sexpr import SExprTokenizer as _SExprTokenizer @nice_repr class SExprTokenizer ( _SExprTokenizer , NltkTokenizer ): def __init__ ( self , strict : BooleanValue ()): self . strict = strict NltkTokenizer . __init__ ( self ) _SExprTokenizer . __init__ ( self , strict = strict ) def run ( self , input : Document ) -> Seq [ Sentence ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.simple import LineTokenizer as _LineTokenizer @nice_repr class LineTokenizer ( _LineTokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _LineTokenizer . __init__ ( self ,) def run ( self , input : Document ) -> Seq [ Sentence ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.simple import SpaceTokenizer as _SpaceTokenizer @nice_repr class SpaceTokenizer ( _SpaceTokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _SpaceTokenizer . __init__ ( self ,) def run ( self , input : Sentence ) -> Seq [ Word ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.simple import TabTokenizer as _TabTokenizer @nice_repr class TabTokenizer ( _TabTokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _TabTokenizer . __init__ ( self ,) def run ( self , input : Document ) -> Seq [ Sentence ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.toktok import ToktokTokenizer as _ToktokTokenizer @nice_repr class ToktokTokenizer ( _ToktokTokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _ToktokTokenizer . __init__ ( self ,) def run ( self , input : Sentence ) -> Seq [ Word ]: return NltkTokenizer . run ( self , input ) from nltk.tokenize.treebank import TreebankWordTokenizer as _TreebankWordTokenizer @nice_repr class TreebankWordTokenizer ( _TreebankWordTokenizer , NltkTokenizer ): def __init__ ( self ,): NltkTokenizer . __init__ ( self ) _TreebankWordTokenizer . __init__ ( self ,) def run ( self , input : Sentence ) -> Seq [ Word ]: return NltkTokenizer . run ( self , input ) from nltk.tag.perceptron import PerceptronTagger as _PerceptronTagger @nice_repr class PerceptronTagger ( _PerceptronTagger , NltkTrainedTagger ): def __init__ ( self ,): NltkTrainedTagger . __init__ ( self ) _PerceptronTagger . __init__ ( self ,) def run ( self , input : Seq [ Word ]) -> Seq [ Postag ]: return NltkTrainedTagger . run ( self , input ) from nltk.tag.sequential import BigramTagger as _BigramTagger @nice_repr class BigramTagger ( NltkTagger ): def __init__ ( self , cutoff : DiscreteValue ( min = 0 , max = 10 ), ): self . cutoff = cutoff self . tagger = _BigramTagger self . values = dict ( cutoff = cutoff ) NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Postag ]]] ) -> Seq [ Seq [ Postag ]]: return NltkTagger . run ( self , input , y ) from nltk.tag.sequential import ClassifierBasedPOSTagger as _ClassifierBasedPOSTagger @nice_repr class ClassifierBasedPOSTagger ( NltkTagger ): def __init__ ( self ,): self . tagger = _ClassifierBasedPOSTagger self . values = dict () NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Postag ]]] ) -> Seq [ Seq [ Postag ]]: return NltkTagger . run ( self , input , y ) from nltk.tag.sequential import TrigramTagger as _TrigramTagger @nice_repr class TrigramTagger ( NltkTagger ): def __init__ ( self , cutoff : DiscreteValue ( min = 0 , max = 10 ), ): self . cutoff = cutoff self . tagger = _TrigramTagger self . values = dict ( cutoff = cutoff ) NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Postag ]]] ) -> Seq [ Seq [ Postag ]]: return NltkTagger . run ( self , input , y ) from nltk.tag.sequential import UnigramTagger as _UnigramTagger @nice_repr class UnigramTagger ( NltkTagger ): def __init__ ( self , cutoff : DiscreteValue ( min = 0 , max = 10 ), ): self . cutoff = cutoff self . tagger = _UnigramTagger self . values = dict ( cutoff = cutoff ) NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Postag ]]] ) -> Seq [ Seq [ Postag ]]: return NltkTagger . run ( self , input , y ) from nltk.tag.tnt import TnT as _TnT @nice_repr class TnT ( _TnT , NltkTrainedTagger ): def __init__ ( self , Trained : BooleanValue (), N : DiscreteValue ( min = 500 , max = 2000 ), C : BooleanValue (), ): self . Trained = Trained self . N = N self . C = C NltkTrainedTagger . __init__ ( self ) _TnT . __init__ ( self , Trained = Trained , N = N , C = C ) def run ( self , input : Seq [ Word ]) -> Seq [ Postag ]: return NltkTrainedTagger . run ( self , input ) __all__ = [ \"Cistem\" , \"ISRIStemmer\" , \"LancasterStemmer\" , \"PorterStemmer\" , \"RSLPStemmer\" , \"SnowballStemmer\" , \"WordNetLemmatizer\" , \"PerceptronTagger\" , \"AffixTagger\" , \"BigramTagger\" , \"ClassifierBasedPOSTagger\" , \"TrigramTagger\" , \"UnigramTagger\" , \"TweetTokenizer\" , \"MWETokenizer\" , \"PunktSentenceTokenizer\" , \"BlanklineTokenizer\" , \"WhitespaceTokenizer\" , \"WordPunctTokenizer\" , \"SExprTokenizer\" , \"LineTokenizer\" , \"SpaceTokenizer\" , \"TabTokenizer\" , \"ToktokTokenizer\" , \"TreebankWordTokenizer\" , \"PerceptronTagger\" , \"BigramTagger\" , \"ClassifierBasedPOSTagger\" , \"TrigramTagger\" , \"UnigramTagger\" , \"TnT\" , ]","title":"Autogoal.contrib.nltk. generated"},{"location":"api/autogoal.contrib.nltk._manual/","text":"import nltk from gensim.models.doc2vec import Doc2Vec as _Doc2Vec from numpy import inf , nan from autogoal.contrib.nltk._builder import NltkTokenizer , NltkTagger from autogoal.contrib.sklearn._builder import SklearnTransformer , SklearnWrapper from autogoal.grammar import ( BooleanValue , CategoricalValue , ContinuousValue , DiscreteValue , ) from autogoal.kb import * from autogoal.utils import nice_repr from autogoal.kb import AlgorithmBase @nice_repr class Doc2Vec ( _Doc2Vec , SklearnTransformer ): def __init__ ( self , dm : DiscreteValue ( min = 0 , max = 2 ), dbow_words : DiscreteValue ( min =- 100 , max = 100 ), dm_concat : DiscreteValue ( min =- 100 , max = 100 ), dm_tag_count : DiscreteValue ( min = 0 , max = 2 ), alpha : ContinuousValue ( min = 0.001 , max = 0.075 ), epochs : DiscreteValue ( min = 2 , max = 10 ), window : DiscreteValue ( min = 2 , max = 10 ), inner_tokenizer : algorithm ( Sentence , Seq [ Word ]), inner_stemmer : algorithm ( Word , Stem ), inner_stopwords : algorithm ( Seq [ Word ], Seq [ Word ]), lowercase : BooleanValue (), stopwords_remove : BooleanValue (), ): self . inner_tokenizer = inner_tokenizer self . inner_stemmer = inner_stemmer self . inner_stopwords = inner_stopwords self . lowercase = lowercase self . stopwords_remove = stopwords_remove super () . __init__ ( dm = dm , dbow_words = dbow_words , dm_concat = dm_concat , dm_tag_count = dm_tag_count , alpha = alpha , epochs = epochs , window = window , ) def tokenize ( self , sentence ): sentence = sentence . lower () if self . lowercase else sentence tokens = self . inner_tokenizer . run ( sentence ) tokens = self . inner_stopwords . run ( sentence ) if self . stopwords_remove else tokens return [ self . inner_stemmer . run ( token ) for token in tokens ] def fit_transform ( self , X , y = None ): self . fit ( X , y ) return self . transform ( X ) def fit ( self , X , y ): Data must be turned to tagged data as TaggedDocument(Seq[Token), Tag) Tag use to be an unique integer from gensim.models.doc2vec import TaggedDocument as _TaggedDocument tagged_data = [ _TaggedDocument ( self . tokenize ( X [ i ]), str ( i )) for i in range ( len ( X )) ] self . build_vocab ( tagged_data ) return self . train ( tagged_data , total_examples = self . corpus_count , epochs = self . epochs ) def transform ( self , X , y = None ): return [ self . infer_vector ( x ) for x in X ] def run ( self , input : Seq [ Sentence ]) -> MatrixContinuousDense : Note \"\"\"This methods receive a document list and transform this into a dense continuous matrix. return SklearnTransformer . run ( self , input ) @nice_repr class StopwordRemover ( SklearnTransformer ): def __init__ ( self , language : CategoricalValue ( \"danish\" , \"dutch\" , \"english\" , \"finnish\" , \"french\" , \"german\" , \"hungarian\" , \"italian\" , \"norwegian\" , \"portuguese\" , \"russian\" , \"spanish\" , \"swedish\" , \"turkish\" , ), ): self . language = language from nltk.corpus import stopwords self . words = stopwords . words ( language ) SklearnTransformer . __init__ ( self ) def fit_transform ( self , X , y = None ): return [ word for word in input if word not in self . words ] def transform ( self , X , y = None ): return self . fit_transform ( X , y ) def run ( self , input : Seq [ Word ]) -> Seq [ Word ]: Note \"\"\"This methods receive a word list list and transform this into a word list list without stopwords. return SklearnTransformer . run ( self , input ) @nice_repr class TextLowerer ( SklearnTransformer ): def fit_transform ( self , X , y = None ): self . fit ( X , y = None ) return self . transform ( X ) def transform ( self , X , y = None ): Considering data as list of raw documents return [ str . lower ( x ) for x in X ] @nice_repr class WordnetConcept ( AlgorithmBase ): Note \"\"\"Find a word in Wordnet and return a List of Synset de Wordnet def __init__ ( self ): from nltk.corpus import wordnet self . wordnet = wordnet def run ( self , input : Word ) -> Seq [ Synset ]: Note \"\"\"Find a word in Wordnet and return a List of Synset de Wordnet synsets = self . wordnet . synsets ( input ) names_synsets = [] for i in synsets : names_synsets . append ( i . name ()) return names_synsets @nice_repr class ConvertSynset2Word: \"\"\"Recive a Synset of nltk and return de Lemma of this \"\"\" def __init__ ( self ) : pass def run ( self , input : Synset ( domain = \"general\" , language = \"english\" )) -> Word () : \"\"\"Recive a Synset of nltk and return de Lemma of this \"\"\" return Lemma ( input ) @nice_repr class SentimentWord ( AlgorithmBase ): Note \"\"\"Find a word in SentiWordnet and return a List of sentiment of the word. def __init__ ( self ): from nltk.corpus import sentiwordnet self . swn = sentiwordnet def run ( self , input : Synset ) -> Sentiment : Note \"\"\"Find a word in SentiWordnet and return a List of sentiment of the word. swn_synset = self . swn . senti_synset ( input ) sentiment = {} sentiment [ \"positive\" ] = swn_synset . pos_score () sentiment [ \"negative\" ] = swn_synset . neg_score () return sentiment from nltk.chunk.named_entity import NEChunkParserTagger as _NEChunkParserTagger @nice_repr class NEChunkParserTagger ( NltkTagger ): def __init__ ( self ,): self . tagger = _NEChunkParserTagger self . values = dict () NltkTagger . __init__ ( self ) def run ( self , input : Seq [ Postag ]) -> Seq [ Chunktag ]: return NltkTagger . run ( self , input ) @nice_repr class GlobalChunker ( SklearnWrapper ): def __init__ ( self , inner_trained_pos_tagger : algorithm ( Seq [ Word ], Seq [ Postag ]), inner_chunker : algorithm ( Seq [ Postag ], Seq [ Chunktag ]), ): self . inner_trained_pos_tagger = inner_trained_pos_tagger self . inner_chunker = inner_chunker SklearnWrapper . __init__ ( self ) def _train ( self , X , y ): postagged_sentences = [] for i in range ( len ( X )): sentence = [] for itoken in range ( len ( X [ i ])): x_sent = X [ i ] word = x_sent [ itoken ] sentence . append ( word ) postag_sentence = self . inner_trained_pos_tagger . run (( sentence , sentence ))[ 0 ] tagged_sentence = [ ( sentence [ k ], postag_sentence [ k ][ 1 ]) for k in range ( len ( sentence )) ] if tagged_sentence : postagged_sentences . append ( tagged_sentence ) tagged_sentences = [] for i in range ( len ( y )): sentence = [] tags = [] for itoken in range ( len ( y [ i ])): y_sent = y [ i ] word , tag = y_sent [ itoken ] sentence . append ( word ) tags . append ( tag ) postag_sentence = self . inner_trained_pos_tagger . run (( sentence , sentence ))[ 0 ] tagged_sentence = [ (( sentence [ k ], postag_sentence [ k ][ 1 ]), tags [ k ]) for k in range ( len ( sentence )) ] if tagged_sentence : tagged_sentences . append ( tagged_sentence ) return self . inner_chunker . run (( postagged_sentences , tagged_sentences )) def _eval ( self , X , y = None ): postagged_document = [] for i in range ( len ( X )): sentence = [] for itoken in range ( len ( X [ i ])): x_sent = X [ i ] word = x_sent [ itoken ] sentence . append ( word ) postag_sentence = self . inner_trained_pos_tagger . run (( sentence , sentence ))[ 0 ] tagged_sentence = [ ( sentence [ k ], postag_sentence [ k ][ 1 ]) for k in range ( len ( sentence )) ] if tagged_sentence : postagged_document . append ( tagged_sentence ) return self . inner_chunker . run (( postagged_document , y )) def run ( self , X : Seq [ Seq [ Word ]], y : Supervised [ Seq [ Seq [ Chunktag ]]] ) -> Seq [ Chunktag ]: return SklearnWrapper . run ( self , input ) @nice_repr class FeatureSeqExtractor ( AlgorithmBase ): Note A simple feature extractor for tokenized sentences based on NLTK. It receives a tokenized sentence (i.e., a list of str) and outputs a list of syntactic features. For each word in the sentence, it builds a dictionary of features of that word, plus features of surrounding words in a pre-defined window (of size 1 to 5). Parameters extract_word : whether to extract words as features. window_size : size of the window around the current word to also look for features. ```python extractor = FeatureSeqExtractor(window_size=2) extractor.run([\"Hello\", \"World\"]) [{'word': 'Hello', 'word+1': 'World'}, {'word': 'World', 'word-1': 'Hello'}] ``` def __init__ ( self , extract_word : BooleanValue () = True , window_size : DiscreteValue ( 0 , 5 ) = 0 , ): self . extract_word = extract_word self . window_size = window_size def extract_features ( self , w ): features = {} if self . extract_word : features [ \"word\" ] = w return features def run ( self , sentence : Seq [ Word ]) -> Seq [ FeatureSet ]: features = [] for w in sentence : features . append ( self . extract_features ( w )) expanded_features = [] for i , f in enumerate ( features ): expanded = dict ( f ) for j in range ( i - self . window_size , i + self . window_size + 1 ): if j == i : continue ff = features [ j ] if 0 <= j < len ( features ) else {} idx = f \"+ { j - i } \" if j > i else str ( j - i ) for k , v in ff . items (): expanded [ k + idx ] = v expanded_features . append ( expanded ) return expanded_features __all__ = [ \"Doc2Vec\" , \"StopwordRemover\" , \"TextLowerer\" , \"WordnetConcept\" , \"SentimentWord\" , \"NEChunkParserTagger\" , \"GlobalChunker\" , \"FeatureSeqExtractor\" , ]","title":"Autogoal.contrib.nltk. manual"},{"location":"api/autogoal.contrib.nltk._utils/","text":"import warnings import inspect import re import numpy as np import scipy.sparse as sp from autogoal import kb from autogoal.contrib.sklearn._utils import ( is_matrix_continuous_dense , is_matrix_continuous_sparse , is_categorical , is_continuous , is_string_list , ) DATA_TYPE_EXAMPLES = { kb . Postag : ( \"lorem\" , \"ipsum\" ), # (str, str) Tagged token kb . Seq [ kb . Postag ]: [( \"lorem\" , \"ipsum\" )] * 10 , # [(str, str), (str, str)] List of tagged tokens kb . Seq [ kb . Seq [ kb . Postag ]]: [ [( \"lorem\" , \"ipsum\" )] * 2 ], # [[(str, str), (str, str)], [(str, str), (str, str)]] List of Tagged Sentences kb . Chunktag : (( \"lorem\" , \"ipsum\" ), \"ipsum\" ), # ((str, str), str) IOB Tagged token kb . Seq [ kb . Chunktag ]: [(( \"lorem\" , \"ipsum\" ), \"ipsum\" )] * 10 , # [((str, str), str), ((str, str), str)] List of IOB Tagged token kb . Seq [ kb . Seq [ kb . Chunktag ]]: [ [(( \"lorem\" , \"ipsum\" ), \"ipsum\" )] * 2 ], # [[((str, str), str), ((str, str), str)], [((str, str), str), ((str, str), str)]] List of IOB Tagged Sentences kb . Stem : \"ips\" , kb . Word : \"ipsum\" , kb . Sentence : \"It is the best of all movies.\" , kb . Document : \"It is the best of all movies. I actually love that action scene.\" , kb . MatrixContinuousDense : np . random . rand ( 10 , 10 ), kb . MatrixContinuousSparse : sp . rand ( 10 , 10 ), kb . VectorCategorical : np . asarray ([ \"A\" ] * 5 + [ \"B\" ] * 5 ), kb . VectorContinuous : np . random . rand ( 10 ), kb . VectorDiscrete : np . random . randint ( 0 , 10 , ( 10 ,), dtype = int ), kb . Seq [ kb . Word ]: [ \"ipsu\" , \"lorem\" ], kb . Seq [ kb . Document ]: [ \"abc ipsu lorem say hello\" , \"ipsum lorem\" , \"abc\" ] * 2 , kb . Seq [ kb . Seq [ kb . Stem ]]: [[ \"abc\" , \"ipsu\" , \"lorem\" ] * 10 ], kb . Seq [ kb . Seq [ kb . Word ]]: [[ \"abc\" , \"ipsu\" , \"lorem\" ] * 10 ], kb . Seq [ kb . Seq [ kb . Sentence ]]: [[ \"abc a sentence lorem\" ], [ \"ipsum lorem\" ], [ \"abc\" ]], } def is_algorithm ( cls , verbose = False ): if _is_classifier ( cls ): return \"classifier\" if _is_clusterer ( cls ): return \"clusterer\" if _is_sent_tokenizer ( cls ): return \"sent_tokenizer\" if _is_word_tokenizer ( cls ): return \"word_tokenizer\" if _is_lemmatizer ( cls ): return \"lemmatizer\" if _is_stemmer ( cls ): return \"stemmer\" if _is_word_embbeder ( cls ): return \"word_embbeder\" if _is_doc_embbeder ( cls ): return \"doc_embbeder\" if is_pretrained_tagger ( cls )[ 0 ]: return \"trained_tagger\" if _is_tagger ( cls ): return \"tagger\" return False def _is_algorithm ( cls , verbose = False ): return ( _is_stemmer ( cls , verbose ) or _is_lemmatizer ( cls , verbose ) or _is_word_tokenizer ( cls , verbose ) or _is_sent_tokenizer ( cls , verbose ) or _is_clusterer ( cls , verbose ) or _is_classifier ( cls , verbose ) or _is_word_embbeder ( cls , verbose ) or _is_doc_embbeder ( cls , verbose ) or _is_tagger ( cls , verbose ) ) def _is_stemmer ( cls , verbose = False ): if hasattr ( cls , \"stem\" ): return True return False def _is_lemmatizer ( cls , verbose = False ): if hasattr ( cls , \"lemmatize\" ): return True return False def _is_word_tokenizer ( cls , verbose = False ): if hasattr ( cls , \"tokenize\" ) or hasattr ( cls , \"word_tokenize\" ): return True return False def _is_sent_tokenizer ( cls , verbose = False ): if hasattr ( cls , \"sent_tokenize\" ) or ( hasattr ( cls , \"tokenize\" )): return True return False def _is_clusterer ( cls , verbose = False ): if hasattr ( cls , \"classify\" ) and hasattr ( cls , \"cluster\" ): return True return False def _is_classifier ( cls , verbose = False ): if hasattr ( cls , \"classify\" ) and hasattr ( cls , \"train\" ): return True return False def _is_word_embbeder ( cls , verbose = False ): if hasattr ( cls , \"build_vocab\" ) and hasattr ( cls , \"train\" ) and hasattr ( cls , \"wv\" ): return True return False def _is_doc_embbeder ( cls , verbose = False ): if ( hasattr ( cls , \"build_vocab\" ) and hasattr ( cls , \"train\" ) and hasattr ( cls , \"infer_vector\" ) ): return True return False def _is_tagger ( cls , verbose = False ): if hasattr ( cls , \"tag\" ): return True return False def _is_trained_tagger ( cls , verbose = False ): if hasattr ( cls , \"train\" ) and _is_tagger ( cls , verbose ): return True return False def _is_chunker ( cls , verbose = False ): if hasattr ( cls , \"parse\" ): return True return False def is_stemmer ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an nltk stemmer. If True, returns the valid (input, output) types. Examples: from sklearn.linear_model import LogisticRegression from nltk.stem import Cistem is_stemmer(Cistem) (True, (Word, Stem)) is_stemmer(LogisticRegression) (False, None) if not _is_stemmer ( cls , verbose = verbose ): return False , None inputs = [] output = kb . Stem for input_type in [ kb . Word ]: try : X = DATA_TYPE_EXAMPLES [ input_type ] stemmer = cls () y = stemmer . stem ( X ) assert DATA_RESOLVERS [ output ]( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , output ) else : return False , None def is_lemmatizer ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an nltk lemmatizer. If True, returns the valid (input, output) types. Examples: from sklearn.linear_model import LogisticRegression from nltk.stem import WordNetLemmatizer is_lemmatizer(WordNetLemmatizer) (True, (Word, Stem)) is_lemmatizer(LogisticRegression) (False, None) if not _is_lemmatizer ( cls , verbose = verbose ): return False , None inputs = [] output = kb . Stem for input_type in [ kb . Word ]: try : X = DATA_TYPE_EXAMPLES [ input_type ] lemmatizer = cls () y = lemmatizer . lemmatize ( X ) assert DATA_RESOLVERS [ output ]( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , output ) else : return False , None def is_word_tokenizer ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an nltk word tokenizer algorithm. If True, returns the valid (input, output) types. Examples: from sklearn.linear_model import LogisticRegression from nltk.tokenize import TweetTokenizer is_word_tokenizer(TweetTokenizer) (True, (Sentence, List(Word))) is_word_tokenizer(LogisticRegression) (False, None) if not _is_word_tokenizer ( cls , verbose = verbose ): return False , None inputs = [] output = kb . Seq [ kb . Word ] for input_type in [ kb . Sentence ]: try : X = DATA_TYPE_EXAMPLES [ input_type ] tokenizer = cls () y = tokenizer . tokenize ( X ) assert DATA_RESOLVERS [ output ]( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , output ) else : return False , None def is_sent_tokenizer ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an nltk sentence tokenizer. If True, returns the valid (input, output) types. Examples: from sklearn.linear_model import LogisticRegression from nltk.tokenize import PunktSentenceTokenizer is_sent_tokenizer(PunktSentenceTokenizer) (True, (Document, List(Sentence))) is_sent_tokenizer(LogisticRegression) (False, None) if not _is_sent_tokenizer ( cls , verbose = verbose ): return False , None inputs = [] output = kb . Seq ( kb . Sentence ) for input_type in [ kb . Document ]: try : X = DATA_TYPE_EXAMPLES [ input_type ] tokenizer = cls () y = tokenizer . tokenize ( X ) assert DATA_RESOLVERS [ output ]( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , output ) else : return False , None def is_clusterer ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an nltk clusterer. If True, returns the valid (input, output) types. Examples: from sklearn.linear_model import LogisticRegression from nltk.cluster import GAAClusterer is_clusterer(GAAClusterer) (True, (MatrixContinuousDense(), CategoricalVector())) is_clusterer(LogisticRegression) (False, None) if not _is_clusterer ( cls , verbose = verbose ): return False , None inputs = [] for input_type in [ kb . MatrixContinuousDense (), kb . MatrixContinuousSparse ()]: try : X = DATA_TYPE_EXAMPLES [ input_type ] y = DATA_TYPE_EXAMPLES [ kb . CategoricalVector ()] clusterer = cls () clusterer . cluster ( X ) y = [ clusterer . classify ( x ) for x in X ] assert is_categorical ( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , kb . CategoricalVector ()) else : return False , None def is_classifier ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an nltk classifier. If True, returns the valid (input, output) types. if not _is_algorithm ( cls , verbose = verbose ): return False , None inputs = [] TODO: Fix somehow compatibility with nltk classifiers inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , kb . ContinuousVector ()) else : return False , None def is_tagger ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an nltk pos tagger. If True, returns the valid (input, output) types. Examples: from nltk.tag import AffixTagger from nltk.tokenize import PunktSentenceTokenizer is_tagger(AffixTagger) (True, (List(List(Word)), List(List(Postag())))) is_tagger(LogisticRegression) (False, None) if not _is_tagger ( cls , verbose = verbose ): return False , None inputs = [] output = kb . Seq ( kb . Seq [ kb . Postag ]) for input_type in [ kb . Seq ( kb . Seq [ kb . Word ])]: try : X = DATA_TYPE_EXAMPLES [ input_type ] X_train = [[( word , word ) for word in sentence ] for sentence in X ] tagger = cls ( train = X_train ) tagger = cls() y = tagger . tag_sents ( X ) assert DATA_RESOLVERS [ output ]( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , output ) else : is_ptt = is_pretrained_tagger ( cls , verbose ) is_ckr = is_chunker ( cls , verbose ) if is_ptt [ 0 ]: return is_ptt if is_ckr [ 0 ]: return is_ckr return False , None def is_chunker ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an nltk chunker. If True, returns the valid (input, output) types. Examples: from nltk.chunk.named_entity import NEChunkParserTagger from nltk.tokenize import PunktSentenceTokenizer is_chunker(NEChunkParserTagger) (True, (List(List(Tuple(Word, Word))), List(List(Tuple(Tuple(Word, Word), Word))))) is_chunker(PunktSentenceTokenizer) (False, None) if not _is_tagger ( cls , verbose = verbose ): return False , None inputs = [] output = kb . Seq ( kb . Seq [ kb . Chunktag ]) for input_type in [ kb . Seq ( kb . Seq [ kb . Postag ])]: try : X = DATA_TYPE_EXAMPLES [ input_type ] X_train = [ [(( word , postag ), postag ) for word , postag in sentence ] for sentence in X ] chunker = cls ( train = X_train ) y = chunker . tag_sents ( X ) assert DATA_RESOLVERS [ output ]( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , output ) else : return False , None def is_pretrained_tagger ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an nltk sentence tokenizer. If True, returns the valid (input, output) types. Examples: from nltk.tag import AffixTagger from nltk.tag.perceptron import PerceptronTagger is_pretrained_tagger(PerceptronTagger) (True, (List(Word), List(Tuple(Word, Word)))) is_pretrained_tagger(AffixTagger) (False, None) if not _is_tagger ( cls , verbose = verbose ): return False , None inputs = [] output = kb . Seq [ kb . Postag ] for input_type in [ kb . Seq [ kb . Word ]]: try : X = DATA_TYPE_EXAMPLES [ input_type ] tagger = cls () y = tagger . tag ( X ) assert DATA_RESOLVERS [ output ]( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , output ) else : return False , None def is_data_type ( X , data_type ): return DATA_RESOLVERS [ data_type ]( X ) IO_TYPE_HANDLER = [ is_stemmer , is_lemmatizer , is_word_tokenizer , is_sent_tokenizer , is_clusterer , is_classifier , is_tagger , is_word_embbeder, is_doc_embbeder ] def get_input_output ( cls , verbose = False ): for func in IO_TYPE_HANDLER : matches , types = func ( cls , verbose = verbose ) if matches : return types return None , None def combine_types ( * types ): if len ( types ) == 1 : return types [ 0 ] types = set ( types ) if types == { kb . MatrixContinuousDense (), kb . MatrixContinuousSparse ()}: return kb . MatrixContinuous () return None def is_word_list ( obj ): Note \"\"\"Determines if obj is a sequence of sequence of strings. Examples: is_word_list(['hello', 'world']) True is_word_list(np.random.rand(10)) False try : oset = set () for word in obj : if len ( word . split ()) > 1 : return False oset . add ( word ) return len ( oset ) > 0.1 * len ( obj ) and all ( isinstance ( x , str ) for x in oset ) except : return False def is_word_list_list ( obj ): Note \"\"\"Determines if obj is a sequence of sequence of strings. Examples: is_word_list_list([['hello'], ['another'], ['word']]) True is_word_list_list(np.random.rand(10)) False try : oset = set () for sent in obj : for word in sent : if len ( word . split ()) > 1 : return False oset . add ( word ) return len ( oset ) > 0.1 * len ( obj ) and all ( isinstance ( x , str ) for x in oset ) except : return False def is_word ( obj ): Note \"\"\"Determines if obj is a sequence of sequence of strings. Examples: is_word('hello') True is_word(np.random.rand(10)) False try : return isinstance ( obj , str ) and len ( obj . split ()) == 1 except : return False def is_sentence ( obj ): Note \"\"\"Determines if obj is a sentence strings. Examples: is_sentence('hello world') True is_word(np.random.rand(10)) False try : return isinstance ( obj , str ) and len ( obj . split ()) > 1 except : return False def is_sentence_list ( obj ): try : return all ([ is_sentence ( x ) for x in obj ]) except : return False def is_text_list_list ( obj ): Note \"\"\"Determines if obj is a sequence of sequence of strings. Examples: is_text_list_list([['hello', 'world'], ['another'], ['sentence']]) True is_text_list_list(np.random.rand(10)) False try : oset = set () for sent in obj : for text in sent : oset . add ( text ) return len ( oset ) > 0.1 * len ( obj ) and all ( isinstance ( x , str ) for x in oset ) except : return False def is_tag ( obj ): Note \"\"\"Determines if obj is a tuple of two strings. Examples: is_tag(('hello', 'yes')) True is_tag(('hi', 22)) False try : return ( isinstance ( obj , tuple ) and len ( obj ) == 2 and all (( isinstance ( x , str ) or x == None ) for x in obj ) ) except : return False def is_tag_list ( obj ): Note \"\"\"Determines if obj is a list of tuple of two strings. Examples: is_tag_list([('hello', 'yes'), ('how', 'is')]) True is_tag_list(('hello', 'yes')) False try : return isinstance ( obj , list ) and all ( is_tag ( x ) for x in obj ) except : return False def is_tagged_sentence_list ( obj ): Note \"\"\"Determines if obj is a list(list(tuple(str, str))) Examples: is_tagged_sentence_list([[('hello', 'yes'), ('how', 'is')]]) True is_tagged_sentence_list([('hello', 'yes')]) False try : return isinstance ( obj , list ) and all ( is_tag_list ( x ) for x in obj ) except : return False def is_chunk ( obj ): Note \"\"\"Determines if obj is a tuple(tuple(str, str), str). Examples: is_chunk((('hello', 'yes'),'how')) True is_chunk(('hello', 'yes')) False try : return ( isinstance ( obj , tuple ) and len ( obj ) == 2 and is_tag ( obj [ 0 ]) and isinstance ( obj [ 1 ], str ) ) except : return False def is_chunk_list ( obj ): Note \"\"\"Determines if obj is a list(tuple(tuple(str, str), str)). Examples: is_chunk_list([(('hello', 'yes'),'how'), (('are', 'you'), 'today')]) True is_chunk_list([('hello', 'yes'), ('how', 'are')]) False try : return isinstance ( obj , list ) and all ( is_chunk ( x ) for x in obj ) except : return False def is_chunked_sentence_list ( obj ): Note \"\"\"Determines if obj is a list(list(tuple(tuple(str, str), str))). Examples: is_chunked_sentence_list([[(('hello', 'yes'),'how'), (('are', 'you'), 'today')]]) True is_chunked_sentence_list([('hello', 'yes'), ('how', 'are')]) False try : return isinstance ( obj , list ) and all ( is_chunk_list ( x ) for x in obj ) except : return False DATA_RESOLVERS = { kb . Postag : is_tag , kb . Seq [ kb . Postag ]: is_tag_list , kb . Seq [ kb . Seq [ kb . Postag ]]: is_tagged_sentence_list , kb . Chunktag : is_chunk , kb . Seq [ kb . Chunktag ]: is_chunk_list , kb . Seq [ kb . Seq [ kb . Chunktag ]]: is_chunked_sentence_list , kb . Stem : is_word , kb . Word : is_word , kb . Sentence : is_sentence , kb . Document : is_sentence , kb . MatrixContinuousDense : is_matrix_continuous_dense , kb . MatrixContinuousSparse : is_matrix_continuous_sparse , kb . VectorCategorical : is_categorical , kb . VectorContinuous : is_continuous , kb . Seq [ kb . Word ]: is_word_list , kb . Seq [ kb . Sentence ]: is_sentence_list , kb . Seq [ kb . Seq [ kb . Stem ]]: is_word_list_list , kb . Seq [ kb . Seq [ kb . Word ]]: is_word_list_list , }","title":"Autogoal.contrib.nltk. utils"},{"location":"api/autogoal.contrib.regex.__init__/","text":"import re import abc from autogoal.utils import nice_repr from autogoal.kb import Word , FeatureSet from autogoal.grammar import BooleanValue from autogoal.kb import AlgorithmBase class _Regex ( AlgorithmBase ): def __init__ ( self , full : BooleanValue ): self . full = full self . _name = self . __class__ . __name__ [: - len ( \"Regex\" )] . lower () @abc . abstractmethod def _regex ( self ): pass def run ( self , input : Word ) -> FeatureSet : r_exp = self . _regex () b = re . fullmatch ( r_exp , input ) if self . full else re . search ( r_exp , input ) return { f \"is_ { self . _name } _regex\" : bool ( b )} @nice_repr class UrlRegex ( _Regex ): Note Finds if a URL is contained inside a word using regular expressions. Examples \u00b6 ```python regex = UrlRegex(full=True) regex.run(\"https://autogoal.gitlab.io/autogoal/contributing/#license\") {'is_url_regex': True} regex = UrlRegex(full=True) regex.run(\"There is a URL at https://autogoal.gitlab.io/autogoal/contributing/#license, who would know?\") {'is_url_regex': False} regex = UrlRegex(full=False) regex.run(\"There is a URL at https://autogoal.gitlab.io/autogoal/contributing/#license, who would know?\") {'is_url_regex': True} ``` def _regex ( self ): return r \"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\" @nice_repr class IPRegex ( _Regex ): Note Finds if an IP-address is contained inside a word using regular expressions. Examples \u00b6 ```python regex = IPRegex(full=True) regex.run(\"192.168.18.1\") {'is_ip_regex': True} regex = IPRegex(full=True) regex.run(\"There is an IP at 192.168.18.1, who would know?\") {'is_ip_regex': False} regex = IPRegex(full=False) regex.run(\"There is an IP at 192.168.18.1, who would know?\") {'is_ip_regex': True} ``` def _regex ( self ): return r \"\\b((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.)?) {4} \\b\" @nice_repr class MACRegex ( _Regex ): Note Finds if a MAC-address is contained inside a word using regular expressions. Examples \u00b6 ```python regex = MACRegex(full=True) regex.run(\"3D:F2:C9:A6:B3:4F\") {'is_mac_regex': True} regex = MACRegex(full=True) regex.run(\"There is an IP at 3D-F2-C9-A6-B3-4F, who would know?\") {'is_mac_regex': False} regex = MACRegex(full=False) regex.run(\"There is an IP at 3D:F2:C9:A6:B3:4F, who would know?\") {'is_mac_regex': True} ``` def _regex ( self ): return r \"([0-9A-Fa-f] {2} [:-]) {5} ([0-9A-Fa-f] {2} )\" @nice_repr class EmailRegex ( _Regex ): Note Finds if an email is contained inside a word using regular expressions. Examples \u00b6 ```python regex = EmailRegex(full=True) regex.run(\"someone@example.com\") {'is_email_regex': True} regex = EmailRegex(full=True) regex.run(\"There is an email at someone@example.com, who would know?\") {'is_email_regex': False} regex = EmailRegex(full=False) regex.run(\"There is an email at someone@example.com, who would know?\") {'is_email_regex': True} ``` def _regex ( self ): return r \"([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})\" @nice_repr class PhoneRegex ( _Regex ): Note Finds if a phone number is contained inside a word using regular expressions. Examples \u00b6 ```python regex = phoneRegex(full=True) regex.run(\"+619123456789\") {'is_phone_regex': True} regex = phoneRegex(full=True) regex.run(\"There is an phone at +619123456789, who would know?\") {'is_phone_regex': False} regex = phoneRegex(full=False) regex.run(\"There is an phone at +619123456789, who would know?\") {'is_phone_regex': True} ``` def _regex ( self ): return r \"^((\\+) {1} 91) {1} [1-9] {1} [0-9] {9} $\"","title":"Autogoal.contrib.regex.  init  "},{"location":"api/autogoal.contrib.regex.__init__/#examples","text":"```python regex = UrlRegex(full=True) regex.run(\"https://autogoal.gitlab.io/autogoal/contributing/#license\") {'is_url_regex': True} regex = UrlRegex(full=True) regex.run(\"There is a URL at https://autogoal.gitlab.io/autogoal/contributing/#license, who would know?\") {'is_url_regex': False} regex = UrlRegex(full=False) regex.run(\"There is a URL at https://autogoal.gitlab.io/autogoal/contributing/#license, who would know?\") {'is_url_regex': True} ``` def _regex ( self ): return r \"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\" @nice_repr class IPRegex ( _Regex ): Note Finds if an IP-address is contained inside a word using regular expressions.","title":"Examples"},{"location":"api/autogoal.contrib.regex.__init__/#examples_1","text":"```python regex = IPRegex(full=True) regex.run(\"192.168.18.1\") {'is_ip_regex': True} regex = IPRegex(full=True) regex.run(\"There is an IP at 192.168.18.1, who would know?\") {'is_ip_regex': False} regex = IPRegex(full=False) regex.run(\"There is an IP at 192.168.18.1, who would know?\") {'is_ip_regex': True} ``` def _regex ( self ): return r \"\\b((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.)?) {4} \\b\" @nice_repr class MACRegex ( _Regex ): Note Finds if a MAC-address is contained inside a word using regular expressions.","title":"Examples"},{"location":"api/autogoal.contrib.regex.__init__/#examples_2","text":"```python regex = MACRegex(full=True) regex.run(\"3D:F2:C9:A6:B3:4F\") {'is_mac_regex': True} regex = MACRegex(full=True) regex.run(\"There is an IP at 3D-F2-C9-A6-B3-4F, who would know?\") {'is_mac_regex': False} regex = MACRegex(full=False) regex.run(\"There is an IP at 3D:F2:C9:A6:B3:4F, who would know?\") {'is_mac_regex': True} ``` def _regex ( self ): return r \"([0-9A-Fa-f] {2} [:-]) {5} ([0-9A-Fa-f] {2} )\" @nice_repr class EmailRegex ( _Regex ): Note Finds if an email is contained inside a word using regular expressions.","title":"Examples"},{"location":"api/autogoal.contrib.regex.__init__/#examples_3","text":"```python regex = EmailRegex(full=True) regex.run(\"someone@example.com\") {'is_email_regex': True} regex = EmailRegex(full=True) regex.run(\"There is an email at someone@example.com, who would know?\") {'is_email_regex': False} regex = EmailRegex(full=False) regex.run(\"There is an email at someone@example.com, who would know?\") {'is_email_regex': True} ``` def _regex ( self ): return r \"([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})\" @nice_repr class PhoneRegex ( _Regex ): Note Finds if a phone number is contained inside a word using regular expressions.","title":"Examples"},{"location":"api/autogoal.contrib.regex.__init__/#examples_4","text":"```python regex = phoneRegex(full=True) regex.run(\"+619123456789\") {'is_phone_regex': True} regex = phoneRegex(full=True) regex.run(\"There is an phone at +619123456789, who would know?\") {'is_phone_regex': False} regex = phoneRegex(full=False) regex.run(\"There is an phone at +619123456789, who would know?\") {'is_phone_regex': True} ``` def _regex ( self ): return r \"^((\\+) {1} 91) {1} [1-9] {1} [0-9] {9} $\"","title":"Examples"},{"location":"api/autogoal.contrib.sklearn.__init__/","text":"Note This module contains wrappers for several estimators and transformers from scikit-learn . Warning Importing this module requires sklearn with a version equal or greater than 0.22 . You can either install it manually or with pip install autogoal[sklearn] . Most of the classes and functions inside this module deal with the automatic generation of wrappers and thus are considered private API. The main public functionality exposed by this module is the function find_classes , which allows to enumerate the wrappers implemented in this module applying some filters. Note You can manually import any wrapper class directly from autogoal.contrib.sklearn._generated buy beware that namespace changes wildly from version to version and classes in it might disappear or change their signature anytime. try : import sklearn major , minor , * rest = sklearn . __version__ . split ( \".\" ) assert int ( major ) == 0 and int ( minor ) >= 22 except : print ( \"(!) Code in `autogoal.contrib.sklearn` requires `sklearn=^0.22`.\" ) print ( \"(!) You can install it with `pip install autogoal[sklearn]`.\" ) raise ImportError () Filter out sklearn convergence and joblib warnings import warnings from sklearn.exceptions import ConvergenceWarning warnings . filterwarnings ( \"ignore\" , message = \".*resource_tracker.*\" ) warnings . filterwarnings ( \"ignore\" , category = ConvergenceWarning ) warnings . filterwarnings ( \"ignore\" , module = \"sklearn\" ) warnings . filterwarnings ( \"ignore\" , module = \"scipy\" ) warnings . filterwarnings ( \"ignore\" , module = \"numpy\" ) from ._generated import * from ._manual import *","title":"autogoal.contrib.sklearn"},{"location":"api/autogoal.contrib.sklearn._builder/","text":"import abc import datetime import inspect import re import textwrap import warnings from pathlib import Path import black import enlighten import numpy as np from autogoal import kb from autogoal.contrib.sklearn._utils import get_input_output , is_algorithm from autogoal.kb import AlgorithmBase from autogoal.grammar import ( BooleanValue , CategoricalValue , ContinuousValue , DiscreteValue , ) from autogoal.utils import nice_repr from joblib import parallel_backend from numpy import inf , nan import sklearn import sklearn.cluster import sklearn.cross_decomposition import sklearn.feature_extraction import sklearn.impute import sklearn.naive_bayes from sklearn.datasets import make_classification try: import dask from dask.distributed import Client DASK_CLIENT = Client ( processes = False ) PARALLEL_BACKEND = 'dask' except ImportError: PARALLEL_BACKEND = 'loky' @nice_repr class SklearnWrapper ( AlgorithmBase ): def __init__ ( self ): self . _mode = \"train\" def train ( self ): self . _mode = \"train\" def eval ( self ): self . _mode = \"eval\" def run ( self , * args ): if self . _mode == \"train\" : return self . _train ( * args ) elif self . _mode == \"eval\" : return self . _eval ( * args ) raise ValueError ( \"Invalid mode: %s \" % self . _mode ) @abc . abstractmethod def _train ( self , * args ): pass @abc . abstractmethod def _eval ( self , * args ): pass class SklearnEstimator ( SklearnWrapper ): def _train ( self , X , y ): self . fit ( X , y ) return y def _eval ( self , X , y = None ): return self . predict ( X ) @abc . abstractmethod def fit ( self , X , y ): pass @abc . abstractmethod def predict ( self , X ): pass class SklearnTransformer ( SklearnWrapper ): def _train ( self , X , y = None ): return self . fit_transform ( X ) def _eval ( self , X , y = None ): return self . transform ( X ) @abc . abstractmethod def fit_transform ( self , X , y = None ): pass @abc . abstractmethod def transform ( self , X , y = None ): pass GENERATION_RULES = dict ( LatentDirichletAllocation = dict ( ignore_params = set ([ \"evaluate_every\" ])), RadiusNeighborsClassifier = dict ( ignore = True ,), KNeighborsTransformer = dict ( ignore_params = set ([ \"metric\" ])), RadiusNeighborsTransformer = dict ( ignore_params = set ([ \"metric\" ])), LocalOutlierFactor = dict ( ignore_params = set ([ \"metric\" ])), RadiusNeighborsRegressor = dict ( ignore_params = set ([ \"metric\" ])), LabelBinarizer = dict ( ignore_params = set ([ \"neg_label\" , \"pos_label\" ]), input_annotation = kb . Seq [ kb . Label ], ), HashingVectorizer = dict ( ignore_params = set ([ \"token_pattern\" , \"analyzer\" , \"input\" , \"decode_error\" ]) ), SpectralBiclustering = dict ( ignore_params = set ([ \"n_components\" , \"n_init\" ])), SpectralCoclustering = dict ( ignore_params = set ([ \"n_init\" ])), KMeans = dict ( ignore_params = set ([ \"n_init\" ])), MiniBatchKMeans = dict ( ignore_params = set ([ \"batch_size\" , \"n_init\" ])), DictionaryLearning = dict ( ignore = True ), MiniBatchDictionaryLearning = dict ( ignore = True ), LassoLars = dict ( ignore_params = [ \"alpha\" ]), TheilSenRegressor = dict ( ignore_params = [ \"max_subpopulation\" ]), TSNE = dict ( ignore = True , ignore_params = [ \"perplexity\" ]), ) def build_sklearn_wrappers (): imports = _walk ( sklearn ) manager = enlighten . get_manager () counter = manager . counter ( total = len ( imports ), unit = \"classes\" ) path = Path ( __file__ ) . parent / \"_generated.py\" with open ( path , \"w\" ) as fp : fp . write ( textwrap . dedent ( f \"\"\" AUTOGENERATED ON {datetime.datetime.now()} from numpy import inf , nan from autogoal.grammar import Continuous , Discrete , Categorical , Boolean from autogoal.contrib.sklearn._builder import SklearnEstimator , SklearnTransformer from autogoal.kb import * Note ) ) for cls in imports: counter.update() _write_class(cls, fp) black.reformat_one( path, True, black.WriteBack.YES, black.FileMode(), black.Report() ) counter.close() manager.stop() def _write_class(cls, fp): rules = GENERATION_RULES.get(cls. name , {}) if rules.get(\"ignore\"): return ignore_args = rules.get(\"ignore_params\", []) print(\"Generating class: %r\" % cls) args = _get_args(cls) for a in ignore_args: args.pop(a, None) args = _get_args_values(cls, args) inputs, outputs = get_input_output(cls) inputs = rules.get(\"input_annotation\", inputs) outputs = rules.get(\"output_annotation\", outputs) if not inputs: warnings.warn(\"Cannot find correct types for %r\" % cls) return s = \" \" * 4 args_str = f\",\\n{s * 4}\".join(f\"{key}: {value}\" for key, value in args.items()) self_str = f\"\\n{s * 4}\".join(f\"self.{key}={key}\" for key in args) \u00b6 init_str = f\",\\n{s * 5}\".join(f\"{key}={key}\" for key in args) input_str, output_str = repr(inputs), repr(outputs) base_class = ( \"SklearnEstimator\" if is_algorithm(cls) == \"estimator\" else \"SklearnTransformer\" ) print(cls) fp.write( textwrap.dedent( f\"\"\" from {cls. module } import {cls. name } as _{cls. name } class {cls. name }( {cls.__name__}, {base_class}): def __init__( self, {args_str} ): {base_class}.__init__(self) {cls. name }. init ( self, {init_str} ) def run(self, input: {input_str}) -> {output_str}: return {base_class}.run(self, input) ) ) fp . flush () def _is_algorithm ( cls , verbose = False ): if hasattr ( cls , \"fit\" ): return True else : if verbose : warnings . warn ( \" %r doesn't have `fit`\" % cls ) if hasattr ( cls , \"transform\" ): return True else : if verbose : warnings . warn ( \" %r doesn't have `transform`\" % cls ) return False def _walk ( module ): imports = set () def _walk_p ( module , visited ): if module in visited : return visited . add ( module ) all_classes = inspect . getmembers ( module , inspect . isclass ) for name , obj in all_classes : if obj in imports : continue try : if not \"sklearn\" in inspect . getfile ( obj ): continue print ( obj ) if isinstance ( obj , type ): if name . endswith ( \"CV\" ): continue if not _is_algorithm ( obj ): continue imports . add ( obj ) except Exception as e : print ( repr ( e )) for name , inner_module in inspect . getmembers ( module , inspect . ismodule ): if not \"sklearn\" in inspect . getfile ( module ): continue print ( inner_module ) _walk_p ( inner_module , visited ) _walk_p ( module , set ()) imports = sorted ( imports , key = lambda c : ( c . __module__ , c . __name__ )) print ( \"Finally found classes:\" ) for cls in imports : print ( cls ) return imports def _find_parameter_values ( parameter , cls ): documentation = [] lines = cls . __doc__ . split ( \" \\n \" ) while lines : l = lines . pop ( 0 ) if l . strip () . startswith ( parameter ): documentation . append ( l ) tabs = l . index ( parameter ) break while lines : l = lines . pop ( 0 ) if not l . strip (): continue if l . startswith ( \" \" * ( tabs + 1 )): documentation . append ( l ) else : break options = set ( re . findall ( r \"'(\\w+)'\" , \" \" . join ( documentation ))) valid = [] invalid = [] skip = set ([ \"deprecated\" , \"auto_deprecated\" , \"precomputed\" ]) for opt in options : opt = opt . lower () if opt in skip : continue with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) if _try ( cls , parameter , opt ): valid . append ( opt ) else : invalid . append ( opt ) if valid : return CategoricalValue ( * sorted ( valid )) return None X , y = make_classification () def _try ( cls , arg , value ): try : print ( f \"Trying { cls } ( { arg } = { value } )... \" , end = \"\" ) cls ( ** { arg : value }) . fit ( X , y ) print ( \"OK\" ) return True except : print ( \"ERROR\" ) return False def _get_args ( cls ): specs = inspect . getfullargspec ( cls . __init__ ) args = specs . args specs = specs . defaults if not args or not specs : return {} args = args [ - len ( specs ) :] args_map = { k : v for k , v in zip ( args , specs )} drop_args = [ \"verbose\" , \"random_state\" , \"n_jobs\" , \"max_iter\" , \"class_weight\" , \"warm_start\" , \"copy_X\" , \"copy_x\" , \"copy\" , \"eps\" , \"n_init\" , ] for arg in drop_args : args_map . pop ( arg , None ) print ( \"Found args: %r \" % args_map ) return args_map def _get_args_values ( cls , args_map ): result = {} for arg , value in args_map . items (): values = _get_arg_values ( arg , value , cls ) if not values : continue result [ arg ] = values return result def _get_arg_values ( arg , value , cls ): print ( f \"Computing valid values for: { arg } = { value } \" ) try : if isinstance ( value , bool ): annotation = BooleanValue () elif isinstance ( value , int ): annotation = _get_integer_values ( arg , value , cls ) elif isinstance ( value , float ): annotation = _get_float_values ( arg , value , cls ) elif isinstance ( value , str ): annotation = _find_parameter_values ( arg , cls ) else : annotation = None except : annotation = None print ( f \"Found annotation { arg } : { annotation } \" ) return annotation def _get_integer_values ( arg , value , cls ): if value > 0 : min_value = 0 max_value = 2 * value elif value == 0 : min_value = - 100 max_value = 100 else : return None binary search for minimum value left = min_value right = value while left < right : current_value = int (( left + right ) / 2 ) if current_value in [ left , right ]: break if _try ( cls , arg , current_value ): right = current_value else : left = current_value min_value = right binary search for maximum value left = value right = max_value while left < right : current_value = int (( left + right ) / 2 ) if current_value in [ left , right ]: break if _try ( cls , arg , current_value ): left = current_value else : right = current_value max_value = left if min_value < max_value : return DiscreteValue ( min = min_value , max = max_value ) return None def _get_float_values ( arg , value , cls ): if value in [ inf , nan ]: return None if value > 0 : min_value = - 10 * value max_value = 10 * value elif value == 0 : min_value = - 1 max_value = 1 else : return None binary search for minimum value left = min_value right = value while abs ( left - right ) > 1e-2 : current_value = round (( left + right ) / 2 , 3 ) if _try ( cls , arg , current_value ): right = current_value else : left = current_value min_value = right binary search for maximum value left = value right = max_value while abs ( left - right ) > 1e-2 : current_value = round (( left + right ) / 2 , 3 ) if _try ( cls , arg , current_value ): left = current_value else : right = current_value max_value = left if max_value - min_value >= 2 * value : return ContinuousValue ( min = min_value , max = max_value ) return None if __name__ == \"__main__\" : build_sklearn_wrappers ()","title":"Autogoal.contrib.sklearn. builder"},{"location":"api/autogoal.contrib.sklearn._builder/#self_str-fns-4joinfselfkeykey-for-key-in-args","text":"init_str = f\",\\n{s * 5}\".join(f\"{key}={key}\" for key in args) input_str, output_str = repr(inputs), repr(outputs) base_class = ( \"SklearnEstimator\" if is_algorithm(cls) == \"estimator\" else \"SklearnTransformer\" ) print(cls) fp.write( textwrap.dedent( f\"\"\" from {cls. module } import {cls. name } as _{cls. name } class {cls. name }( {cls.__name__}, {base_class}): def __init__( self, {args_str} ): {base_class}.__init__(self) {cls. name }. init ( self, {init_str} ) def run(self, input: {input_str}) -> {output_str}: return {base_class}.run(self, input) ) ) fp . flush () def _is_algorithm ( cls , verbose = False ): if hasattr ( cls , \"fit\" ): return True else : if verbose : warnings . warn ( \" %r doesn't have `fit`\" % cls ) if hasattr ( cls , \"transform\" ): return True else : if verbose : warnings . warn ( \" %r doesn't have `transform`\" % cls ) return False def _walk ( module ): imports = set () def _walk_p ( module , visited ): if module in visited : return visited . add ( module ) all_classes = inspect . getmembers ( module , inspect . isclass ) for name , obj in all_classes : if obj in imports : continue try : if not \"sklearn\" in inspect . getfile ( obj ): continue print ( obj ) if isinstance ( obj , type ): if name . endswith ( \"CV\" ): continue if not _is_algorithm ( obj ): continue imports . add ( obj ) except Exception as e : print ( repr ( e )) for name , inner_module in inspect . getmembers ( module , inspect . ismodule ): if not \"sklearn\" in inspect . getfile ( module ): continue print ( inner_module ) _walk_p ( inner_module , visited ) _walk_p ( module , set ()) imports = sorted ( imports , key = lambda c : ( c . __module__ , c . __name__ )) print ( \"Finally found classes:\" ) for cls in imports : print ( cls ) return imports def _find_parameter_values ( parameter , cls ): documentation = [] lines = cls . __doc__ . split ( \" \\n \" ) while lines : l = lines . pop ( 0 ) if l . strip () . startswith ( parameter ): documentation . append ( l ) tabs = l . index ( parameter ) break while lines : l = lines . pop ( 0 ) if not l . strip (): continue if l . startswith ( \" \" * ( tabs + 1 )): documentation . append ( l ) else : break options = set ( re . findall ( r \"'(\\w+)'\" , \" \" . join ( documentation ))) valid = [] invalid = [] skip = set ([ \"deprecated\" , \"auto_deprecated\" , \"precomputed\" ]) for opt in options : opt = opt . lower () if opt in skip : continue with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) if _try ( cls , parameter , opt ): valid . append ( opt ) else : invalid . append ( opt ) if valid : return CategoricalValue ( * sorted ( valid )) return None X , y = make_classification () def _try ( cls , arg , value ): try : print ( f \"Trying { cls } ( { arg } = { value } )... \" , end = \"\" ) cls ( ** { arg : value }) . fit ( X , y ) print ( \"OK\" ) return True except : print ( \"ERROR\" ) return False def _get_args ( cls ): specs = inspect . getfullargspec ( cls . __init__ ) args = specs . args specs = specs . defaults if not args or not specs : return {} args = args [ - len ( specs ) :] args_map = { k : v for k , v in zip ( args , specs )} drop_args = [ \"verbose\" , \"random_state\" , \"n_jobs\" , \"max_iter\" , \"class_weight\" , \"warm_start\" , \"copy_X\" , \"copy_x\" , \"copy\" , \"eps\" , \"n_init\" , ] for arg in drop_args : args_map . pop ( arg , None ) print ( \"Found args: %r \" % args_map ) return args_map def _get_args_values ( cls , args_map ): result = {} for arg , value in args_map . items (): values = _get_arg_values ( arg , value , cls ) if not values : continue result [ arg ] = values return result def _get_arg_values ( arg , value , cls ): print ( f \"Computing valid values for: { arg } = { value } \" ) try : if isinstance ( value , bool ): annotation = BooleanValue () elif isinstance ( value , int ): annotation = _get_integer_values ( arg , value , cls ) elif isinstance ( value , float ): annotation = _get_float_values ( arg , value , cls ) elif isinstance ( value , str ): annotation = _find_parameter_values ( arg , cls ) else : annotation = None except : annotation = None print ( f \"Found annotation { arg } : { annotation } \" ) return annotation def _get_integer_values ( arg , value , cls ): if value > 0 : min_value = 0 max_value = 2 * value elif value == 0 : min_value = - 100 max_value = 100 else : return None binary search for minimum value left = min_value right = value while left < right : current_value = int (( left + right ) / 2 ) if current_value in [ left , right ]: break if _try ( cls , arg , current_value ): right = current_value else : left = current_value min_value = right binary search for maximum value left = value right = max_value while left < right : current_value = int (( left + right ) / 2 ) if current_value in [ left , right ]: break if _try ( cls , arg , current_value ): left = current_value else : right = current_value max_value = left if min_value < max_value : return DiscreteValue ( min = min_value , max = max_value ) return None def _get_float_values ( arg , value , cls ): if value in [ inf , nan ]: return None if value > 0 : min_value = - 10 * value max_value = 10 * value elif value == 0 : min_value = - 1 max_value = 1 else : return None binary search for minimum value left = min_value right = value while abs ( left - right ) > 1e-2 : current_value = round (( left + right ) / 2 , 3 ) if _try ( cls , arg , current_value ): right = current_value else : left = current_value min_value = right binary search for maximum value left = value right = max_value while abs ( left - right ) > 1e-2 : current_value = round (( left + right ) / 2 , 3 ) if _try ( cls , arg , current_value ): left = current_value else : right = current_value max_value = left if max_value - min_value >= 2 * value : return ContinuousValue ( min = min_value , max = max_value ) return None if __name__ == \"__main__\" : build_sklearn_wrappers ()","title":"self_str = f\"\\n{s * 4}\".join(f\"self.{key}={key}\" for key in args)"},{"location":"api/autogoal.contrib.sklearn._generated/","text":"AUTOGENERATED ON 2020-01-30 22:15:23.661530 from numpy import inf , nan from autogoal import grammar from autogoal.contrib.sklearn._builder import SklearnEstimator , SklearnTransformer from autogoal.kb import * from autogoal.utils import nice_repr from autogoal.kb import Supervised from sklearn.cluster._affinity_propagation import ( AffinityPropagation as _AffinityPropagation , ) @nice_repr class AffinityPropagation ( _AffinityPropagation , SklearnEstimator ): def __init__ ( self , convergence_iter : grammar . DiscreteValue ( min = 1 , max = 29 ), affinity : grammar . CategoricalValue ( \"euclidean\" ), ): SklearnEstimator . __init__ ( self ) _AffinityPropagation . __init__ ( self , convergence_iter = convergence_iter , affinity = affinity ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.cluster._agglomerative import FeatureAgglomeration as _FeatureAgglomeration @nice_repr class FeatureAgglomeration ( _FeatureAgglomeration , SklearnTransformer ): def __init__ ( self , n_clusters : grammar . DiscreteValue ( min = 1 , max = 3 ), affinity : grammar . CategoricalValue ( \"euclidean\" ), compute_full_tree : grammar . CategoricalValue ( \"auto\" ), linkage : grammar . CategoricalValue ( \"average\" , \"complete\" , \"single\" , \"ward\" ), ): SklearnTransformer . __init__ ( self ) _FeatureAgglomeration . __init__ ( self , n_clusters = n_clusters , affinity = affinity , compute_full_tree = compute_full_tree , linkage = linkage , ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.cluster._birch import Birch as _Birch @nice_repr class Birch ( _Birch , SklearnEstimator ): def __init__ ( self , threshold : grammar . ContinuousValue ( min =- 4.995 , max = 4.991 ), branching_factor : grammar . DiscreteValue ( min = 2 , max = 99 ), n_clusters : grammar . DiscreteValue ( min = 1 , max = 5 ), compute_labels : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _Birch . __init__ ( self , threshold = threshold , branching_factor = branching_factor , n_clusters = n_clusters , compute_labels = compute_labels , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.cluster._kmeans import KMeans as _KMeans @nice_repr class KMeans ( _KMeans , SklearnEstimator ): def __init__ ( self , n_clusters : grammar . DiscreteValue ( min = 1 , max = 15 ), init : grammar . CategoricalValue ( \"random\" ), precompute_distances : grammar . CategoricalValue ( \"auto\" ), ): SklearnEstimator . __init__ ( self ) _KMeans . __init__ ( self , n_clusters = n_clusters , init = init , precompute_distances = precompute_distances , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.cluster._kmeans import MiniBatchKMeans as _MiniBatchKMeans @nice_repr class MiniBatchKMeans ( _MiniBatchKMeans , SklearnEstimator ): def __init__ ( self , n_clusters : grammar . DiscreteValue ( min = 1 , max = 15 ), init : grammar . CategoricalValue ( \"random\" ), compute_labels : grammar . BooleanValue (), tol : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), max_no_improvement : grammar . DiscreteValue ( min = 1 , max = 19 ), reassignment_ratio : grammar . ContinuousValue ( min =- 0.093 , max = 0.094 ), ): SklearnEstimator . __init__ ( self ) _MiniBatchKMeans . __init__ ( self , n_clusters = n_clusters , init = init , compute_labels = compute_labels , tol = tol , max_no_improvement = max_no_improvement , reassignment_ratio = reassignment_ratio , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.cluster._mean_shift import MeanShift as _MeanShift @nice_repr class MeanShift ( _MeanShift , SklearnEstimator ): def __init__ ( self , bin_seeding : grammar . BooleanValue (), cluster_all : grammar . BooleanValue () ): SklearnEstimator . __init__ ( self ) _MeanShift . __init__ ( self , bin_seeding = bin_seeding , cluster_all = cluster_all ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.decomposition._factor_analysis import FactorAnalysis as _FactorAnalysis @nice_repr class FactorAnalysis ( _FactorAnalysis , SklearnTransformer ): def __init__ ( self , tol : grammar . ContinuousValue ( min =- 0.093 , max = 0.094 ), svd_method : grammar . CategoricalValue ( \"lapack\" , \"randomized\" ), iterated_power : grammar . DiscreteValue ( min = 1 , max = 5 ), ): SklearnTransformer . __init__ ( self ) _FactorAnalysis . __init__ ( self , tol = tol , svd_method = svd_method , iterated_power = iterated_power ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.decomposition._fastica import FastICA as _FastICA @nice_repr class FastICA ( _FastICA , SklearnTransformer ): def __init__ ( self , algorithm : grammar . CategoricalValue ( \"deflation\" , \"parallel\" ), whiten : grammar . BooleanValue (), fun : grammar . CategoricalValue ( \"cube\" , \"exp\" , \"logcosh\" ), ): SklearnTransformer . __init__ ( self ) _FastICA . __init__ ( self , algorithm = algorithm , whiten = whiten , fun = fun ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.decomposition._incremental_pca import IncrementalPCA as _IncrementalPCA @nice_repr class IncrementalPCA ( _IncrementalPCA , SklearnTransformer ): def __init__ ( self , whiten : grammar . BooleanValue ()): SklearnTransformer . __init__ ( self ) _IncrementalPCA . __init__ ( self , whiten = whiten ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.decomposition._kernel_pca import KernelPCA as _KernelPCA @nice_repr class KernelPCA ( _KernelPCA , SklearnTransformer ): def __init__ ( self , degree : grammar . DiscreteValue ( min = 1 , max = 5 ), alpha : grammar . ContinuousValue ( min =- 9.995 , max = 9.991 ), fit_inverse_transform : grammar . BooleanValue (), eigen_solver : grammar . CategoricalValue ( \"arpack\" , \"auto\" , \"dense\" ), tol : grammar . DiscreteValue ( min =- 99 , max = 99 ), remove_zero_eig : grammar . BooleanValue (), ): SklearnTransformer . __init__ ( self ) _KernelPCA . __init__ ( self , degree = degree , alpha = alpha , fit_inverse_transform = fit_inverse_transform , eigen_solver = eigen_solver , tol = tol , remove_zero_eig = remove_zero_eig , ) def run ( self , input : MatrixContinuous ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.decomposition._lda import ( LatentDirichletAllocation as _LatentDirichletAllocation , ) @nice_repr class LatentDirichletAllocation ( _LatentDirichletAllocation , SklearnTransformer ): def __init__ ( self ,): SklearnTransformer . __init__ ( self ) _LatentDirichletAllocation . __init__ ( self ,) def run ( self , input : MatrixContinuous ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.decomposition._nmf import NMF as _NMF @nice_repr class NMF ( _NMF , SklearnTransformer ): def __init__ ( self , alpha : grammar . ContinuousValue ( min = 0.0 , max = 0.0 ), l1_ratio : grammar . ContinuousValue ( min = 0.0 , max = 0.0 ), shuffle : grammar . BooleanValue (), ): SklearnTransformer . __init__ ( self ) _NMF . __init__ ( self , alpha = alpha , l1_ratio = l1_ratio , shuffle = shuffle ) def run ( self , input : MatrixContinuous ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.decomposition._pca import PCA as _PCA @nice_repr class PCA ( _PCA , SklearnTransformer ): def __init__ ( self , whiten : grammar . BooleanValue (), svd_solver : grammar . CategoricalValue ( \"arpack\" , \"auto\" , \"full\" , \"randomized\" ), tol : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), iterated_power : grammar . CategoricalValue ( \"auto\" , \"randomized\" ), ): SklearnTransformer . __init__ ( self ) _PCA . __init__ ( self , whiten = whiten , svd_solver = svd_solver , tol = tol , iterated_power = iterated_power , ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.decomposition._sparse_pca import MiniBatchSparsePCA as _MiniBatchSparsePCA @nice_repr class MiniBatchSparsePCA ( _MiniBatchSparsePCA , SklearnTransformer ): def __init__ ( self , ridge_alpha : grammar . ContinuousValue ( min =- 0.093 , max = 0.094 ), n_iter : grammar . DiscreteValue ( min = 1 , max = 199 ), batch_size : grammar . DiscreteValue ( min = 1 , max = 5 ), shuffle : grammar . BooleanValue (), method : grammar . CategoricalValue ( \"cd\" , \"lars\" ), ): SklearnTransformer . __init__ ( self ) _MiniBatchSparsePCA . __init__ ( self , ridge_alpha = ridge_alpha , n_iter = n_iter , batch_size = batch_size , shuffle = shuffle , method = method , ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.decomposition._sparse_pca import SparsePCA as _SparsePCA @nice_repr class SparsePCA ( _SparsePCA , SklearnTransformer ): def __init__ ( self , ridge_alpha : grammar . ContinuousValue ( min =- 0.093 , max = 0.094 ), method : grammar . CategoricalValue ( \"cd\" , \"lars\" ), ): SklearnTransformer . __init__ ( self ) _SparsePCA . __init__ ( self , ridge_alpha = ridge_alpha , method = method ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.decomposition._truncated_svd import TruncatedSVD as _TruncatedSVD @nice_repr class TruncatedSVD ( _TruncatedSVD , SklearnTransformer ): def __init__ ( self , n_components : grammar . DiscreteValue ( min = 1 , max = 3 ), n_iter : grammar . DiscreteValue ( min = 1 , max = 9 ), tol : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), ): SklearnTransformer . __init__ ( self ) _TruncatedSVD . __init__ ( self , n_components = n_components , n_iter = n_iter , tol = tol ) def run ( self , input : MatrixContinuous ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.feature_extraction.text import CountVectorizer as _CountVectorizer @nice_repr class CountVectorizer ( _CountVectorizer , SklearnTransformer ): def __init__ ( self , lowercase : grammar . BooleanValue (), binary : grammar . BooleanValue () ): SklearnTransformer . __init__ ( self ) _CountVectorizer . __init__ ( self , lowercase = lowercase , binary = binary ) def run ( self , input : Seq [ Sentence ]) -> MatrixContinuousSparse : return SklearnTransformer . run ( self , input ) from sklearn.feature_extraction.text import HashingVectorizer as _HashingVectorizer @nice_repr class HashingVectorizer ( _HashingVectorizer , SklearnTransformer ): def __init__ ( self , lowercase : grammar . BooleanValue (), n_features : grammar . DiscreteValue ( min = 1 , max = 2097151 ), binary : grammar . BooleanValue (), norm : grammar . CategoricalValue ( \"l1\" ), alternate_sign : grammar . BooleanValue (), ): SklearnTransformer . __init__ ( self ) _HashingVectorizer . __init__ ( self , lowercase = lowercase , n_features = n_features , binary = binary , norm = norm , alternate_sign = alternate_sign , ) def run ( self , input : Seq [ Sentence ]) -> MatrixContinuousSparse : return SklearnTransformer . run ( self , input ) from sklearn.feature_extraction.text import TfidfTransformer as _TfidfTransformer @nice_repr class TfidfTransformer ( _TfidfTransformer , SklearnTransformer ): def __init__ ( self , norm : grammar . CategoricalValue ( \"l1\" , \"l2\" ), use_idf : grammar . BooleanValue (), smooth_idf : grammar . BooleanValue (), sublinear_tf : grammar . BooleanValue (), ): SklearnTransformer . __init__ ( self ) _TfidfTransformer . __init__ ( self , norm = norm , use_idf = use_idf , smooth_idf = smooth_idf , sublinear_tf = sublinear_tf , ) def run ( self , input : MatrixContinuous ) -> MatrixContinuousSparse : return SklearnTransformer . run ( self , input ) from sklearn.feature_extraction.text import TfidfVectorizer as _TfidfVectorizer @nice_repr class TfidfVectorizer ( _TfidfVectorizer , SklearnTransformer ): def __init__ ( self , lowercase : grammar . BooleanValue (), binary : grammar . BooleanValue (), use_idf : grammar . BooleanValue (), smooth_idf : grammar . BooleanValue (), sublinear_tf : grammar . BooleanValue (), ): SklearnTransformer . __init__ ( self ) _TfidfVectorizer . __init__ ( self , lowercase = lowercase , binary = binary , use_idf = use_idf , smooth_idf = smooth_idf , sublinear_tf = sublinear_tf , ) def run ( self , input : Seq [ Sentence ]) -> MatrixContinuousSparse : return SklearnTransformer . run ( self , input ) from sklearn.impute._knn import KNNImputer as _KNNImputer @nice_repr class KNNImputer ( _KNNImputer , SklearnTransformer ): def __init__ ( self , n_neighbors : grammar . DiscreteValue ( min = 1 , max = 9 ), weights : grammar . CategoricalValue ( \"distance\" , \"uniform\" ), metric : grammar . CategoricalValue ( \"nan_euclidean\" ), add_indicator : grammar . BooleanValue (), ): SklearnTransformer . __init__ ( self ) _KNNImputer . __init__ ( self , n_neighbors = n_neighbors , weights = weights , metric = metric , add_indicator = add_indicator , ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.linear_model._base import LinearRegression as _LinearRegression @nice_repr class LinearRegression ( _LinearRegression , SklearnEstimator ): def __init__ ( self , fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue () ): SklearnEstimator . __init__ ( self ) _LinearRegression . __init__ ( self , fit_intercept = fit_intercept , normalize = normalize ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._bayes import ARDRegression as _ARDRegression @nice_repr class ARDRegression ( _ARDRegression , SklearnEstimator ): def __init__ ( self , n_iter : grammar . DiscreteValue ( min = 1 , max = 599 ), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), compute_score : grammar . BooleanValue (), threshold_lambda : grammar . ContinuousValue ( min =- 99999.993 , max = 99999.995 ), fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _ARDRegression . __init__ ( self , n_iter = n_iter , tol = tol , compute_score = compute_score , threshold_lambda = threshold_lambda , fit_intercept = fit_intercept , normalize = normalize , ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._bayes import BayesianRidge as _BayesianRidge @nice_repr class BayesianRidge ( _BayesianRidge , SklearnEstimator ): def __init__ ( self , n_iter : grammar . DiscreteValue ( min = 1 , max = 599 ), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), compute_score : grammar . BooleanValue (), fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _BayesianRidge . __init__ ( self , n_iter = n_iter , tol = tol , compute_score = compute_score , fit_intercept = fit_intercept , normalize = normalize , ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._coordinate_descent import ElasticNet as _ElasticNet @nice_repr class ElasticNet ( _ElasticNet , SklearnEstimator ): def __init__ ( self , alpha : grammar . ContinuousValue ( min =- 9.995 , max = 9.991 ), l1_ratio : grammar . ContinuousValue ( min =- 4.995 , max = 4.991 ), fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), precompute : grammar . BooleanValue (), positive : grammar . BooleanValue (), selection : grammar . CategoricalValue ( \"cyclic\" , \"random\" ), ): SklearnEstimator . __init__ ( self ) _ElasticNet . __init__ ( self , alpha = alpha , l1_ratio = l1_ratio , fit_intercept = fit_intercept , normalize = normalize , precompute = precompute , positive = positive , selection = selection , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._coordinate_descent import Lasso as _Lasso @nice_repr class Lasso ( _Lasso , SklearnEstimator ): def __init__ ( self , alpha : grammar . ContinuousValue ( min =- 9.995 , max = 9.991 ), fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), precompute : grammar . BooleanValue (), positive : grammar . BooleanValue (), selection : grammar . CategoricalValue ( \"cyclic\" , \"random\" ), ): SklearnEstimator . __init__ ( self ) _Lasso . __init__ ( self , alpha = alpha , fit_intercept = fit_intercept , normalize = normalize , precompute = precompute , positive = positive , selection = selection , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._huber import HuberRegressor as _HuberRegressor @nice_repr class HuberRegressor ( _HuberRegressor , SklearnEstimator ): def __init__ ( self , epsilon : grammar . ContinuousValue ( min = 1.002 , max = 13.494 ), fit_intercept : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _HuberRegressor . __init__ ( self , epsilon = epsilon , fit_intercept = fit_intercept ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._least_angle import Lars as _Lars @nice_repr class Lars ( _Lars , SklearnEstimator ): def __init__ ( self , fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), precompute : grammar . CategoricalValue ( \"auto\" ), n_nonzero_coefs : grammar . DiscreteValue ( min = 1 , max = 999 ), fit_path : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _Lars . __init__ ( self , fit_intercept = fit_intercept , normalize = normalize , precompute = precompute , n_nonzero_coefs = n_nonzero_coefs , fit_path = fit_path , ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._least_angle import LassoLars as _LassoLars @nice_repr class LassoLars ( _LassoLars , SklearnEstimator ): def __init__ ( self , fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), precompute : grammar . CategoricalValue ( \"auto\" ), fit_path : grammar . BooleanValue (), positive : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _LassoLars . __init__ ( self , fit_intercept = fit_intercept , normalize = normalize , precompute = precompute , fit_path = fit_path , positive = positive , ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._least_angle import LassoLarsIC as _LassoLarsIC @nice_repr class LassoLarsIC ( _LassoLarsIC , SklearnEstimator ): def __init__ ( self , criterion : grammar . CategoricalValue ( \"aic\" , \"bic\" ), fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), precompute : grammar . CategoricalValue ( \"auto\" ), positive : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _LassoLarsIC . __init__ ( self , criterion = criterion , fit_intercept = fit_intercept , normalize = normalize , precompute = precompute , positive = positive , ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._logistic import LogisticRegression as _LogisticRegression @nice_repr class LogisticRegression ( _LogisticRegression , SklearnEstimator ): def __init__ ( self , penalty : grammar . CategoricalValue ( \"l2\" , \"none\" ), dual : grammar . BooleanValue (), C : grammar . ContinuousValue ( min = 0.005 , max = 9.991 ), fit_intercept : grammar . BooleanValue (), solver : grammar . CategoricalValue ( \"lbfgs\" , \"liblinear\" , \"sag\" , \"saga\" ), multi_class : grammar . CategoricalValue ( \"auto\" , \"multinomial\" , \"ovr\" ), ): SklearnEstimator . __init__ ( self ) _LogisticRegression . __init__ ( self , penalty = penalty , dual = dual , C = C , fit_intercept = fit_intercept , solver = solver , multi_class = multi_class , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._omp import ( OrthogonalMatchingPursuit as _OrthogonalMatchingPursuit , ) @nice_repr class OrthogonalMatchingPursuit ( _OrthogonalMatchingPursuit , SklearnEstimator ): def __init__ ( self , fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), precompute : grammar . CategoricalValue ( \"auto\" ), ): SklearnEstimator . __init__ ( self ) _OrthogonalMatchingPursuit . __init__ ( self , fit_intercept = fit_intercept , normalize = normalize , precompute = precompute , ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._passive_aggressive import ( PassiveAggressiveClassifier as _PassiveAggressiveClassifier , ) @nice_repr class PassiveAggressiveClassifier ( _PassiveAggressiveClassifier , SklearnEstimator ): def __init__ ( self , C : grammar . ContinuousValue ( min =- 9.995 , max = 9.991 ), fit_intercept : grammar . BooleanValue (), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), early_stopping : grammar . BooleanValue (), validation_fraction : grammar . ContinuousValue ( min = 0.006 , max = 0.993 ), n_iter_no_change : grammar . DiscreteValue ( min = 1 , max = 9 ), shuffle : grammar . BooleanValue (), average : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _PassiveAggressiveClassifier . __init__ ( self , C = C , fit_intercept = fit_intercept , tol = tol , early_stopping = early_stopping , validation_fraction = validation_fraction , n_iter_no_change = n_iter_no_change , shuffle = shuffle , average = average , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._passive_aggressive import ( PassiveAggressiveRegressor as _PassiveAggressiveRegressor , ) @nice_repr class PassiveAggressiveRegressor ( _PassiveAggressiveRegressor , SklearnEstimator ): def __init__ ( self , C : grammar . ContinuousValue ( min =- 9.995 , max = 9.991 ), fit_intercept : grammar . BooleanValue (), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), early_stopping : grammar . BooleanValue (), validation_fraction : grammar . ContinuousValue ( min = 0.006 , max = 0.993 ), n_iter_no_change : grammar . DiscreteValue ( min = 1 , max = 9 ), shuffle : grammar . BooleanValue (), epsilon : grammar . ContinuousValue ( min =- 0.992 , max = 0.993 ), average : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _PassiveAggressiveRegressor . __init__ ( self , C = C , fit_intercept = fit_intercept , tol = tol , early_stopping = early_stopping , validation_fraction = validation_fraction , n_iter_no_change = n_iter_no_change , shuffle = shuffle , epsilon = epsilon , average = average , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._perceptron import Perceptron as _Perceptron @nice_repr class Perceptron ( _Perceptron , SklearnEstimator ): def __init__ ( self , fit_intercept : grammar . BooleanValue (), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), shuffle : grammar . BooleanValue (), eta0 : grammar . ContinuousValue ( min = 0.005 , max = 9.991 ), early_stopping : grammar . BooleanValue (), validation_fraction : grammar . ContinuousValue ( min = 0.006 , max = 0.993 ), n_iter_no_change : grammar . DiscreteValue ( min = 1 , max = 9 ), ): SklearnEstimator . __init__ ( self ) _Perceptron . __init__ ( self , fit_intercept = fit_intercept , tol = tol , shuffle = shuffle , eta0 = eta0 , early_stopping = early_stopping , validation_fraction = validation_fraction , n_iter_no_change = n_iter_no_change , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._ridge import Ridge as _Ridge @nice_repr class Ridge ( _Ridge , SklearnEstimator ): def __init__ ( self , alpha : grammar . ContinuousValue ( min =- 9.995 , max = 9.991 ), fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), solver : grammar . CategoricalValue ( \"auto\" , \"cholesky\" , \"lsqr\" , \"sag\" , \"saga\" , \"sparse_cg\" , \"svd\" ), ): SklearnEstimator . __init__ ( self ) _Ridge . __init__ ( self , alpha = alpha , fit_intercept = fit_intercept , normalize = normalize , tol = tol , solver = solver , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._ridge import RidgeClassifier as _RidgeClassifier @nice_repr class RidgeClassifier ( _RidgeClassifier , SklearnEstimator ): def __init__ ( self , alpha : grammar . ContinuousValue ( min =- 9.995 , max = 9.991 ), fit_intercept : grammar . BooleanValue (), normalize : grammar . BooleanValue (), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), solver : grammar . CategoricalValue ( \"auto\" , \"cholesky\" , \"lsqr\" , \"sag\" , \"saga\" , \"sparse_cg\" , \"svd\" ), ): SklearnEstimator . __init__ ( self ) _RidgeClassifier . __init__ ( self , alpha = alpha , fit_intercept = fit_intercept , normalize = normalize , tol = tol , solver = solver , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._stochastic_gradient import SGDClassifier as _SGDClassifier @nice_repr class SGDClassifier ( _SGDClassifier , SklearnEstimator ): def __init__ ( self , loss : grammar . CategoricalValue ( \"epsilon_insensitive\" , \"hinge\" , \"huber\" , \"log\" , \"modified_huber\" , \"perceptron\" , \"squared_epsilon_insensitive\" , \"squared_hinge\" , \"squared_loss\" , ), penalty : grammar . CategoricalValue ( \"elasticnet\" , \"l1\" , \"l2\" ), l1_ratio : grammar . ContinuousValue ( min = 0.001 , max = 0.999 ), fit_intercept : grammar . BooleanValue (), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), shuffle : grammar . BooleanValue (), epsilon : grammar . ContinuousValue ( min =- 0.992 , max = 0.993 ), learning_rate : grammar . CategoricalValue ( \"optimal\" ), eta0 : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), power_t : grammar . ContinuousValue ( min =- 4.995 , max = 4.991 ), early_stopping : grammar . BooleanValue (), validation_fraction : grammar . ContinuousValue ( min = 0.006 , max = 0.993 ), n_iter_no_change : grammar . DiscreteValue ( min = 1 , max = 9 ), average : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _SGDClassifier . __init__ ( self , loss = loss , penalty = penalty , l1_ratio = l1_ratio , fit_intercept = fit_intercept , tol = tol , shuffle = shuffle , epsilon = epsilon , learning_rate = learning_rate , eta0 = eta0 , power_t = power_t , early_stopping = early_stopping , validation_fraction = validation_fraction , n_iter_no_change = n_iter_no_change , average = average , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._stochastic_gradient import SGDRegressor as _SGDRegressor @nice_repr class SGDRegressor ( _SGDRegressor , SklearnEstimator ): def __init__ ( self , loss : grammar . CategoricalValue ( \"epsilon_insensitive\" , \"huber\" , \"squared_epsilon_insensitive\" , \"squared_loss\" , ), penalty : grammar . CategoricalValue ( \"elasticnet\" , \"l1\" , \"l2\" ), l1_ratio : grammar . ContinuousValue ( min = 0.001 , max = 0.999 ), fit_intercept : grammar . BooleanValue (), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), shuffle : grammar . BooleanValue (), epsilon : grammar . ContinuousValue ( min =- 0.992 , max = 0.993 ), learning_rate : grammar . CategoricalValue ( \"adaptive\" , \"constant\" , \"invscaling\" , \"optimal\" ), eta0 : grammar . ContinuousValue ( min = 0.003 , max = 0.094 ), power_t : grammar . ContinuousValue ( min =- 2.494 , max = 2.491 ), early_stopping : grammar . BooleanValue (), validation_fraction : grammar . ContinuousValue ( min = 0.006 , max = 0.993 ), n_iter_no_change : grammar . DiscreteValue ( min = 1 , max = 9 ), average : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _SGDRegressor . __init__ ( self , loss = loss , penalty = penalty , l1_ratio = l1_ratio , fit_intercept = fit_intercept , tol = tol , shuffle = shuffle , epsilon = epsilon , learning_rate = learning_rate , eta0 = eta0 , power_t = power_t , early_stopping = early_stopping , validation_fraction = validation_fraction , n_iter_no_change = n_iter_no_change , average = average , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.linear_model._theil_sen import TheilSenRegressor as _TheilSenRegressor @nice_repr class TheilSenRegressor ( _TheilSenRegressor , SklearnEstimator ): def __init__ ( self , fit_intercept : grammar . BooleanValue (), tol : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), ): SklearnEstimator . __init__ ( self ) _TheilSenRegressor . __init__ ( self , fit_intercept = fit_intercept , tol = tol ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.manifold._isomap import Isomap as _Isomap @nice_repr class Isomap ( _Isomap , SklearnTransformer ): def __init__ ( self , n_neighbors : grammar . DiscreteValue ( min = 1 , max = 9 ), n_components : grammar . DiscreteValue ( min = 1 , max = 3 ), eigen_solver : grammar . CategoricalValue ( \"arpack\" , \"auto\" , \"dense\" ), tol : grammar . DiscreteValue ( min =- 99 , max = 99 ), path_method : grammar . CategoricalValue ( \"auto\" ), neighbors_algorithm : grammar . CategoricalValue ( \"auto\" , \"ball_tree\" , \"brute\" , \"kd_tree\" ), p : grammar . DiscreteValue ( min = 1 , max = 3 ), ): SklearnTransformer . __init__ ( self ) _Isomap . __init__ ( self , n_neighbors = n_neighbors , n_components = n_components , eigen_solver = eigen_solver , tol = tol , path_method = path_method , neighbors_algorithm = neighbors_algorithm , p = p , ) def run ( self , input : MatrixContinuous ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.manifold._locally_linear import ( LocallyLinearEmbedding as _LocallyLinearEmbedding , ) @nice_repr class LocallyLinearEmbedding ( _LocallyLinearEmbedding , SklearnTransformer ): def __init__ ( self , n_neighbors : grammar . DiscreteValue ( min = 1 , max = 9 ), n_components : grammar . DiscreteValue ( min = 1 , max = 3 ), reg : grammar . ContinuousValue ( min =- 0.005 , max = 0.001 ), eigen_solver : grammar . CategoricalValue ( \"arpack\" , \"auto\" , \"dense\" ), method : grammar . CategoricalValue ( \"ltsa\" , \"modified\" , \"standard\" ), neighbors_algorithm : grammar . CategoricalValue ( \"auto\" , \"ball_tree\" , \"brute\" , \"kd_tree\" ), ): SklearnTransformer . __init__ ( self ) _LocallyLinearEmbedding . __init__ ( self , n_neighbors = n_neighbors , n_components = n_components , reg = reg , eigen_solver = eigen_solver , method = method , neighbors_algorithm = neighbors_algorithm , ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.naive_bayes import BernoulliNB as _BernoulliNB @nice_repr class BernoulliNB ( _BernoulliNB , SklearnEstimator ): def __init__ ( self , alpha : grammar . ContinuousValue ( min = 0.0 , max = 9.991 ), binarize : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), fit_prior : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _BernoulliNB . __init__ ( self , alpha = alpha , binarize = binarize , fit_prior = fit_prior ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.naive_bayes import CategoricalNB as _CategoricalNB @nice_repr class CategoricalNB ( _CategoricalNB , SklearnEstimator ): def __init__ ( self , fit_prior : grammar . BooleanValue ()): SklearnEstimator . __init__ ( self ) _CategoricalNB . __init__ ( self , fit_prior = fit_prior ) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.naive_bayes import ComplementNB as _ComplementNB @nice_repr class ComplementNB ( _ComplementNB , SklearnEstimator ): def __init__ ( self , fit_prior : grammar . BooleanValue (), norm : grammar . BooleanValue ()): SklearnEstimator . __init__ ( self ) _ComplementNB . __init__ ( self , fit_prior = fit_prior , norm = norm ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.naive_bayes import GaussianNB as _GaussianNB @nice_repr class GaussianNB ( _GaussianNB , SklearnEstimator ): def __init__ ( self ,): SklearnEstimator . __init__ ( self ) _GaussianNB . __init__ ( self ,) def run ( self , X : MatrixContinuousDense , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.naive_bayes import MultinomialNB as _MultinomialNB @nice_repr class MultinomialNB ( _MultinomialNB , SklearnEstimator ): def __init__ ( self , fit_prior : grammar . BooleanValue ()): SklearnEstimator . __init__ ( self ) _MultinomialNB . __init__ ( self , fit_prior = fit_prior ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.neighbors._classification import ( KNeighborsClassifier as _KNeighborsClassifier , ) @nice_repr class KNeighborsClassifier ( _KNeighborsClassifier , SklearnEstimator ): def __init__ ( self , n_neighbors : grammar . DiscreteValue ( min = 1 , max = 9 ), weights : grammar . CategoricalValue ( \"distance\" , \"uniform\" ), algorithm : grammar . CategoricalValue ( \"auto\" , \"ball_tree\" , \"brute\" , \"kd_tree\" ), leaf_size : grammar . DiscreteValue ( min = 1 , max = 59 ), p : grammar . DiscreteValue ( min = 1 , max = 3 ), metric : grammar . CategoricalValue ( \"minkowski\" ), ): SklearnEstimator . __init__ ( self ) _KNeighborsClassifier . __init__ ( self , n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , p = p , metric = metric , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.neighbors._graph import KNeighborsTransformer as _KNeighborsTransformer @nice_repr class KNeighborsTransformer ( _KNeighborsTransformer , SklearnTransformer ): def __init__ ( self , mode : grammar . CategoricalValue ( \"connectivity\" , \"distance\" ), n_neighbors : grammar . DiscreteValue ( min = 1 , max = 9 ), algorithm : grammar . CategoricalValue ( \"auto\" , \"ball_tree\" , \"brute\" , \"kd_tree\" ), leaf_size : grammar . DiscreteValue ( min = 1 , max = 59 ), p : grammar . DiscreteValue ( min = 1 , max = 3 ), ): SklearnTransformer . __init__ ( self ) _KNeighborsTransformer . __init__ ( self , mode = mode , n_neighbors = n_neighbors , algorithm = algorithm , leaf_size = leaf_size , p = p , ) def run ( self , input : MatrixContinuous ) -> MatrixContinuousSparse : return SklearnTransformer . run ( self , input ) from sklearn.neighbors._graph import ( RadiusNeighborsTransformer as _RadiusNeighborsTransformer , ) @nice_repr class RadiusNeighborsTransformer ( _RadiusNeighborsTransformer , SklearnTransformer ): def __init__ ( self , mode : grammar . CategoricalValue ( \"connectivity\" , \"distance\" ), radius : grammar . ContinuousValue ( min =- 9.995 , max = 9.991 ), algorithm : grammar . CategoricalValue ( \"auto\" , \"ball_tree\" , \"brute\" , \"kd_tree\" ), leaf_size : grammar . DiscreteValue ( min = 1 , max = 59 ), p : grammar . DiscreteValue ( min = 1 , max = 3 ), ): SklearnTransformer . __init__ ( self ) _RadiusNeighborsTransformer . __init__ ( self , mode = mode , radius = radius , algorithm = algorithm , leaf_size = leaf_size , p = p , ) def run ( self , input : MatrixContinuous ) -> MatrixContinuousSparse : return SklearnTransformer . run ( self , input ) from sklearn.neighbors._lof import LocalOutlierFactor as _LocalOutlierFactor @nice_repr class LocalOutlierFactor ( _LocalOutlierFactor , SklearnEstimator ): def __init__ ( self , n_neighbors : grammar . DiscreteValue ( min = 1 , max = 39 ), algorithm : grammar . CategoricalValue ( \"auto\" , \"ball_tree\" , \"brute\" , \"kd_tree\" ), leaf_size : grammar . DiscreteValue ( min = 1 , max = 59 ), p : grammar . DiscreteValue ( min = 1 , max = 3 ), contamination : grammar . CategoricalValue ( \"auto\" ), novelty : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _LocalOutlierFactor . __init__ ( self , n_neighbors = n_neighbors , algorithm = algorithm , leaf_size = leaf_size , p = p , contamination = contamination , novelty = novelty , ) def run ( self , input : MatrixContinuous ) -> VectorDiscrete : return SklearnTransformer . run ( self , input , None ) from sklearn.neighbors._nearest_centroid import NearestCentroid as _NearestCentroid @nice_repr class NearestCentroid ( _NearestCentroid , SklearnEstimator ): def __init__ ( self ,): SklearnEstimator . __init__ ( self ) _NearestCentroid . __init__ ( self ,) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.neighbors._regression import KNeighborsRegressor as _KNeighborsRegressor @nice_repr class KNeighborsRegressor ( _KNeighborsRegressor , SklearnEstimator ): def __init__ ( self , n_neighbors : grammar . DiscreteValue ( min = 1 , max = 9 ), weights : grammar . CategoricalValue ( \"distance\" , \"uniform\" ), algorithm : grammar . CategoricalValue ( \"auto\" , \"ball_tree\" , \"brute\" , \"kd_tree\" ), leaf_size : grammar . DiscreteValue ( min = 1 , max = 59 ), p : grammar . DiscreteValue ( min = 1 , max = 3 ), metric : grammar . CategoricalValue ( \"minkowski\" ), ): SklearnEstimator . __init__ ( self ) _KNeighborsRegressor . __init__ ( self , n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , p = p , metric = metric , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.neighbors._regression import ( RadiusNeighborsRegressor as _RadiusNeighborsRegressor , ) @nice_repr class RadiusNeighborsRegressor ( _RadiusNeighborsRegressor , SklearnEstimator ): def __init__ ( self , radius : grammar . ContinuousValue ( min =- 9.995 , max = 9.991 ), weights : grammar . CategoricalValue ( \"distance\" , \"uniform\" ), algorithm : grammar . CategoricalValue ( \"auto\" , \"ball_tree\" , \"brute\" , \"kd_tree\" ), leaf_size : grammar . DiscreteValue ( min = 1 , max = 59 ), p : grammar . DiscreteValue ( min = 1 , max = 3 ), ): SklearnEstimator . __init__ ( self ) _RadiusNeighborsRegressor . __init__ ( self , radius = radius , weights = weights , algorithm = algorithm , leaf_size = leaf_size , p = p , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.preprocessing._data import KernelCenterer as _KernelCenterer @nice_repr class KernelCenterer ( _KernelCenterer , SklearnTransformer ): def __init__ ( self ,): SklearnTransformer . __init__ ( self ) _KernelCenterer . __init__ ( self ,) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.preprocessing._data import MinMaxScaler as _MinMaxScaler @nice_repr class MinMaxScaler ( _MinMaxScaler , SklearnTransformer ): def __init__ ( self ,): SklearnTransformer . __init__ ( self ) _MinMaxScaler . __init__ ( self ,) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.preprocessing._data import PowerTransformer as _PowerTransformer @nice_repr class PowerTransformer ( _PowerTransformer , SklearnTransformer ): def __init__ ( self , standardize : grammar . BooleanValue ()): SklearnTransformer . __init__ ( self ) _PowerTransformer . __init__ ( self , standardize = standardize ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.preprocessing._data import RobustScaler as _RobustScaler @nice_repr class RobustScaler ( _RobustScaler , SklearnTransformer ): def __init__ ( self , with_centering : grammar . BooleanValue (), with_scaling : grammar . BooleanValue (), ): SklearnTransformer . __init__ ( self ) _RobustScaler . __init__ ( self , with_centering = with_centering , with_scaling = with_scaling ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.preprocessing._data import StandardScaler as _StandardScaler @nice_repr class StandardScaler ( _StandardScaler , SklearnTransformer ): def __init__ ( self , with_mean : grammar . BooleanValue (), with_std : grammar . BooleanValue () ): SklearnTransformer . __init__ ( self ) _StandardScaler . __init__ ( self , with_mean = with_mean , with_std = with_std ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.preprocessing._discretization import KBinsDiscretizer as _KBinsDiscretizer @nice_repr class KBinsDiscretizer ( _KBinsDiscretizer , SklearnTransformer ): def __init__ ( self , n_bins : grammar . DiscreteValue ( min = 2 , max = 9 ), encode : grammar . CategoricalValue ( \"onehot\" , \"ordinal\" ), strategy : grammar . CategoricalValue ( \"kmeans\" , \"quantile\" , \"uniform\" ), ): SklearnTransformer . __init__ ( self ) _KBinsDiscretizer . __init__ ( self , n_bins = n_bins , encode = encode , strategy = strategy ) def run ( self , input : MatrixContinuousDense ) -> MatrixContinuousSparse : return SklearnTransformer . run ( self , input ) from sklearn.preprocessing._encoders import OneHotEncoder as _OneHotEncoder @nice_repr class OneHotEncoder ( _OneHotEncoder , SklearnTransformer ): def __init__ ( self , categories : grammar . CategoricalValue ( \"auto\" ), sparse : grammar . BooleanValue (), handle_unknown : grammar . CategoricalValue ( \"error\" , \"ignore\" ), ): SklearnTransformer . __init__ ( self ) _OneHotEncoder . __init__ ( self , categories = categories , sparse = sparse , handle_unknown = handle_unknown ) def run ( self , input : MatrixCategorical ) -> MatrixContinuousSparse : return SklearnTransformer . run ( self , input ) from sklearn.preprocessing._encoders import OrdinalEncoder as _OrdinalEncoder @nice_repr class OrdinalEncoder ( _OrdinalEncoder , SklearnTransformer ): def __init__ ( self , categories : grammar . CategoricalValue ( \"auto\" )): SklearnTransformer . __init__ ( self ) _OrdinalEncoder . __init__ ( self , categories = categories ) def run ( self , input : MatrixCategorical ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.preprocessing._label import LabelBinarizer as _LabelBinarizer @nice_repr class LabelBinarizer ( _LabelBinarizer , SklearnTransformer ): def __init__ ( self , sparse_output : grammar . BooleanValue ()): SklearnTransformer . __init__ ( self ) _LabelBinarizer . __init__ ( self , sparse_output = sparse_output ) def run ( self , input : VectorCategorical ) -> MatrixContinuousDense : return SklearnTransformer . run ( self , input ) from sklearn.svm._classes import LinearSVC as _LinearSVC @nice_repr class LinearSVC ( _LinearSVC , SklearnEstimator ): def __init__ ( self , penalty : grammar . CategoricalValue ( \"l2\" ), loss : grammar . CategoricalValue ( \"hinge\" , \"squared_hinge\" ), dual : grammar . BooleanValue (), C : grammar . ContinuousValue ( min = 0.005 , max = 9.991 ), multi_class : grammar . CategoricalValue ( \"crammer_singer\" , \"ovr\" ), fit_intercept : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _LinearSVC . __init__ ( self , penalty = penalty , loss = loss , dual = dual , C = C , multi_class = multi_class , fit_intercept = fit_intercept , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.svm._classes import LinearSVR as _LinearSVR @nice_repr class LinearSVR ( _LinearSVR , SklearnEstimator ): def __init__ ( self , epsilon : grammar . ContinuousValue ( min = 0.0 , max = 0.992 ), C : grammar . ContinuousValue ( min = 0.005 , max = 9.991 ), loss : grammar . CategoricalValue ( \"epsilon_insensitive\" , \"squared_epsilon_insensitive\" ), fit_intercept : grammar . BooleanValue (), intercept_scaling : grammar . ContinuousValue ( min = 0.005 , max = 9.991 ), dual : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _LinearSVR . __init__ ( self , epsilon = epsilon , C = C , loss = loss , fit_intercept = fit_intercept , intercept_scaling = intercept_scaling , dual = dual , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.svm._classes import NuSVC as _NuSVC @nice_repr class NuSVC ( _NuSVC , SklearnEstimator ): def __init__ ( self , kernel : grammar . CategoricalValue ( \"linear\" , \"poly\" , \"rbf\" , \"sigmoid\" ), degree : grammar . DiscreteValue ( min = 1 , max = 5 ), gamma : grammar . CategoricalValue ( \"auto\" , \"scale\" ), coef0 : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), shrinking : grammar . BooleanValue (), probability : grammar . BooleanValue (), cache_size : grammar . DiscreteValue ( min = 1 , max = 399 ), decision_function_shape : grammar . CategoricalValue ( \"ovo\" , \"ovr\" ), break_ties : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _NuSVC . __init__ ( self , kernel = kernel , degree = degree , gamma = gamma , coef0 = coef0 , shrinking = shrinking , probability = probability , cache_size = cache_size , decision_function_shape = decision_function_shape , break_ties = break_ties , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.svm._classes import NuSVR as _NuSVR @nice_repr class NuSVR ( _NuSVR , SklearnEstimator ): def __init__ ( self , C : grammar . ContinuousValue ( min = 0.005 , max = 9.991 ), kernel : grammar . CategoricalValue ( \"linear\" , \"poly\" , \"rbf\" , \"sigmoid\" ), degree : grammar . DiscreteValue ( min = 1 , max = 5 ), gamma : grammar . CategoricalValue ( \"auto\" , \"scale\" ), coef0 : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), shrinking : grammar . BooleanValue (), cache_size : grammar . DiscreteValue ( min = 1 , max = 399 ), ): SklearnEstimator . __init__ ( self ) _NuSVR . __init__ ( self , C = C , kernel = kernel , degree = degree , gamma = gamma , coef0 = coef0 , shrinking = shrinking , cache_size = cache_size , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.svm._classes import OneClassSVM as _OneClassSVM @nice_repr class OneClassSVM ( _OneClassSVM , SklearnEstimator ): def __init__ ( self , kernel : grammar . CategoricalValue ( \"linear\" , \"poly\" , \"rbf\" , \"sigmoid\" ), degree : grammar . DiscreteValue ( min = 1 , max = 5 ), gamma : grammar . CategoricalValue ( \"auto\" , \"scale\" ), coef0 : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), shrinking : grammar . BooleanValue (), cache_size : grammar . DiscreteValue ( min = 1 , max = 399 ), ): SklearnEstimator . __init__ ( self ) _OneClassSVM . __init__ ( self , kernel = kernel , degree = degree , gamma = gamma , coef0 = coef0 , shrinking = shrinking , cache_size = cache_size , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.svm._classes import SVC as _SVC @nice_repr class SVC ( _SVC , SklearnEstimator ): def __init__ ( self , C : grammar . ContinuousValue ( min = 0.005 , max = 9.991 ), degree : grammar . DiscreteValue ( min = 1 , max = 5 ), gamma : grammar . CategoricalValue ( \"auto\" , \"scale\" ), coef0 : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), shrinking : grammar . BooleanValue (), probability : grammar . BooleanValue (), cache_size : grammar . DiscreteValue ( min = 1 , max = 399 ), decision_function_shape : grammar . CategoricalValue ( \"ovo\" , \"ovr\" ), break_ties : grammar . BooleanValue (), ): SklearnEstimator . __init__ ( self ) _SVC . __init__ ( self , C = C , degree = degree , gamma = gamma , coef0 = coef0 , shrinking = shrinking , probability = probability , cache_size = cache_size , decision_function_shape = decision_function_shape , break_ties = break_ties , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.svm._classes import SVR as _SVR @nice_repr class SVR ( _SVR , SklearnEstimator ): def __init__ ( self , kernel : grammar . CategoricalValue ( \"linear\" , \"poly\" , \"rbf\" , \"sigmoid\" ), degree : grammar . DiscreteValue ( min = 1 , max = 5 ), gamma : grammar . CategoricalValue ( \"auto\" , \"scale\" ), coef0 : grammar . ContinuousValue ( min =- 0.992 , max = 0.992 ), C : grammar . ContinuousValue ( min = 0.005 , max = 9.991 ), epsilon : grammar . ContinuousValue ( min = 0.006 , max = 0.993 ), shrinking : grammar . BooleanValue (), cache_size : grammar . DiscreteValue ( min = 1 , max = 399 ), ): SklearnEstimator . __init__ ( self ) _SVR . __init__ ( self , kernel = kernel , degree = degree , gamma = gamma , coef0 = coef0 , C = C , epsilon = epsilon , shrinking = shrinking , cache_size = cache_size , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.tree._classes import DecisionTreeClassifier as _DecisionTreeClassifier @nice_repr class DecisionTreeClassifier ( _DecisionTreeClassifier , SklearnEstimator ): def __init__ ( self , min_samples_split : grammar . DiscreteValue ( min = 2 , max = 3 ), min_weight_fraction_leaf : grammar . ContinuousValue ( min = 0.0 , max = 0.5 ), min_impurity_decrease : grammar . ContinuousValue ( min = 0.0 , max = 0.992 ), ccp_alpha : grammar . ContinuousValue ( min = 0.0 , max = 0.992 ), ): SklearnEstimator . __init__ ( self ) _DecisionTreeClassifier . __init__ ( self , min_samples_split = min_samples_split , min_weight_fraction_leaf = min_weight_fraction_leaf , min_impurity_decrease = min_impurity_decrease , ccp_alpha = ccp_alpha , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.tree._classes import DecisionTreeRegressor as _DecisionTreeRegressor @nice_repr class DecisionTreeRegressor ( _DecisionTreeRegressor , SklearnEstimator ): def __init__ ( self , min_samples_split : grammar . DiscreteValue ( min = 2 , max = 3 ), min_weight_fraction_leaf : grammar . ContinuousValue ( min = 0.0 , max = 0.5 ), min_impurity_decrease : grammar . ContinuousValue ( min = 0.0 , max = 0.992 ), ccp_alpha : grammar . ContinuousValue ( min = 0.0 , max = 0.992 ), ): SklearnEstimator . __init__ ( self ) _DecisionTreeRegressor . __init__ ( self , min_samples_split = min_samples_split , min_weight_fraction_leaf = min_weight_fraction_leaf , min_impurity_decrease = min_impurity_decrease , ccp_alpha = ccp_alpha , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) from sklearn.tree._classes import ExtraTreeClassifier as _ExtraTreeClassifier @nice_repr class ExtraTreeClassifier ( _ExtraTreeClassifier , SklearnEstimator ): def __init__ ( self , min_samples_split : grammar . DiscreteValue ( min = 2 , max = 3 ), min_weight_fraction_leaf : grammar . ContinuousValue ( min = 0.0 , max = 0.5 ), min_impurity_decrease : grammar . ContinuousValue ( min = 0.0 , max = 0.992 ), ccp_alpha : grammar . ContinuousValue ( min = 0.0 , max = 0.992 ), ): SklearnEstimator . __init__ ( self ) _ExtraTreeClassifier . __init__ ( self , min_samples_split = min_samples_split , min_weight_fraction_leaf = min_weight_fraction_leaf , min_impurity_decrease = min_impurity_decrease , ccp_alpha = ccp_alpha , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorCategorical ] ) -> VectorCategorical : return SklearnEstimator . run ( self , X , y ) from sklearn.tree._classes import ExtraTreeRegressor as _ExtraTreeRegressor @nice_repr class ExtraTreeRegressor ( _ExtraTreeRegressor , SklearnEstimator ): def __init__ ( self , min_samples_split : grammar . DiscreteValue ( min = 2 , max = 3 ), min_weight_fraction_leaf : grammar . ContinuousValue ( min = 0.0 , max = 0.5 ), min_impurity_decrease : grammar . ContinuousValue ( min = 0.0 , max = 0.992 ), ccp_alpha : grammar . ContinuousValue ( min = 0.0 , max = 0.992 ), ): SklearnEstimator . __init__ ( self ) _ExtraTreeRegressor . __init__ ( self , min_samples_split = min_samples_split , min_weight_fraction_leaf = min_weight_fraction_leaf , min_impurity_decrease = min_impurity_decrease , ccp_alpha = ccp_alpha , ) def run ( self , X : MatrixContinuous , y : Supervised [ VectorContinuous ] ) -> VectorContinuous : return SklearnEstimator . run ( self , X , y ) __all__ = [ \"AffinityPropagation\" , \"FeatureAgglomeration\" , \"Birch\" , \"KMeans\" , \"MiniBatchKMeans\" , \"MeanShift\" , \"FactorAnalysis\" , \"FastICA\" , \"IncrementalPCA\" , \"KernelPCA\" , \"LatentDirichletAllocation\" , \"NMF\" , \"PCA\" , \"MiniBatchSparsePCA\" , \"SparsePCA\" , \"TruncatedSVD\" , \"CountVectorizer\" , \"HashingVectorizer\" , \"TfidfTransformer\" , \"TfidfVectorizer\" , \"KNNImputer\" , \"LinearRegression\" , \"ARDRegression\" , \"BayesianRidge\" , \"ElasticNet\" , \"Lasso\" , \"HuberRegressor\" , \"Lars\" , \"LassoLars\" , \"LassoLarsIC\" , \"LogisticRegression\" , \"OrthogonalMatchingPursuit\" , \"PassiveAggressiveClassifier\" , \"PassiveAggressiveRegressor\" , \"Perceptron\" , \"Ridge\" , \"RidgeClassifier\" , \"SGDClassifier\" , \"SGDRegressor\" , \"TheilSenRegressor\" , \"Isomap\" , \"LocallyLinearEmbedding\" , \"BernoulliNB\" , \"CategoricalNB\" , \"ComplementNB\" , \"GaussianNB\" , \"MultinomialNB\" , \"KNeighborsClassifier\" , \"KNeighborsTransformer\" , \"RadiusNeighborsTransformer\" , \"LocalOutlierFactor\" , \"NearestCentroid\" , \"KNeighborsRegressor\" , \"RadiusNeighborsRegressor\" , \"KernelCenterer\" , \"MinMaxScaler\" , \"PowerTransformer\" , \"RobustScaler\" , \"StandardScaler\" , \"KBinsDiscretizer\" , \"OneHotEncoder\" , \"OrdinalEncoder\" , \"LabelBinarizer\" , \"LinearSVC\" , \"LinearSVR\" , \"NuSVC\" , \"NuSVR\" , \"OneClassSVM\" , \"SVC\" , \"SVR\" , \"DecisionTreeClassifier\" , \"DecisionTreeRegressor\" , \"ExtraTreeClassifier\" , \"ExtraTreeRegressor\" , ]","title":"Autogoal.contrib.sklearn. generated"},{"location":"api/autogoal.contrib.sklearn._manual/","text":"from sklearn.feature_extraction.text import CountVectorizer as _CountVectorizer from sklearn.feature_extraction import DictVectorizer from sklearn_crfsuite import CRF from autogoal.contrib.sklearn._builder import SklearnTransformer , SklearnEstimator from autogoal.kb import * from autogoal.grammar import ( BooleanValue , CategoricalValue , DiscreteValue , ContinuousValue , ) from autogoal.utils import nice_repr from autogoal.kb import AlgorithmBase , Supervised @nice_repr class CountVectorizerNoTokenize ( _CountVectorizer , SklearnTransformer ): def __init__ ( self , lowercase : BooleanValue (), stopwords_remove : BooleanValue (), binary : BooleanValue (), inner_tokenizer : algorithm ( Sentence , Seq [ Word ]), inner_stemmer : algorithm ( Word , Stem ), inner_stopwords : algorithm ( Seq [ Word ], Seq [ Word ]), ): self . stopwords_remove = stopwords_remove self . inner_tokenizer = inner_tokenizer self . inner_stemmer = inner_stemmer self . inner_stopwords = inner_stopwords SklearnTransformer . __init__ ( self ) _CountVectorizer . __init__ ( self , lowercase = lowercase , binary = binary ) def build_tokenizer ( self ): def func ( sentence ): tokens = self . inner_tokenizer . run ( sentence ) tokens = ( self . inner_stopwords . run ( sentence ) if self . stopwords_remove else tokens ) return [ self . inner_stemmer . run ( token ) for token in tokens ] return func def run ( self , input : Seq [ Sentence ]) -> MatrixContinuousSparse : return SklearnTransformer . run ( self , input ) class _FeatureVectorizer ( SklearnTransformer ): def __init__ ( self , sparse ): self . vectorizer = DictVectorizer ( sparse = sparse ) super () . __init__ () def fit_transform ( self , X , y = None ): return self . vectorizer . fit_transform ( X ) def transform ( self , X , y = None ): return self . vectorizer . transform ( X , y = y ) @nice_repr class FeatureSparseVectorizer ( _FeatureVectorizer ): def __init__ ( self ): super () . __init__ ( sparse = True ) def run ( self , input : Seq [ FeatureSet ]) -> MatrixContinuousSparse : return super () . run ( input ) @nice_repr class FeatureDenseVectorizer ( _FeatureVectorizer ): def __init__ ( self ): super () . __init__ ( sparse = False ) def run ( self , input : Seq [ FeatureSet ]) -> MatrixContinuousDense : return super () . run ( input ) @nice_repr class CRFTagger ( CRF , SklearnEstimator ): def __init__ ( self , algorithm : CategoricalValue ( \"lbfgs\" , \"l2sgd\" , \"ap\" , \"pa\" , \"arow\" ) ) -> None : SklearnEstimator . __init__ ( self ) super () . __init__ ( algorithm = algorithm ) def run ( self , X : Seq [ Seq [ FeatureSet ]], y : Supervised [ Seq [ Seq [ Label ]]] ) -> Seq [ Seq [ Label ]]: return SklearnEstimator . run ( self , X , y ) __all__ = [ \"CountVectorizerNoTokenize\" , \"FeatureSparseVectorizer\" , \"FeatureDenseVectorizer\" , \"CRFTagger\" , ]","title":"Autogoal.contrib.sklearn. manual"},{"location":"api/autogoal.contrib.sklearn._utils/","text":"import warnings import numpy as np import scipy.sparse as sp class String ( str ): def __new__ ( cls , x ): return str . __new__ ( cls , x ) def _get_class_name ( cls ): return str ( cls ) . split ( \".\" )[ - 1 ] def combine_types ( * types ): if len ( types ) == 1 : return types [ 0 ] types = set ( types ) if types == { kb . MatrixContinuousDense (), kb . MatrixContinuousSparse ()}: return kb . MatrixContinuous () return None def is_matrix_continuous_dense ( obj ): Note \"\"\"Determine if obj is a continuous dense matrix (i.e., NxM floats). Examples: is_matrix_continuous_dense([[0, 1],[0, 1]]) True is_matrix_continuous_dense([[0,1]]) True is_matrix_continuous_dense([0,1]) False is_matrix_continuous_dense(np.random.rand(10,10)) True is_matrix_continuous_dense(np.random.rand(10)) False try : obj = np . asarray ( obj ) return len ( obj . shape ) == 2 except : return False def is_matrix_continuous_sparse ( obj ): Note \"\"\"Determine if obj is a continuous sparse matrix (i.e., NxM floats). Examples: is_matrix_continuous_sparse(sp.rand(10,10)) True is_matrix_continuous_sparse([[0,1]]) False is_matrix_continuous_sparse([0,1]) False is_matrix_continuous_sparse(np.random.rand(10,10)) False is_matrix_continuous_sparse(np.random.rand(10)) False try : sp . rand return sp . issparse ( obj ) and len ( obj . shape ) == 2 except : return False def is_categorical ( obj ): Note \"\"\"Determines if obj is a sequence of categories (integer or string) Examples: is_categorical(['A'] * 5 + ['B'] * 5) True is_categorical(np.asarray(list('ABCABCABC'))) True try : obj = np . asarray ( obj ) assert len ( obj . shape ) == 1 original_length = len ( obj ) obj = set ( obj ) a = len ( obj ) < max ( 0.1 * original_length , 10 ) b = all ( isinstance ( x , ( str , int , np . int64 , np . int32 )) for x in obj ) return a and b except : return False def is_continuous ( obj ): Note \"\"\"Determines if obj is a sequence of float values Examples: is_continuous(np.random.rand(10)) True is_continuous(np.random.rand(10,10)) False try : obj = np . asarray ( obj ) assert len ( obj . shape ) == 1 return not all ( obj . astype ( int ) == obj ) except : return False def is_discrete ( obj ): Note \"\"\"Determines if obj is a sequence of integer values Examples: is_discrete(np.random.randint(0,1,(10,))) True is_discrete(np.random.rand(10)) False try : obj = np . asarray ( obj ) assert len ( obj . shape ) == 1 return all ( obj . astype ( int ) == obj ) except : return False def is_string_list ( obj ): Note \"\"\"Determines if obj is a sequence of strings. Examples: is_string_list(['hello world', 'another sentence']) True is_string_list(np.random.rand(10)) False try : obj = np . asarray ( obj ) assert len ( obj . shape ) == 1 original_length = len ( obj ) obj = set ( obj ) return len ( obj ) > 0.1 * original_length and all ( isinstance ( x , str ) for x in obj ) except : return False from autogoal import kb DATA_RESOLVERS = { kb . MatrixContinuousDense : is_matrix_continuous_dense , kb . MatrixContinuousSparse : is_matrix_continuous_sparse , kb . VectorCategorical : is_categorical , kb . VectorContinuous : is_continuous , kb . Seq [ kb . Sentence ]: is_string_list , } DATA_TYPE_EXAMPLES = { kb . MatrixContinuousDense : np . random . rand ( 10 , 10 ), kb . MatrixContinuousSparse : sp . rand ( 10 , 10 ), kb . VectorCategorical : np . asarray ([ \"A\" ] * 5 + [ \"B\" ] * 5 ), kb . VectorContinuous : np . random . rand ( 10 ), kb . VectorDiscrete : np . random . randint ( 0 , 10 , ( 10 ,), dtype = int ), kb . Seq [ kb . Sentence ]: [ \"abc bcd def feg geh hij jkl lmn nop pqr\" ] * 10 , } def is_algorithm ( cls , verbose = False ): if hasattr ( cls , \"fit\" ) and hasattr ( cls , \"predict\" ): return \"estimator\" else : if verbose : warnings . warn ( \" %r doesn't have `fit`\" % cls ) if hasattr ( cls , \"transform\" ): return \"transformer\" else : if verbose : warnings . warn ( \" %r doesn't have `transform`\" % cls ) return False def is_classifier ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an sklearn classifier. If True, returns the valid (input, output) types. Examples: from sklearn.linear_model import LogisticRegression, LinearRegression is_classifier(LogisticRegression) (True, (Tuple(MatrixContinuous(), CategoricalVector()), CategoricalVector())) is_classifier(LinearRegression) (False, None) if not is_algorithm ( cls , verbose = verbose ): return False , None inputs = [] for input_type in [ kb . MatrixContinuousDense (), kb . MatrixContinuousSparse ()]: try : X = DATA_TYPE_EXAMPLES [ input_type ] y = DATA_TYPE_EXAMPLES [ kb . CategoricalVector ()] clf = cls () clf . fit ( X , y ) y = clf . predict ( X ) assert is_categorical ( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( kb . Tuple ( inputs , kb . CategoricalVector ()), kb . CategoricalVector ()) else : return False , None def is_regressor ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an sklearn regressor. If True, returns the valid (input, output) types. Examples: from sklearn.linear_model import LogisticRegression, LinearRegression is_regressor(LogisticRegression) (False, None) is_regressor(LinearRegression) (True, (Tuple(MatrixContinuous(), ContinuousVector()), ContinuousVector())) if not is_algorithm ( cls , verbose = verbose ): return False , None inputs = [] for input_type in [ kb . MatrixContinuousDense (), kb . MatrixContinuousSparse ()]: try : X = DATA_TYPE_EXAMPLES [ input_type ] y = DATA_TYPE_EXAMPLES [ kb . ContinuousVector ()] clf = cls () clf . fit ( X , y ) y = clf . predict ( X ) assert is_continuous ( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( kb . Tuple ( inputs , kb . ContinuousVector ()), kb . ContinuousVector ()) else : return False , None def is_clusterer ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an sklearn clustering algorithm. If True, returns the valid (input, output) types. Examples: from sklearn.linear_model import LogisticRegression, LinearRegression is_clusterer(LogisticRegression) (False, None) is_clusterer(LinearRegression) (False, None) from sklearn.cluster import KMeans is_clusterer(KMeans) (True, (MatrixContinuous(), DiscreteVector())) if not is_algorithm ( cls , verbose = verbose ): return False , None inputs = [] for input_type in [ kb . MatrixContinuousDense (), kb . MatrixContinuousSparse ()]: try : X = DATA_TYPE_EXAMPLES [ input_type ] clf = cls () y = clf . fit_predict ( X ) assert is_discrete ( y ) inputs . append ( input_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) inputs = combine_types ( * inputs ) if inputs : return True , ( inputs , kb . DiscreteVector ()) else : return False , None def is_transformer ( cls , verbose = False ): Note \"\"\"Determine if cls corresponds to something that resembles an sklearn general transformer. If True, returns the valid (input, output) types. Examples: from sklearn.feature_extraction.text import CountVectorizer is_transformer(CountVectorizer) (True, (List(Sentence()), MatrixContinuousSparse())) from sklearn.decomposition.pca import PCA is_transformer(PCA) (True, (MatrixContinuousDense(), MatrixContinuousDense())) if not is_algorithm ( cls , verbose = verbose ): return False , None allowed_inputs = set () allowed_outputs = set () for input_type in [ kb . MatrixContinuousDense (), kb . MatrixContinuousSparse (), kb . List ( kb . Sentence ()), ]: for output_type in [ kb . MatrixContinuousDense (), kb . MatrixContinuousSparse (), kb . List ( kb . Sentence ()), ]: try : X = DATA_TYPE_EXAMPLES [ input_type ] clf = cls () X = clf . fit_transform ( X ) assert is_data_type ( X , output_type ) allowed_inputs . add ( input_type ) allowed_outputs . add ( output_type ) except Exception as e : if verbose : warnings . warn ( str ( e )) if len ( allowed_outputs ) != 1 : return False , None inputs = combine_types ( * allowed_inputs ) if allowed_inputs : return True , ( inputs , list ( allowed_outputs )[ 0 ]) else : return False , None def is_data_type ( X , data_type ): return DATA_RESOLVERS [ data_type ]( X ) IO_TYPE_HANDLER = [ is_classifier , is_regressor , is_clusterer , is_transformer ] def get_input_output ( cls , verbose = False ): for func in IO_TYPE_HANDLER : matches , types = func ( cls , verbose = verbose ) if matches : return types return None , None def solve_type ( obj ): for type_ , resolver in DATA_RESOLVERS . items (): if resolver ( obj ): return type_ raise ValueError ( \"Unresolved type for %r \" % obj )","title":"Autogoal.contrib.sklearn. utils"},{"location":"api/autogoal.contrib.spacy.__init__/","text":"try : import spacy except : print ( \"(!) Code in `autogoal.contrib.spacy` requires `spacy =^2.2.3`.\" ) print ( \"(!) You can install it with `pip install autogoal[spacy]`.\" ) raise ImportError () from autogoal.contrib.spacy._base import SpacyNLP","title":"Autogoal.contrib.spacy.  init  "},{"location":"api/autogoal.contrib.spacy._base/","text":"from autogoal.kb import AlgorithmBase import spacy from autogoal.grammar import CategoricalValue , BooleanValue from autogoal.kb import Sentence , Word , FeatureSet , Seq from autogoal.kb import Supervised from autogoal.utils import nice_repr @nice_repr class SpacyNLP ( AlgorithmBase ): def __init__ ( self , language : CategoricalValue ( \"en\" , \"es\" ), extract_pos : BooleanValue (), extract_lemma : BooleanValue (), extract_pos_tag : BooleanValue (), extract_dep : BooleanValue (), extract_entity : BooleanValue (), extract_details : BooleanValue (), extract_sentiment : BooleanValue (), ): self . language = language self . extract_pos = extract_pos self . extract_lemma = extract_lemma self . extract_pos_tag = extract_pos_tag self . extract_dep = extract_dep self . extract_entity = extract_entity self . extract_details = extract_details self . extract_sentiment = extract_sentiment self . _nlp = None @property def nlp ( self ): if self . _nlp is None : self . _nlp = spacy . load ( self . language ) return self . _nlp def run ( self , input : Sentence ) -> Seq [ FeatureSet ]: tokenized = self . nlp ( input ) tokens = [] flags = [] for token in tokenized : token_flags = {} token_flags [ \"text\" ] = token . text if self . extract_lemma : token_flags [ \"lemma\" ] = token . lemma_ if self . extract_pos_tag : token_flags [ \"pos\" ] = token . pos_ for kv in token . tag_ . split ( \"|\" ): kv = kv . split ( \"=\" ) if len ( kv ) == 2 : token_flags [ \"tag_\" + kv [ 0 ]] = kv [ 1 ] else : token_flags [ \"tag_\" + kv [ 0 ]] = True if self . extract_dep : token_flags [ \"dep\" ] = token . dep_ if self . extract_entity : token_flags [ \"ent_type\" ] = token . ent_type_ token_flags [ \"ent_kb_id\" ] = token . ent_kb_id_ if self . extract_details : token_flags [ \"is_alpha\" ] = token . is_alpha token_flags [ \"is_ascii\" ] = token . is_ascii token_flags [ \"is_digit\" ] = token . is_digit token_flags [ \"is_lower\" ] = token . is_lower token_flags [ \"is_upper\" ] = token . is_upper token_flags [ \"is_title\" ] = token . is_title token_flags [ \"is_punct\" ] = token . is_punct token_flags [ \"is_left_punct\" ] = token . is_left_punct token_flags [ \"is_right_punct\" ] = token . is_right_punct token_flags [ \"is_space\" ] = token . is_space token_flags [ \"is_bracket\" ] = token . is_bracket token_flags [ \"is_quote\" ] = token . is_quote token_flags [ \"is_currency\" ] = token . is_currency token_flags [ \"like_url\" ] = token . like_url token_flags [ \"like_num\" ] = token . like_num token_flags [ \"like_email\" ] = token . like_email token_flags [ \"is_oov\" ] = token . is_oov token_flags [ \"is_stop\" ] = token . is_stop if self . extract_sentiment : token_flags [ \"sentiment\" ] = token . sentiment flags . append ( token_flags ) return flags","title":"Autogoal.contrib.spacy. base"},{"location":"api/autogoal.contrib.streamlit.__init__/","text":"try : import streamlit as st except ImportError : print ( \"(!) The code inside `autogoal.contrib.streamlit` requires `streamlit>=0.55`.\" ) print ( \"(!) Fix it by running `pip install autogoal[streamlit]`.\" ) raise from autogoal.search import Logger class StreamlitLogger ( Logger ): def __init__ ( self ): self . evaluations = 0 self . current = 0 self . status = st . info ( \"Waiting for evaluation start.\" ) self . progress = st . progress ( 0 ) self . error_log = st . empty () self . best_fn = 0 self . chart = st . line_chart ([ dict ( current = 0.0 , best = 0.0 )]) self . current_pipeline = st . code ( \"\" ) self . best_pipeline = None def begin ( self , evaluations , pop_size ): self . status . info ( f \"Starting evaluation for { evaluations } iterations.\" ) self . progress . progress ( 0 ) self . evaluations = evaluations def update_best ( self , new_best , new_fn , previous_best , previous_fn ): self . best_fn = new_fn self . best_pipeline = repr ( new_best ) def sample_solution ( self , solution ): self . current += 1 self . status . info ( f \"\"\" [Best= { self . best_fn : 0.3 } ] \ud83d\udd50 Iteration { self . current } / { self . evaluations } . Note ) self.progress.progress(self.current / self.evaluations) self.current_pipeline.code(repr(solution)) def eval_solution(self, solution, fitness): self.chart.add_rows([dict(current=fitness, best=self.best_fn)]) def end(self, best, best_fn): self.status.success( f\"\"\" Evaluation completed: \ud83d\udc4d Best solution={best_fn:0.3} ) self . progress . progress ( 1.0 ) self . current_pipeline . code ( self . best_pipeline )","title":"Autogoal.contrib.streamlit.  init  "},{"location":"api/autogoal.contrib.streamlit.demo/","text":"import sys import streamlit as st import textwrap import networkx as nx import pandas as pd import altair as alt import nx_altair as nxa import inspect from autogoal.kb import build_pipeline_graph @st . cache ( allow_output_mutation = True ) def eval_code ( code , * variables ): locals_dict = {} exec ( code , globals (), locals_dict ) if len ( variables ) == 0 : return None if len ( variables ) == 1 : return locals_dict [ variables [ 0 ]] else : return [ locals_dict [ var ] for var in variables ] class Demo : def __init__ ( self ): self . main_sections = { \"Intro\" : self . intro , \"High-Level API\" : self . high_level , \"Pipelines\" : self . build_pipelines , } def intro ( self ): st . write ( \"# AutoGOAL Demos\" ) st . write ( Note Welcome to the AutoGOAL Demo. In the left sidebar you will find all the available demos and additional controls specific to each of them. AutoGOAL is a framework in Python for automatically finding the best way to solve a given task. It has been designed mainly for automatic machine learning~(AutoML) but it can be used in any scenario where several possible strategies are available to solve a given computational task. ) st . write ( Note About this demo \u00b6 The purpose of this demo application is to showcase the main use cases of AutoGOAL. Keep in mind that AutoGOAL is a software library, i.e., meant to be used from source code. This demo serves as an interactive and user-friendly introduction to the library, but it is in no case a full-featured AutoML application. There are two sections to showcase different components of AutoGOAL. You can switch sections in the left sidebar. The High-Level API section presents the public interface of AutoGOAL in several datasets. The Pipelines section shows the internal components of AutoGOAL and allows to explore the possible pipelines. ) st . write ( Note Running the code \u00b6 To execute this demo on your own infrastructure, you need AutoGOAL's docker image. There are two images available, without and without GPU support. Download the corresponding Docker image: docker pull autogoal/autogoal ) st . write ( Note Launch a Docker container. docker run --rm -p 8501:8501 autogoal/autogoal Navigate to http://localhost:8501 . ) def high_level ( self ): st . write ( \"# High-Level API\" ) st . write ( Note AutoGOAL is first and foremost a framework for Automatic Machine Learning. With a few simple lines of code, you can quickly find a close to optimal solution for classic machine learning problems. ) from autogoal import datasets dataset_descriptions = { \"cars\" : \"\"\" [Cars](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) is a low-dimensionality supervised problem with 21 one-hot encoded features. Note \"\"\", \"german_credit\": \"\"\" German Credit is a low-dimensionality supervised problem with 20 categorical or numerical features. \"abalone\" : \"\"\" [Abalone](https://archive.ics.uci.edu/ml/datasets/Abalone) is a low-dimensionality supervised problem with 8 categorical or numerical features. Note \"\"\", \"shuttle\": \"\"\" Shuttle is a low-dimensionality supervised problem with 9 numerical features. \"yeast\" : \"\"\" [Yeast](https://archive.ics.uci.edu/ml/datasets/Yeast) is a low-dimensionality supervised problem with 9 numerical features. Note \"\"\", \"dorothea\": \"\"\" Dorothea is a high-dimensionality sparse supervised problem with 100,000 numerical features. \"gisette\" : \"\"\" [Gisette](https://archive.ics.uci.edu/ml/datasets/Gisette) is a high-dimensionality sparse supervised problem with 5,000 numerical features. Note \"\"\", \"haha\": \"\"\" HAHA 2019 is a text classification problem with binary classes in Spanish. \"meddocan\" : \"\"\" [MEDDOCAN 2019](https://github.com/PlanTL-SANIDAD/SPACCC_MEDDOCAN) is an entity recognition problem in Spanish medical documents. Note \"\"\", } override_types = { \"german_credit\": (\"MatrixContinuousDense()\", \"CategoricalVector()\"), \"dorothea\": (\"MatrixContinuousSparse()\", \"CategoricalVector()\"), \"gisette\": (\"MatrixContinuousSparse()\", \"CategoricalVector()\"), \"haha\": (\"List(Sentence())\", \"CategoricalVector()\"), \"meddocan\": (\"List(List(Word()))\", \"List(List(Postag()))\"), } st.write( These are sample datasets which are automatically downloaded by AutoGOAL , and can be used to benchmark new algorithms and showcase AutoML tools . Note ) dataset = st.selectbox(\"Select a dataset\", list(dataset_descriptions)) st.write( dataset_descriptions[dataset] + \"Here is the code to load this dataset.\" ) code = textwrap.dedent( f\"\"\" from autogoal.datasets import {dataset} X, y, *_ = {dataset}.load() ) st . code ( code ) X , y = eval_code ( code , \"X\" , \"y\" ) if st . checkbox ( \"Preview data\" ): try : l = len ( X ) except : l = X . shape [ 0 ] head = st . slider ( \"Preview N first items\" , 0 , l , 5 ) if isinstance ( X , list ): st . write ( X [: head ]) else : st . write ( X [: head , :]) st . write ( y [: head ]) st . write ( Note The next step is to instantiate an AutoML solver and run it on this problem. The AutoML class provides a black-box interface to AutoGOAL. You can tweak the most important parameters at the left sidebar, even though sensible defaults are provided for all the parameters. ) st . sidebar . markdown ( \"### AutoML parameters\" ) iterations = st . sidebar . number_input ( \"Number of iterations\" , 1 , 10000 , 100 ) global_timeout = st . sidebar . number_input ( \"Global timeout (seconds)\" , 1 , 1000 , 60 ) pipeline_timeout = st . sidebar . number_input ( \"Timeout per pipeline (seconds)\" , 1 , 1000 , 5 ) from autogoal.contrib.streamlit import StreamlitLogger if dataset in override_types : input_type , output_type = override_types [ dataset ] types_code = f \"\"\" input= { input_type } , output= { output_type } , Note st.info( f\"\"\" In most cases AutoGOAL can automatically infer the input and output type from the dataset. Sometimes, such as with {dataset} , the user will need to provide them explicitely. ) else : types_code = \"\" code = textwrap . dedent ( f \"\"\" from autogoal.kb import * from autogoal.ml import AutoML automl = AutoML( errors=\"ignore\", # ignore exceptions (e.g., timeouts) search_iterations= { iterations } , # total iterations search_kwargs=dict( search_timeout= { global_timeout } , # max time in total (approximate) evaluation_timeout= { pipeline_timeout } , # max time per pipeline (approximate) ), { types_code } ) Note ) st.code(code) automl = eval_code(code, \"automl\") st.write( Click run to call the ` fit ` method . Keep in mind that many of these pipelines can be quite computationally heavy and both the hyperparameter configuration as well as the infrastructure where this demo is running might not allow for the best pipelines to execute . Note ) st.code(\"automl.fit(X, y)\", language=\"Python\") if st.button(\"Run it!\"): automl.fit(X, y, logger=StreamlitLogger()) st.write( Take a look at the remaining examples in the sidebar . Note ) def build_pipelines(self): st.write(\"# Pipelines\") st.write( \"This example illustrates how AutoGOAL automatically builds \" \"a graph of pipelines for different problems settings.\" ) from autogoal.kb._data import DATA_TYPES types_str = [cls. name for cls in DATA_TYPES] st.write( AutoGOAL pipeline discovery is based on a hierarchy of semantic datatypes . Each type represents a semantic datum that can be used in a machine learning algorithm , from matrices and vectors to sentences , entities and and images . The following picture shows all available semantic data types . You can click the top right corner to enlarge . Note ) st.image(\"/code/docs/guide/datatypes.png\", use_column_width=True) from autogoal.contrib import find_classes all_classes = {k. name : k for k in find_classes()} st.write( f\"\"\" Algorithm Library \u00b6 AutoGOAL automatically builds pipelines by selecting from a wide range of algorithms implemented in contrib modules. The list of all available algorithms is shown here. There are a total of {len(all_classes)} algorithms implemented. Select one to display some information. ) class_name = st . selectbox ( \"Select an algorithm\" , list ( all_classes )) class_type = all_classes [ class_name ] st . write ( f \"### { class_type . __module__ } . { class_name } \" ) run_signature = inspect . signature ( class_type . run ) st . write ( f \"**Input type**: { run_signature . parameters [ 'input' ] . annotation } \" ) st . write ( f \"**Output type**: { run_signature . return_annotation } \" ) st . write ( \"#### Parameters\" ) params = [] for name , param in inspect . signature ( class_type . __init__ ) . parameters . items (): if name == \"self\" : continue params . append ( f \"* ** { name } **: { param . annotation } \" ) st . write ( \" \\n \" . join ( params )) st . write ( \"## Pipeline Builder\" ) st . write ( Note AutoGOAL can automatically build pipelines given a desired input and output value. It uses the annotations of the run method of each algorithm to detect which algorithms can be connected. In the following section, you can select a desired input and output types and explore the pipelines that AutoGOAL discovers. In the left sidebar you can fine-tune the input value, e.g., make it a list of elements instead of a single element. ) st . sidebar . markdown ( \"### Configure input and output types\" ) list_input = st . sidebar . number_input ( \"Input list (level)\" , 0 , 3 , 1 ) list_output = st . sidebar . number_input ( \"Output list (level)\" , 0 , 3 , 0 ) tuples = st . sidebar . checkbox ( \"Is supervised (use Tuple in input)\" , True ) input_type = st . selectbox ( \"Select an input type\" , types_str , types_str . index ( \"Sentence\" ) ) output_type = st . selectbox ( \"Select and output type\" , types_str , types_str . index ( \"CategoricalVector\" ) ) input_type = input_type + \"()\" for i in range ( list_input ): input_type = f \"List( { input_type } )\" output_type = output_type + \"()\" for i in range ( list_output ): input_type = f \"List( { output_type } )\" if tuples : input_type = f \"Tuple( { input_type } , { output_type } )\" st . write ( f \"#### Defined input type: ` { input_type } `\" ) st . write ( f \"#### Defined output type: ` { output_type } `\" ) st . write ( Note The following code uses explicitely AutoGOAL's pipeline discovery engine to find all the pipelines that can be constructed from the desired input to the desired output. ) code = textwrap . dedent ( f \"\"\" from autogoal.kb import * from autogoal.kb import build_pipelines from autogoal.contrib import find_classes explicitly build the graph of pipelines space = build_pipelines ( input = { input_type }, output = { output_type }, registry = find_classes (), ) Note ) st.code(code) try: space = eval_code(code, \"space\") except Exception as e: if \"No pipelines can be constructed\" in str(e): st.error(str(e)) st.info( \"Try changing the input and output type or select Is supervised in the left sidebar.\" ) return raise st.write( This is the graph that represents all the posible pipelines find by AutoGOAL . Each node in this graph is an algorithm from the _Algorithm Library_ that is compatible with the input and output types of its neighbors . Any path from the top to the bottom of the graph represents a valid pipeline . Note ) graph = nx.DiGraph() def get_node_repr(node): try: return get_node_repr(node.inner) except: return dict( label=str(node).split(\".\")[-1], module=node. module .split(\"_\")[0] ) for node in space.graph.nodes: attrs = get_node_repr(node) graph.add_node(attrs[\"label\"], **attrs) for u, v in space.graph.edges: graph.add_edge(get_node_repr(u)[\"label\"], get_node_repr(v)[\"label\"]) pos = nx.nx_pydot.pydot_layout(graph, prog=\"dot\", root=space.Start) chart = ( nxa.draw_networkx(graph, pos=pos, node_color=\"module\", node_tooltip=\"label\") .properties(height=500) .interactive() ) st.altair_chart(chart, use_container_width=True) st.write( Here is an example pipeline that has been randomly sampled from the previous graph . You can try different samples . Notice how not only the nodes ( algorithms ) that participate in the pipeline are different each time , but also their internal hyperparameters change . When sampling a pipeline from the graph AutoGOAL samples all the internal hyperparameters as defined by the constructor . When these hyperparameters have complex values ( e . g . , an algorithm per - se ), AutoGOAL recursively samples instances of the internal algorithms , and so on . Note ) st.code(space.sample()) st.button(\"Sample another pipeline\") def run(self): main_section = st.sidebar.selectbox(\"Section\", list(self.main_sections)) self.main_sections main_section demo = Demo() demo.run()","title":"Autogoal.contrib.streamlit.demo"},{"location":"api/autogoal.contrib.streamlit.demo/#about-this-demo","text":"The purpose of this demo application is to showcase the main use cases of AutoGOAL. Keep in mind that AutoGOAL is a software library, i.e., meant to be used from source code. This demo serves as an interactive and user-friendly introduction to the library, but it is in no case a full-featured AutoML application. There are two sections to showcase different components of AutoGOAL. You can switch sections in the left sidebar. The High-Level API section presents the public interface of AutoGOAL in several datasets. The Pipelines section shows the internal components of AutoGOAL and allows to explore the possible pipelines. ) st . write ( Note","title":"About this demo"},{"location":"api/autogoal.contrib.streamlit.demo/#running-the-code","text":"To execute this demo on your own infrastructure, you need AutoGOAL's docker image. There are two images available, without and without GPU support. Download the corresponding Docker image: docker pull autogoal/autogoal ) st . write ( Note Launch a Docker container. docker run --rm -p 8501:8501 autogoal/autogoal Navigate to http://localhost:8501 . ) def high_level ( self ): st . write ( \"# High-Level API\" ) st . write ( Note AutoGOAL is first and foremost a framework for Automatic Machine Learning. With a few simple lines of code, you can quickly find a close to optimal solution for classic machine learning problems. ) from autogoal import datasets dataset_descriptions = { \"cars\" : \"\"\" [Cars](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) is a low-dimensionality supervised problem with 21 one-hot encoded features. Note \"\"\", \"german_credit\": \"\"\" German Credit is a low-dimensionality supervised problem with 20 categorical or numerical features. \"abalone\" : \"\"\" [Abalone](https://archive.ics.uci.edu/ml/datasets/Abalone) is a low-dimensionality supervised problem with 8 categorical or numerical features. Note \"\"\", \"shuttle\": \"\"\" Shuttle is a low-dimensionality supervised problem with 9 numerical features. \"yeast\" : \"\"\" [Yeast](https://archive.ics.uci.edu/ml/datasets/Yeast) is a low-dimensionality supervised problem with 9 numerical features. Note \"\"\", \"dorothea\": \"\"\" Dorothea is a high-dimensionality sparse supervised problem with 100,000 numerical features. \"gisette\" : \"\"\" [Gisette](https://archive.ics.uci.edu/ml/datasets/Gisette) is a high-dimensionality sparse supervised problem with 5,000 numerical features. Note \"\"\", \"haha\": \"\"\" HAHA 2019 is a text classification problem with binary classes in Spanish. \"meddocan\" : \"\"\" [MEDDOCAN 2019](https://github.com/PlanTL-SANIDAD/SPACCC_MEDDOCAN) is an entity recognition problem in Spanish medical documents. Note \"\"\", } override_types = { \"german_credit\": (\"MatrixContinuousDense()\", \"CategoricalVector()\"), \"dorothea\": (\"MatrixContinuousSparse()\", \"CategoricalVector()\"), \"gisette\": (\"MatrixContinuousSparse()\", \"CategoricalVector()\"), \"haha\": (\"List(Sentence())\", \"CategoricalVector()\"), \"meddocan\": (\"List(List(Word()))\", \"List(List(Postag()))\"), } st.write( These are sample datasets which are automatically downloaded by AutoGOAL , and can be used to benchmark new algorithms and showcase AutoML tools . Note ) dataset = st.selectbox(\"Select a dataset\", list(dataset_descriptions)) st.write( dataset_descriptions[dataset] + \"Here is the code to load this dataset.\" ) code = textwrap.dedent( f\"\"\" from autogoal.datasets import {dataset} X, y, *_ = {dataset}.load() ) st . code ( code ) X , y = eval_code ( code , \"X\" , \"y\" ) if st . checkbox ( \"Preview data\" ): try : l = len ( X ) except : l = X . shape [ 0 ] head = st . slider ( \"Preview N first items\" , 0 , l , 5 ) if isinstance ( X , list ): st . write ( X [: head ]) else : st . write ( X [: head , :]) st . write ( y [: head ]) st . write ( Note The next step is to instantiate an AutoML solver and run it on this problem. The AutoML class provides a black-box interface to AutoGOAL. You can tweak the most important parameters at the left sidebar, even though sensible defaults are provided for all the parameters. ) st . sidebar . markdown ( \"### AutoML parameters\" ) iterations = st . sidebar . number_input ( \"Number of iterations\" , 1 , 10000 , 100 ) global_timeout = st . sidebar . number_input ( \"Global timeout (seconds)\" , 1 , 1000 , 60 ) pipeline_timeout = st . sidebar . number_input ( \"Timeout per pipeline (seconds)\" , 1 , 1000 , 5 ) from autogoal.contrib.streamlit import StreamlitLogger if dataset in override_types : input_type , output_type = override_types [ dataset ] types_code = f \"\"\" input= { input_type } , output= { output_type } , Note st.info( f\"\"\" In most cases AutoGOAL can automatically infer the input and output type from the dataset. Sometimes, such as with {dataset} , the user will need to provide them explicitely. ) else : types_code = \"\" code = textwrap . dedent ( f \"\"\" from autogoal.kb import * from autogoal.ml import AutoML automl = AutoML( errors=\"ignore\", # ignore exceptions (e.g., timeouts) search_iterations= { iterations } , # total iterations search_kwargs=dict( search_timeout= { global_timeout } , # max time in total (approximate) evaluation_timeout= { pipeline_timeout } , # max time per pipeline (approximate) ), { types_code } ) Note ) st.code(code) automl = eval_code(code, \"automl\") st.write( Click run to call the ` fit ` method . Keep in mind that many of these pipelines can be quite computationally heavy and both the hyperparameter configuration as well as the infrastructure where this demo is running might not allow for the best pipelines to execute . Note ) st.code(\"automl.fit(X, y)\", language=\"Python\") if st.button(\"Run it!\"): automl.fit(X, y, logger=StreamlitLogger()) st.write( Take a look at the remaining examples in the sidebar . Note ) def build_pipelines(self): st.write(\"# Pipelines\") st.write( \"This example illustrates how AutoGOAL automatically builds \" \"a graph of pipelines for different problems settings.\" ) from autogoal.kb._data import DATA_TYPES types_str = [cls. name for cls in DATA_TYPES] st.write( AutoGOAL pipeline discovery is based on a hierarchy of semantic datatypes . Each type represents a semantic datum that can be used in a machine learning algorithm , from matrices and vectors to sentences , entities and and images . The following picture shows all available semantic data types . You can click the top right corner to enlarge . Note ) st.image(\"/code/docs/guide/datatypes.png\", use_column_width=True) from autogoal.contrib import find_classes all_classes = {k. name : k for k in find_classes()} st.write( f\"\"\"","title":"Running the code"},{"location":"api/autogoal.contrib.streamlit.demo/#algorithm-library","text":"AutoGOAL automatically builds pipelines by selecting from a wide range of algorithms implemented in contrib modules. The list of all available algorithms is shown here. There are a total of {len(all_classes)} algorithms implemented. Select one to display some information. ) class_name = st . selectbox ( \"Select an algorithm\" , list ( all_classes )) class_type = all_classes [ class_name ] st . write ( f \"### { class_type . __module__ } . { class_name } \" ) run_signature = inspect . signature ( class_type . run ) st . write ( f \"**Input type**: { run_signature . parameters [ 'input' ] . annotation } \" ) st . write ( f \"**Output type**: { run_signature . return_annotation } \" ) st . write ( \"#### Parameters\" ) params = [] for name , param in inspect . signature ( class_type . __init__ ) . parameters . items (): if name == \"self\" : continue params . append ( f \"* ** { name } **: { param . annotation } \" ) st . write ( \" \\n \" . join ( params )) st . write ( \"## Pipeline Builder\" ) st . write ( Note AutoGOAL can automatically build pipelines given a desired input and output value. It uses the annotations of the run method of each algorithm to detect which algorithms can be connected. In the following section, you can select a desired input and output types and explore the pipelines that AutoGOAL discovers. In the left sidebar you can fine-tune the input value, e.g., make it a list of elements instead of a single element. ) st . sidebar . markdown ( \"### Configure input and output types\" ) list_input = st . sidebar . number_input ( \"Input list (level)\" , 0 , 3 , 1 ) list_output = st . sidebar . number_input ( \"Output list (level)\" , 0 , 3 , 0 ) tuples = st . sidebar . checkbox ( \"Is supervised (use Tuple in input)\" , True ) input_type = st . selectbox ( \"Select an input type\" , types_str , types_str . index ( \"Sentence\" ) ) output_type = st . selectbox ( \"Select and output type\" , types_str , types_str . index ( \"CategoricalVector\" ) ) input_type = input_type + \"()\" for i in range ( list_input ): input_type = f \"List( { input_type } )\" output_type = output_type + \"()\" for i in range ( list_output ): input_type = f \"List( { output_type } )\" if tuples : input_type = f \"Tuple( { input_type } , { output_type } )\" st . write ( f \"#### Defined input type: ` { input_type } `\" ) st . write ( f \"#### Defined output type: ` { output_type } `\" ) st . write ( Note The following code uses explicitely AutoGOAL's pipeline discovery engine to find all the pipelines that can be constructed from the desired input to the desired output. ) code = textwrap . dedent ( f \"\"\" from autogoal.kb import * from autogoal.kb import build_pipelines from autogoal.contrib import find_classes explicitly build the graph of pipelines space = build_pipelines ( input = { input_type }, output = { output_type }, registry = find_classes (), ) Note ) st.code(code) try: space = eval_code(code, \"space\") except Exception as e: if \"No pipelines can be constructed\" in str(e): st.error(str(e)) st.info( \"Try changing the input and output type or select Is supervised in the left sidebar.\" ) return raise st.write( This is the graph that represents all the posible pipelines find by AutoGOAL . Each node in this graph is an algorithm from the _Algorithm Library_ that is compatible with the input and output types of its neighbors . Any path from the top to the bottom of the graph represents a valid pipeline . Note ) graph = nx.DiGraph() def get_node_repr(node): try: return get_node_repr(node.inner) except: return dict( label=str(node).split(\".\")[-1], module=node. module .split(\"_\")[0] ) for node in space.graph.nodes: attrs = get_node_repr(node) graph.add_node(attrs[\"label\"], **attrs) for u, v in space.graph.edges: graph.add_edge(get_node_repr(u)[\"label\"], get_node_repr(v)[\"label\"]) pos = nx.nx_pydot.pydot_layout(graph, prog=\"dot\", root=space.Start) chart = ( nxa.draw_networkx(graph, pos=pos, node_color=\"module\", node_tooltip=\"label\") .properties(height=500) .interactive() ) st.altair_chart(chart, use_container_width=True) st.write( Here is an example pipeline that has been randomly sampled from the previous graph . You can try different samples . Notice how not only the nodes ( algorithms ) that participate in the pipeline are different each time , but also their internal hyperparameters change . When sampling a pipeline from the graph AutoGOAL samples all the internal hyperparameters as defined by the constructor . When these hyperparameters have complex values ( e . g . , an algorithm per - se ), AutoGOAL recursively samples instances of the internal algorithms , and so on . Note ) st.code(space.sample()) st.button(\"Sample another pipeline\") def run(self): main_section = st.sidebar.selectbox(\"Section\", list(self.main_sections)) self.main_sections main_section demo = Demo() demo.run()","title":"Algorithm Library"},{"location":"api/autogoal.contrib.telegram.__init__/","text":"import time import textwrap from autogoal.search import Logger from telegram.ext import Updater , Dispatcher , CommandHandler from telegram import ParseMode class TelegramLogger ( Logger ): def __init__ ( self , token , channel : str = None , name = \"\" ): self . name = name self . channel = int ( channel ) if channel and channel . isdigit () else channel self . last_time = time . time () self . updater = Updater ( token ) self . dispatcher = self . updater . dispatcher self . progress = 0 self . generations = 1 self . best = 0.0 self . current = \"\" self . message = self . message = self . dispatcher . bot . send_message ( chat_id = self . channel , text = f \"** { self . name } ** \\n Starting...\" , parse_mode = ParseMode . MARKDOWN , ) def begin ( self , generations , pop_size ): self . generations = generations self . _send () def update_best ( self , new_best , new_fn , * args ): self . best = new_fn self . _send () def end ( self , best , best_fn ): self . best = best_fn self . _send () def eval_solution ( self , solution , fitness ): self . progress += 1 self . _send () def _send ( self ): if not self . channel : return if time . time () - self . last_time < 10 : return self . last_time = time . time () text = textwrap . dedent ( f \"\"\" ** { self . name } ** Best: ` { float ( self . best ) : 0.3 } ` Iterations: ` { self . progress } / { self . generations } ` Note ) try: self.message.edit_text( text=text, parse_mode=ParseMode.MARKDOWN, ) except: pass","title":"Autogoal.contrib.telegram.  init  "},{"location":"api/autogoal.contrib.transformers.__init__/","text":"import os from pathlib import Path DATA_PATH = Path . home () / \".autogoal\" / \"contrib\" / \"transformers\" Setting up download location os . environ [ \"TRANSFORMERS_CACHE\" ] = str ( DATA_PATH ) try : import torch import transformers assert sklearn. version == \"0.22\" except : print ( \"(!) Code in `autogoal.contrib.transformers` requires `pytorch==0.1.4` and `transformers==2.3.0`.\" ) print ( \"(!) You can install it with `pip install autogoal[transformers]`.\" ) raise from ._bert import BertEmbedding , BertTokenizeEmbedding def download (): BertEmbedding . download () return True def status (): from autogoal.contrib import ContribStatus try : BertEmbedding . check_files () except OSError : return ContribStatus . RequiresDownload return ContribStatus . Ready","title":"Autogoal.contrib.transformers.  init  "},{"location":"api/autogoal.contrib.transformers._bert/","text":"from autogoal.kb import AlgorithmBase from transformers import BertModel , BertTokenizer from pathlib import Path import torch import numpy as np from autogoal.kb import Sentence , MatrixContinuousDense , Tensor3 , Seq , Word from autogoal.kb import Supervised from autogoal.grammar import DiscreteValue , CategoricalValue from autogoal.utils import CacheManager , nice_repr @nice_repr class BertEmbedding ( AlgorithmBase ): Note Transforms a sentence already tokenized into a list of vector embeddings using a Bert pretrained multilingual model. Examples \u00b6 ```python sentence = \"embed this wrongword\".split() bert = BertEmbedding(verbose=False) embedding = bert.run(sentence) embedding.shape (3, 768) embedding array([[ 0.38879532, -0.22509766, 0.24768747, ..., 0.7490126 , 0.00565394, -0.21448825], [ 0.14288183, -0.25218976, 0.19961306, ..., 0.96493024, 0.58167326, -0.22977187], [ 0.63840294, -0.09097129, -0.80802095, ..., 0.91956913, 0.27364522, 0.14955784]], dtype=float32) ``` Notes \u00b6 On the first use the model bert-base-multilingual-cased from huggingface/transformers will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. use_cache = False def __init__ ( self , merge_mode : CategoricalValue ( \"avg\" , \"first\" ) = \"avg\" , * , verbose = False ): # , length: Discrete(16, 512)): self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) self . verbose = verbose self . print ( \"Using device: %s \" % self . device ) self . merge_mode = merge_mode self . model = None self . tokenizer = None def print ( self , * args , ** kwargs ): if not self . verbose : return print ( * args , ** kwargs ) @classmethod def check_files ( cls ): BertModel . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) BertTokenizer . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) @classmethod def download ( cls ): BertModel . from_pretrained ( \"bert-base-multilingual-cased\" ) BertTokenizer . from_pretrained ( \"bert-base-multilingual-cased\" ) def run ( self , input : Seq [ Word ]) -> MatrixContinuousDense : if self . model is None : try : self . model = BertModel . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) . to ( self . device ) self . tokenizer = BertTokenizer . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) except OSError : raise TypeError ( \"BERT requires to run `autogoal contrib download transformers`.\" ) bert_tokens = [ self . tokenizer . tokenize ( x ) for x in input ] bert_sequence = self . tokenizer . encode_plus ( [ t for tokens in bert_tokens for t in tokens ], return_tensors = \"pt\" ) with torch . no_grad (): output = self . model ( ** bert_sequence ) . last_hidden_state output = output . squeeze ( 0 ) count = 0 matrix = [] for i , token in enumerate ( input ): contiguous = len ( bert_tokens [ i ]) vectors = output [ count : count + contiguous , :] vector = self . _merge ( vectors ) matrix . append ( vector ) count += contiguous matrix = np . vstack ( matrix ) return matrix def _merge ( self , vectors ): if not vectors . size ( 0 ): return np . zeros ( vectors . size ( 1 ), dtype = \"float32\" ) if self . merge_mode == \"avg\" : return vectors . mean ( dim = 0 ) . numpy () elif self . merge_mode == \"first\" : return vectors [ 0 , :] . numpy () else : raise ValueError ( \"Unknown merge mode\" ) @nice_repr class BertTokenizeEmbedding ( AlgorithmBase ): Note Transforms a sentence into a list of vector embeddings using a Bert pretrained English model. Notes \u00b6 On the first use the model bert-base-multilingual-cased from huggingface/transformers will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. def __init__ ( self , verbose = False ): # , length: Discrete(16, 512)): self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) self . verbose = verbose self . print ( \"Using device: %s \" % self . device ) self . model = None self . tokenizer = None def print ( self , * args , ** kwargs ): if not self . verbose : return print ( * args , ** kwargs ) def run ( self , input : Seq [ Sentence ]) -> Tensor3 : if self . model is None : try : self . model = BertModel . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) . to ( self . device ) self . tokenizer = BertTokenizer . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) except OSError : raise TypeError ( \"BERT requires to run `autogoal contrib download transformers`.\" ) self . print ( \"Tokenizing...\" , end = \"\" , flush = True ) tokens = [ self . tokenizer ( x , max_length = 32 , pad_to_max_length = True ) for x in input ] self . print ( \"done\" ) ids = torch . tensor ( tokens ) . to ( self . device ) with torch . no_grad (): self . print ( \"Embedding...\" , end = \"\" , flush = True ) output = self . model ( ids )[ 0 ] . numpy () self . print ( \"done\" ) return output","title":"Autogoal.contrib.transformers. bert"},{"location":"api/autogoal.contrib.transformers._bert/#examples","text":"```python sentence = \"embed this wrongword\".split() bert = BertEmbedding(verbose=False) embedding = bert.run(sentence) embedding.shape (3, 768) embedding array([[ 0.38879532, -0.22509766, 0.24768747, ..., 0.7490126 , 0.00565394, -0.21448825], [ 0.14288183, -0.25218976, 0.19961306, ..., 0.96493024, 0.58167326, -0.22977187], [ 0.63840294, -0.09097129, -0.80802095, ..., 0.91956913, 0.27364522, 0.14955784]], dtype=float32) ```","title":"Examples"},{"location":"api/autogoal.contrib.transformers._bert/#notes","text":"On the first use the model bert-base-multilingual-cased from huggingface/transformers will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. use_cache = False def __init__ ( self , merge_mode : CategoricalValue ( \"avg\" , \"first\" ) = \"avg\" , * , verbose = False ): # , length: Discrete(16, 512)): self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) self . verbose = verbose self . print ( \"Using device: %s \" % self . device ) self . merge_mode = merge_mode self . model = None self . tokenizer = None def print ( self , * args , ** kwargs ): if not self . verbose : return print ( * args , ** kwargs ) @classmethod def check_files ( cls ): BertModel . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) BertTokenizer . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) @classmethod def download ( cls ): BertModel . from_pretrained ( \"bert-base-multilingual-cased\" ) BertTokenizer . from_pretrained ( \"bert-base-multilingual-cased\" ) def run ( self , input : Seq [ Word ]) -> MatrixContinuousDense : if self . model is None : try : self . model = BertModel . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) . to ( self . device ) self . tokenizer = BertTokenizer . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) except OSError : raise TypeError ( \"BERT requires to run `autogoal contrib download transformers`.\" ) bert_tokens = [ self . tokenizer . tokenize ( x ) for x in input ] bert_sequence = self . tokenizer . encode_plus ( [ t for tokens in bert_tokens for t in tokens ], return_tensors = \"pt\" ) with torch . no_grad (): output = self . model ( ** bert_sequence ) . last_hidden_state output = output . squeeze ( 0 ) count = 0 matrix = [] for i , token in enumerate ( input ): contiguous = len ( bert_tokens [ i ]) vectors = output [ count : count + contiguous , :] vector = self . _merge ( vectors ) matrix . append ( vector ) count += contiguous matrix = np . vstack ( matrix ) return matrix def _merge ( self , vectors ): if not vectors . size ( 0 ): return np . zeros ( vectors . size ( 1 ), dtype = \"float32\" ) if self . merge_mode == \"avg\" : return vectors . mean ( dim = 0 ) . numpy () elif self . merge_mode == \"first\" : return vectors [ 0 , :] . numpy () else : raise ValueError ( \"Unknown merge mode\" ) @nice_repr class BertTokenizeEmbedding ( AlgorithmBase ): Note Transforms a sentence into a list of vector embeddings using a Bert pretrained English model.","title":"Notes"},{"location":"api/autogoal.contrib.transformers._bert/#notes_1","text":"On the first use the model bert-base-multilingual-cased from huggingface/transformers will be downloaded. This may take a few minutes. If you are using the development container the model should be already downloaded for you. def __init__ ( self , verbose = False ): # , length: Discrete(16, 512)): self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) self . verbose = verbose self . print ( \"Using device: %s \" % self . device ) self . model = None self . tokenizer = None def print ( self , * args , ** kwargs ): if not self . verbose : return print ( * args , ** kwargs ) def run ( self , input : Seq [ Sentence ]) -> Tensor3 : if self . model is None : try : self . model = BertModel . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) . to ( self . device ) self . tokenizer = BertTokenizer . from_pretrained ( \"bert-base-multilingual-cased\" , local_files_only = True ) except OSError : raise TypeError ( \"BERT requires to run `autogoal contrib download transformers`.\" ) self . print ( \"Tokenizing...\" , end = \"\" , flush = True ) tokens = [ self . tokenizer ( x , max_length = 32 , pad_to_max_length = True ) for x in input ] self . print ( \"done\" ) ids = torch . tensor ( tokens ) . to ( self . device ) with torch . no_grad (): self . print ( \"Embedding...\" , end = \"\" , flush = True ) output = self . model ( ids )[ 0 ] . numpy () self . print ( \"done\" ) return output","title":"Notes"},{"location":"api/autogoal.contrib.wikipedia.__init__/","text":"try : import wikipedia assert wikipedia. version == \"1.4.0\" except : print ( \"(!) Code in `autogoal.contrib.wikipedia` requires `wikipedia==1.4.0`.\" ) print ( \"(!) You can install it with `pip install autogoal[wikipedia]`.\" ) raise from ._base import ( WikipediaContainsWord , WikipediaContainsWordSpanish , WikipediaSummary , WikipediaSummarySpanish , )","title":"Autogoal.contrib.wikipedia.  init  "},{"location":"api/autogoal.contrib.wikipedia._base/","text":"import wikipedia from autogoal.kb import Word , Document , FeatureSet from autogoal.utils import nice_repr from autogoal.kb import AlgorithmBase @nice_repr class WikipediaSummary ( AlgorithmBase ): Note \"\"\"This class find a word in Wikipedia and return a summary in english. def __init__ ( self ): pass def run ( self , input : Word ) -> Document : Note \"\"\"This method use Word2Vect of gensim for tranform a word in embedding vector. try : return wikipedia . summary ( input ) except : return \"\" @nice_repr class WikipediaContainsWord ( AlgorithmBase ): Note \"\"\"This class find a word in Wikipedia and return a summary in english. def __init__ ( self ): pass def run ( self , input : Word ) -> FeatureSet : Note \"\"\"This method use Word2Vect of gensim for tranform a word in embedding vector. return dict ( in_wikipedia = bool ( wikipedia . search ( input ))) @nice_repr class WikipediaSummarySpanish ( AlgorithmBase ): Note \"\"\"This class find a word in Wikipedia and return a summary in Spanish. def __init__ ( self ): wikipedia . set_lang ( \"es\" ) def run ( self , input : Word ) -> Document : Note \"\"\"This method use Word2Vect of gensim for tranform a word in embedding vector. try : return wikipedia . summary ( input ) except : return \"\" @nice_repr class WikipediaContainsWordSpanish ( AlgorithmBase ): Note \"\"\"This class find a word in Wikipedia and return a summary in Spanish. def __init__ ( self ): wikipedia . set_lang ( \"es\" ) def run ( self , input : Word ) -> FeatureSet : Note \"\"\"This method use Word2Vect of gensim for tranform a word in embedding vector. return dict ( in_wikipedia = bool ( wikipedia . search ( input )))","title":"Autogoal.contrib.wikipedia. base"},{"location":"api/autogoal.contrib.wrappers/","text":"from autogoal.kb import * from autogoal.grammar import CategoricalValue , BooleanValue from autogoal.utils import nice_repr from autogoal.kb import AlgorithmBase import numpy as np @nice_repr class VectorAggregator ( AlgorithmBase ): def __init__ ( self , mode : CategoricalValue ( \"mean\" , \"max\" )): self . mode = mode def run ( self , input : Seq [ VectorContinuous ]) -> VectorContinuous : input = np . vstack ( input ) if self . mode == \"mean\" : return input . mean ( axis = 1 ) elif self . mode == \"max\" : return input . max ( axis = 1 ) raise ValueError ( \"Invalid mode: %s \" % self . mode ) @nice_repr class MatrixBuilder ( AlgorithmBase ): Note Builds a matrix from a list of vectors. Examples \u00b6 ```python import numpy as np x1 = np.asarray([1,2,3]) x2 = np.asarray([2,3,4]) x3 = np.asarray([3,4,5]) MatrixBuilder().run([x1, x2, x3]) array([[1, 2, 3], [2, 3, 4], [3, 4, 5]]) ``` def run ( self , input : Seq [ VectorContinuous ]) -> MatrixContinuousDense : return np . vstack ( input ) @nice_repr class TensorBuilder ( AlgorithmBase ): Note Builds a 3D tensor from a list of matrices. Examples \u00b6 ```python import numpy as np x1 = np.asarray([[1,2],[3,4]]) x2 = np.asarray([[2,3],[4,5]]) x3 = np.asarray([[3,4],[5,6]]) TensorBuilder().run([x1, x2, x3]) array([[[1, 2], [3, 4]], [[2, 3], [4, 5]], [[3, 4], [5, 6]]]) ``` def run ( self , input : Seq [ MatrixContinuousDense ]) -> Tensor3 : return np . vstack ([ np . expand_dims ( m , axis = 0 ) for m in input ]) @nice_repr class FlagsMerger ( AlgorithmBase ): def run ( self , input : Seq [ FeatureSet ]) -> FeatureSet : result = {} for d in input : result . update ( d ) return result @nice_repr class MultipleFeatureExtractor ( AlgorithmBase ): def __init__ ( self , extractors : Distinct ( algorithm ( Word , FeatureSet ), exceptions = [ \"MultipleFeatureExtractor\" ] ), merger : algorithm ( Seq [ FeatureSet ], FeatureSet ), ): self . extractors = extractors self . merger = merger def run ( self , input : Word ) -> FeatureSet : flags = [ extractor . run ( input ) for extractor in self . extractors ] return self . merger . run ( flags ) @nice_repr class SentenceFeatureExtractor ( AlgorithmBase ): def __init__ ( self , tokenizer : algorithm ( Sentence , Seq [ Word ]), feature_extractor : algorithm ( Word , FeatureSet ), include_text : BooleanValue (), ): self . tokenizer = tokenizer self . feature_extractor = feature_extractor self . include_text = include_text def run ( self , input : Sentence ) -> FeatureSet : tokens = self . tokenizer . run ( input ) flags = [ self . feature_extractor ( w ) for w in tokens ] if self . include_text : return { f \" { w } | { f } \" : v for w , flag in zip ( tokens , flags ) for f , v in flag . items () } else : return { f : v for flag in flags for f , v in flag . items ()} @nice_repr class DocumentFeatureExtractor ( AlgorithmBase ): def __init__ ( self , tokenizer : algorithm ( Document , Seq [ Sentence ]), feature_extractor : algorithm ( Sentence , FeatureSet ), ): self . tokenizer = tokenizer self . feature_extractor = feature_extractor def run ( self , input : Document ) -> Seq [ FeatureSet ]: tokens = self . tokenizer . run ( input ) flags = [ self . feature_extractor ( w ) for w in tokens ] return @nice_repr class TextEntityEncoder(AlgorithmBase): \"\"\" Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas categorias BILOUV. \"\"\" def __init__ ( self , tokenizer : algorithm ( Sentence , Seq [ Word ] )) : self . tokenizer = tokenizer def run ( self , input : Tuple ( Sentence , Seq [ Entity ] ) ) -> Tuple ( Seq [ Word ] , Seq [ Label ] : pass @nice_repr class TextRelationEncoder(AlgorithmBase): \"\"\" Convierte una oraci\u00f3n en texto plano y una lista de relaciones que se cumplen entre entidades, en una lista de ejemplos por cada oraci\u00f3n. \"\"\" def __init__ ( self , tokenizer : algorithm ( Sentence , Seq [ Word ] ), token_feature_extractor : algorithm ( Word , FeatureSet ), # token_sentence_encoder : algorithm ( Word , ) ) : pass def run ( self , input : Tuple ( Sentence , Seq [ Tupl ] ( Entity (), Entity (), Category ()))) ) -> Tuple ( Seq [ Vect ] r ()), CategoricalVector ()) : pass","title":"Autogoal.contrib.wrappers"},{"location":"api/autogoal.contrib.wrappers/#examples","text":"```python import numpy as np x1 = np.asarray([1,2,3]) x2 = np.asarray([2,3,4]) x3 = np.asarray([3,4,5]) MatrixBuilder().run([x1, x2, x3]) array([[1, 2, 3], [2, 3, 4], [3, 4, 5]]) ``` def run ( self , input : Seq [ VectorContinuous ]) -> MatrixContinuousDense : return np . vstack ( input ) @nice_repr class TensorBuilder ( AlgorithmBase ): Note Builds a 3D tensor from a list of matrices.","title":"Examples"},{"location":"api/autogoal.contrib.wrappers/#examples_1","text":"```python import numpy as np x1 = np.asarray([[1,2],[3,4]]) x2 = np.asarray([[2,3],[4,5]]) x3 = np.asarray([[3,4],[5,6]]) TensorBuilder().run([x1, x2, x3]) array([[[1, 2], [3, 4]], [[2, 3], [4, 5]], [[3, 4], [5, 6]]]) ``` def run ( self , input : Seq [ MatrixContinuousDense ]) -> Tensor3 : return np . vstack ([ np . expand_dims ( m , axis = 0 ) for m in input ]) @nice_repr class FlagsMerger ( AlgorithmBase ): def run ( self , input : Seq [ FeatureSet ]) -> FeatureSet : result = {} for d in input : result . update ( d ) return result @nice_repr class MultipleFeatureExtractor ( AlgorithmBase ): def __init__ ( self , extractors : Distinct ( algorithm ( Word , FeatureSet ), exceptions = [ \"MultipleFeatureExtractor\" ] ), merger : algorithm ( Seq [ FeatureSet ], FeatureSet ), ): self . extractors = extractors self . merger = merger def run ( self , input : Word ) -> FeatureSet : flags = [ extractor . run ( input ) for extractor in self . extractors ] return self . merger . run ( flags ) @nice_repr class SentenceFeatureExtractor ( AlgorithmBase ): def __init__ ( self , tokenizer : algorithm ( Sentence , Seq [ Word ]), feature_extractor : algorithm ( Word , FeatureSet ), include_text : BooleanValue (), ): self . tokenizer = tokenizer self . feature_extractor = feature_extractor self . include_text = include_text def run ( self , input : Sentence ) -> FeatureSet : tokens = self . tokenizer . run ( input ) flags = [ self . feature_extractor ( w ) for w in tokens ] if self . include_text : return { f \" { w } | { f } \" : v for w , flag in zip ( tokens , flags ) for f , v in flag . items () } else : return { f : v for flag in flags for f , v in flag . items ()} @nice_repr class DocumentFeatureExtractor ( AlgorithmBase ): def __init__ ( self , tokenizer : algorithm ( Document , Seq [ Sentence ]), feature_extractor : algorithm ( Sentence , FeatureSet ), ): self . tokenizer = tokenizer self . feature_extractor = feature_extractor def run ( self , input : Document ) -> Seq [ FeatureSet ]: tokens = self . tokenizer . run ( input ) flags = [ self . feature_extractor ( w ) for w in tokens ] return @nice_repr class TextEntityEncoder(AlgorithmBase): \"\"\" Convierte una oraci\u00f3n en texto plano m\u00e1s la lista de entidades reconocidas en la oraci\u00f3n, en una lista de tokens con sus respectivas categorias BILOUV. \"\"\" def __init__ ( self , tokenizer : algorithm ( Sentence , Seq [ Word ] )) : self . tokenizer = tokenizer def run ( self , input : Tuple ( Sentence , Seq [ Entity ] ) ) -> Tuple ( Seq [ Word ] , Seq [ Label ] : pass @nice_repr class TextRelationEncoder(AlgorithmBase): \"\"\" Convierte una oraci\u00f3n en texto plano y una lista de relaciones que se cumplen entre entidades, en una lista de ejemplos por cada oraci\u00f3n. \"\"\" def __init__ ( self , tokenizer : algorithm ( Sentence , Seq [ Word ] ), token_feature_extractor : algorithm ( Word , FeatureSet ), # token_sentence_encoder : algorithm ( Word , ) ) : pass def run ( self , input : Tuple ( Sentence , Seq [ Tupl ] ( Entity (), Entity (), Category ()))) ) -> Tuple ( Seq [ Vect ] r ()), CategoricalVector ()) : pass","title":"Examples"},{"location":"api/autogoal.datasets.__init__/","text":"import shutil from pathlib import Path from typing import Dict import json import requests import os from tqdm import tqdm from functools import lru_cache DATASETS_METADATA = ( \"https://raw.githubusercontent.com/autogoal/datasets/master/datasets.json\" ) DATA_PATH = Path . home () / \".autogoal\" / \"data\" ensure data path directory creation os . makedirs ( DATA_PATH , exist_ok = True ) @lru_cache () def get_datasets_list () -> Dict [ str , str ]: try : data = requests . get ( DATASETS_METADATA ) . json () with open ( DATA_PATH / \"datasets.json\" , \"w\" ) as fp : json . dump ( data , fp , indent = 2 ) return data except requests . ConnectionError as e : try : with open ( DATA_PATH / \"datasets.json\" , \"r\" ) as fp : return json . load ( fp ) except IOError : raise Exception ( \"Cannot download dataset list and no cached version exists.\" ) def datapath ( path : str ) -> Path : Note Returns a Path object that points to the dataset path where path is located. Examples \u00b6 ```python datapath(\"movie_reviews\") PosixPath('/home/coder/.autogoal/data/movie_reviews') ``` return Path ( DATA_PATH ) / path def pack ( folder : str ): filename = datapath ( folder ) rootdir = datapath ( folder ) shutil . make_archive ( filename , \"zip\" , root_dir = rootdir ) def unpack ( zipfile : str ): filename = datapath ( zipfile ) rootdir = datapath ( zipfile [: - 4 ]) shutil . unpack_archive ( filename , extract_dir = rootdir , format = \"zip\" ) def download ( dataset : str , unpackit : bool = True ): fname = f \" { dataset } .zip\" path = datapath ( fname ) if path . exists (): return datasets = get_datasets_list () url = datasets [ dataset ] download_and_save ( url , path , True ) if unpackit : unpack ( fname ) def download_and_save ( url , path : Path , overwrite = False , data_length = None ): stream = requests . get ( url , stream = True ) total_size = data_length or int ( stream . headers . get ( \"content-length\" , 0 )) if path . exists () and not overwrite : return False try : with path . open ( \"wb\" ) as f : with tqdm ( total = total_size , unit = \"B\" , unit_scale = True , unit_divisor = 1024 ) as pbar : for data in stream . iter_content ( 32 * 1024 ): f . write ( data ) pbar . update ( len ( data )) return True except : path . unlink () raise","title":"Autogoal.datasets.  init  "},{"location":"api/autogoal.datasets.__init__/#examples","text":"```python datapath(\"movie_reviews\") PosixPath('/home/coder/.autogoal/data/movie_reviews') ``` return Path ( DATA_PATH ) / path def pack ( folder : str ): filename = datapath ( folder ) rootdir = datapath ( folder ) shutil . make_archive ( filename , \"zip\" , root_dir = rootdir ) def unpack ( zipfile : str ): filename = datapath ( zipfile ) rootdir = datapath ( zipfile [: - 4 ]) shutil . unpack_archive ( filename , extract_dir = rootdir , format = \"zip\" ) def download ( dataset : str , unpackit : bool = True ): fname = f \" { dataset } .zip\" path = datapath ( fname ) if path . exists (): return datasets = get_datasets_list () url = datasets [ dataset ] download_and_save ( url , path , True ) if unpackit : unpack ( fname ) def download_and_save ( url , path : Path , overwrite = False , data_length = None ): stream = requests . get ( url , stream = True ) total_size = data_length or int ( stream . headers . get ( \"content-length\" , 0 )) if path . exists () and not overwrite : return False try : with path . open ( \"wb\" ) as f : with tqdm ( total = total_size , unit = \"B\" , unit_scale = True , unit_divisor = 1024 ) as pbar : for data in stream . iter_content ( 32 * 1024 ): f . write ( data ) pbar . update ( len ( data )) return True except : path . unlink () raise","title":"Examples"},{"location":"api/autogoal.datasets.abalone/","text":"import numpy as np import os from sklearn.feature_extraction import DictVectorizer from autogoal.datasets import datapath , download def load ( representation = \"numeric\" ): Note Loads corpora from ABALONE uci dataset . Examples \u00b6 ```python X, y = load() X.shape (4177, 6047) len(y) 4177 ``` try : download ( \"abalone\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise f = open ( datapath ( \"abalone\" ) / \"abalone.data\" , \"r\" ) X = [] y = [] for i in f . readlines (): clean_line = i . strip () . split ( \",\" ) temp = {} temp [ \"Sex\" ] = clean_line [ 0 ] temp [ \"Length\" ] = clean_line [ 1 ] temp [ \"Diameter\" ] = clean_line [ 2 ] temp [ \"Height\" ] = clean_line [ 3 ] temp [ \"Shucked weight\" ] = clean_line [ 4 ] temp [ \"Whole weight\" ] = clean_line [ 5 ] temp [ \"Viscera weight\" ] = clean_line [ 6 ] temp [ \"Shell weight\" ] = clean_line [ 7 ] X . append ( temp ) y . append ( clean_line [ 8 ]) if representation == \"numeric\" : return _load_numeric ( X , y ) elif representation == \"onehot\" : return _load_onehot ( X , y ) raise ValueError ( \"Invalid value for represenation: %s \" % representation ) def _load_numeric ( X , y ): new_X = [] for d in X : new_d = d . copy () v = d [ \"Sex\" ] if v == \"M\" : new_d [ \"Sex\" ] = 3 elif v == \"F\" : new_d [ \"Sex\" ] = 2 elif v == \"I\" : new_d [ \"Sex\" ] = 1 new_X . append ( new_d ) return _load_onehot ( new_X , y ) def _load_onehot ( X , y ): vec = DictVectorizer ( sparse = False ) return vec . fit_transform ( X ), np . asarray ( y )","title":"Autogoal.datasets.abalone"},{"location":"api/autogoal.datasets.abalone/#examples","text":"```python X, y = load() X.shape (4177, 6047) len(y) 4177 ``` try : download ( \"abalone\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise f = open ( datapath ( \"abalone\" ) / \"abalone.data\" , \"r\" ) X = [] y = [] for i in f . readlines (): clean_line = i . strip () . split ( \",\" ) temp = {} temp [ \"Sex\" ] = clean_line [ 0 ] temp [ \"Length\" ] = clean_line [ 1 ] temp [ \"Diameter\" ] = clean_line [ 2 ] temp [ \"Height\" ] = clean_line [ 3 ] temp [ \"Shucked weight\" ] = clean_line [ 4 ] temp [ \"Whole weight\" ] = clean_line [ 5 ] temp [ \"Viscera weight\" ] = clean_line [ 6 ] temp [ \"Shell weight\" ] = clean_line [ 7 ] X . append ( temp ) y . append ( clean_line [ 8 ]) if representation == \"numeric\" : return _load_numeric ( X , y ) elif representation == \"onehot\" : return _load_onehot ( X , y ) raise ValueError ( \"Invalid value for represenation: %s \" % representation ) def _load_numeric ( X , y ): new_X = [] for d in X : new_d = d . copy () v = d [ \"Sex\" ] if v == \"M\" : new_d [ \"Sex\" ] = 3 elif v == \"F\" : new_d [ \"Sex\" ] = 2 elif v == \"I\" : new_d [ \"Sex\" ] = 1 new_X . append ( new_d ) return _load_onehot ( new_X , y ) def _load_onehot ( X , y ): vec = DictVectorizer ( sparse = False ) return vec . fit_transform ( X ), np . asarray ( y )","title":"Examples"},{"location":"api/autogoal.datasets.cars/","text":"import numpy as np import os from autogoal.datasets import download , datapath from sklearn.feature_extraction import DictVectorizer def load ( representation = \"onehot\" ): download ( \"uci_cars\" ) f = open ( datapath ( \"uci_cars\" ) / \"car.data\" , \"r\" ) X = [] y = [] for i in f . readlines (): clean_line = i . strip () . split ( \",\" ) temp = {} temp [ \"buying\" ] = clean_line [ 0 ] temp [ \"maint\" ] = clean_line [ 1 ] temp [ \"doors\" ] = clean_line [ 2 ] temp [ \"persons\" ] = clean_line [ 3 ] temp [ \"lug_boot\" ] = clean_line [ 4 ] temp [ \"safety\" ] = clean_line [ 5 ] X . append ( temp ) y . append ( clean_line [ 6 ]) if representation == \"numeric\" : return _load_numeric ( X , y ) elif representation == \"onehot\" : return _load_onehot ( X , y ) raise ValueError ( \"Invalid value for represenation: %s \" % representation ) def _load_numeric ( X , y ): new_X = [] for d in X : new_d = {} for k , v in d . items (): if k == \"buying\" : if v == \"vhigh\" : new_d [ \"buying\" ] = 4 elif v == \"high\" : new_d [ \"buying\" ] = 3 elif v == \"med\" : new_d [ \"buying\" ] = 2 elif v == \"low\" : new_d [ \"buying\" ] = 1 if k == \"maint\" : if v == \"vhigh\" : new_d [ \"maint\" ] = 4 elif v == \"high\" : new_d [ \"maint\" ] = 3 elif v == \"med\" : new_d [ \"maint\" ] = 2 elif v == \"low\" : new_d [ \"maint\" ] = 1 if k == \"doors\" : if v == \"5more\" : new_d [ \"doors\" ] = 4 elif v == \"4\" : new_d [ \"doors\" ] = 3 elif v == \"3\" : new_d [ \"doors\" ] = 2 elif v == \"2\" : new_d [ \"doors\" ] = 1 if k == \"persons\" : if v == \"more\" : new_d [ \"persons\" ] = 3 elif v == \"4\" : new_d [ \"persons\" ] = 2 elif v == \"2\" : new_d [ \"persons\" ] = 1 if k == \"lug_boot\" : if v == \"big\" : new_d [ \"lug_boot\" ] = 3 elif v == \"med\" : new_d [ \"lug_boot\" ] = 2 elif v == \"small\" : new_d [ \"lug_boot\" ] = 1 if k == \"safety\" : if v == \"high\" : new_d [ \"safety\" ] = 3 elif v == \"med\" : new_d [ \"safety\" ] = 2 elif v == \"low\" : new_d [ \"safety\" ] = 1 new_X . append ( new_d ) return _load_onehot ( X , y ) def _load_onehot ( X , y ): vec = DictVectorizer ( sparse = False ) return vec . fit_transform ( X ), np . asarray ( y )","title":"Autogoal.datasets.cars"},{"location":"api/autogoal.datasets.cifar10/","text":"from autogoal.datasets import download , datapath import pickle import numpy as np def load ( training_batches = 5 ): Note Load the CIFAR-10 dataset Parameters \u00b6 'training_batches': maximum number of batches to load for training, each batch has 10,000 examples (min= 1 , max= 5 , default= 5 ). Examples \u00b6 X_train, y_train, X_test, y_test = load(training_batches=5) X_train.shape (50000, 32, 32, 3) len(y_train) 50000 X_test.shape (10000, 32, 32, 3) len(y_test) 10000 y_train[0] 6 download ( \"cifar10\" ) X_train = [] y_train = [] for i in range ( 1 , training_batches + 1 ): batch = datapath ( \"cifar10\" ) / f \"data_batch_ { i } \" with open ( batch , \"rb\" ) as fp : data = pickle . load ( fp , encoding = \"bytes\" ) X_train . append ( data [ b \"data\" ]) y_train . extend ( data [ b \"labels\" ]) X_train = np . vstack ( X_train ) X_train = np . reshape ( X_train , ( - 1 , 3 , 32 , 32 )) . transpose ( 0 , 2 , 3 , 1 ) test_batch = datapath ( \"cifar10\" ) / \"test_batch\" with open ( test_batch , \"rb\" ) as fp : data = pickle . load ( fp , encoding = \"bytes\" ) X_test , y_test = data [ b \"data\" ], data [ b \"labels\" ] X_test = np . reshape ( X_test , ( - 1 , 3 , 32 , 32 )) . transpose ( 0 , 2 , 3 , 1 ) return X_train , y_train , X_test , y_test","title":"Autogoal.datasets.cifar10"},{"location":"api/autogoal.datasets.cifar10/#parameters","text":"'training_batches': maximum number of batches to load for training, each batch has 10,000 examples (min= 1 , max= 5 , default= 5 ).","title":"Parameters"},{"location":"api/autogoal.datasets.cifar10/#examples","text":"X_train, y_train, X_test, y_test = load(training_batches=5) X_train.shape (50000, 32, 32, 3) len(y_train) 50000 X_test.shape (10000, 32, 32, 3) len(y_test) 10000 y_train[0] 6 download ( \"cifar10\" ) X_train = [] y_train = [] for i in range ( 1 , training_batches + 1 ): batch = datapath ( \"cifar10\" ) / f \"data_batch_ { i } \" with open ( batch , \"rb\" ) as fp : data = pickle . load ( fp , encoding = \"bytes\" ) X_train . append ( data [ b \"data\" ]) y_train . extend ( data [ b \"labels\" ]) X_train = np . vstack ( X_train ) X_train = np . reshape ( X_train , ( - 1 , 3 , 32 , 32 )) . transpose ( 0 , 2 , 3 , 1 ) test_batch = datapath ( \"cifar10\" ) / \"test_batch\" with open ( test_batch , \"rb\" ) as fp : data = pickle . load ( fp , encoding = \"bytes\" ) X_test , y_test = data [ b \"data\" ], data [ b \"labels\" ] X_test = np . reshape ( X_test , ( - 1 , 3 , 32 , 32 )) . transpose ( 0 , 2 , 3 , 1 ) return X_train , y_train , X_test , y_test","title":"Examples"},{"location":"api/autogoal.datasets.dorothea/","text":"import os import numpy as np from scipy import sparse as sp from autogoal.datasets import datapath , download def load (): Note Loads train and valid datasets from DOROTHEA uci dataset . Examples \u00b6 ```python X_train, y_train, X_valid, y_valid = load() X_train.shape, X_valid.shape ((800, 100000), (350, 100000)) len(y_train), len(y_valid) (800, 350) ``` download ( \"dorothea\" ) train_data = open ( datapath ( \"dorothea\" ) / \"dorothea_train.data\" , \"r\" ) train_labels = open ( datapath ( \"dorothea\" ) / \"dorothea_train.labels\" , \"r\" ) valid_data = open ( datapath ( \"dorothea\" ) / \"dorothea_valid.data\" , \"r\" ) valid_labels = open ( datapath ( \"dorothea\" ) / \"dorothea_valid.labels\" , \"r\" ) Xtrain = sp . lil_matrix (( 800 , 100000 ), dtype = int ) ytrain = [] Xvalid = sp . lil_matrix (( 350 , 100000 ), dtype = int ) yvalid = [] for row , line in enumerate ( train_data ): for col in line . split (): Xtrain [ row , int ( col ) - 1 ] = 1 for row , line in enumerate ( valid_data ): for col in line . split (): Xvalid [ row , int ( col ) - 1 ] = 1 for line in train_labels : ytrain . append ( int ( line )) for line in valid_labels : yvalid . append ( int ( line )) return Xtrain . tocsr (), np . asarray ( ytrain ), Xvalid . tocsr (), np . asarray ( yvalid )","title":"Autogoal.datasets.dorothea"},{"location":"api/autogoal.datasets.dorothea/#examples","text":"```python X_train, y_train, X_valid, y_valid = load() X_train.shape, X_valid.shape ((800, 100000), (350, 100000)) len(y_train), len(y_valid) (800, 350) ``` download ( \"dorothea\" ) train_data = open ( datapath ( \"dorothea\" ) / \"dorothea_train.data\" , \"r\" ) train_labels = open ( datapath ( \"dorothea\" ) / \"dorothea_train.labels\" , \"r\" ) valid_data = open ( datapath ( \"dorothea\" ) / \"dorothea_valid.data\" , \"r\" ) valid_labels = open ( datapath ( \"dorothea\" ) / \"dorothea_valid.labels\" , \"r\" ) Xtrain = sp . lil_matrix (( 800 , 100000 ), dtype = int ) ytrain = [] Xvalid = sp . lil_matrix (( 350 , 100000 ), dtype = int ) yvalid = [] for row , line in enumerate ( train_data ): for col in line . split (): Xtrain [ row , int ( col ) - 1 ] = 1 for row , line in enumerate ( valid_data ): for col in line . split (): Xvalid [ row , int ( col ) - 1 ] = 1 for line in train_labels : ytrain . append ( int ( line )) for line in valid_labels : yvalid . append ( int ( line )) return Xtrain . tocsr (), np . asarray ( ytrain ), Xvalid . tocsr (), np . asarray ( yvalid )","title":"Examples"},{"location":"api/autogoal.datasets.dummy/","text":"Note This module generates a random dataset useful for quickly testing the interface of AutoGOAL methods. import numpy as np def generate ( samples = 100 , classes = 2 , features = 10 , exponent = 1 , error = 0.1 , seed = None ): Note Create a random X,y pair. Examples \u00b6 ```python X, y = generate(samples=4, features=2, seed=0) print(X) [[0.5488135 0.71518937] [0.60276338 0.54488318] [0.4236548 0.64589411] [0.43758721 0.891773 ]] y array([0, 0, 0, 1]) ``` if seed is not None : np . random . seed ( seed ) X = np . random . random (( samples , features )) y = np . random . randint ( 0 , classes , samples ) . astype ( str ) return X , y","title":"Autogoal.datasets.dummy"},{"location":"api/autogoal.datasets.dummy/#examples","text":"```python X, y = generate(samples=4, features=2, seed=0) print(X) [[0.5488135 0.71518937] [0.60276338 0.54488318] [0.4236548 0.64589411] [0.43758721 0.891773 ]] y array([0, 0, 0, 1]) ``` if seed is not None : np . random . seed ( seed ) X = np . random . random (( samples , features )) y = np . random . randint ( 0 , classes , samples ) . astype ( str ) return X , y","title":"Examples"},{"location":"api/autogoal.datasets.ehealthkd20.__init__/","text":"from autogoal.datasets import download , datapath from autogoal.datasets.ehealthkd20._utils import Collection def load_training_entities (): download(\"ehealthkd20\") training_path = datapath ( \"ehealthkd20\" ) / \"training\" collection = Collection () . load_dir ( training_path )","title":"Autogoal.datasets.ehealthkd20.  init  "},{"location":"api/autogoal.datasets.ehealthkd20._encoding/","text":"from autogoal.datasets.ehealthkd20._utils import Collection , Sentence def to_biluov ( tokensxsentence , entitiesxsentence ): labelsxsentence = [] for tokens , entities in zip ( tokensxsentence , entitiesxsentence ): offset = 0 labels = [] for token in tokens : Recently found that (token.idx, token.idx + len(token)) is the span matches = find_match ( offset , offset + len ( token . text ), entities ) tag = select_tag ( matches ) labels . append ( tag ) offset += len ( token . text_with_ws ) labelsxsentence . append ( labels ) return labelsxsentence # , \"BILUOV\" def find_match ( start , end , entities ): def match ( other ): return other [ 0 ] <= start and end <= other [ 1 ] matches = [] for spans in entities : UNIT if len ( spans ) == 1 : if match ( spans [ 0 ]): matches . append (( spans [ 0 ], \"U\" )) continue BEGIN begin , * tail = spans if match ( begin ): matches . append (( begin , \"B\" )) continue LAST * body , last = tail if match ( last ): matches . append (( last , \"L\" )) continue INNER for inner in body : if match ( inner ): matches . append (( inner , \"I\" )) break return matches def select_tag ( matches ): if not matches : return \"O\" if len ( matches ) == 1 : return matches [ 0 ][ 1 ] tags = [ tag for _ , tag in matches ] return \"U\" if ( \"U\" in tags and not \"B\" in tags and not \"L\" in tags ) else \"V\" def make_sentence ( doc , bilouv , labels ) -> Sentence : sentence = Sentence ( doc . text ) logger . debug ( f \"[make_sentence]: doc.text= { doc . text } \" ) logger . debug ( f \"[make_sentence]: bilouv= { bilouv } \" ) labels = set ( l [ 2 :] for l in labels if l != \"O\" ) for label in labels : specific_bilouv = [] for tag in bilouv : if tag . endswith ( label ): tag = tag [ 0 ] specific_bilouv . append ( tag [ 0 ]) else : specific_bilouv . append ( \"O\" ) logger . debug ( f \"[make_sentence]: label= { label } specific_bilouv= { specific_bilouv } \" ) spans = from_biluov ( specific_bilouv , doc , spans = True ) sentence . keyphrases . extend ( Keyphrase ( sentence , label , i , sp ) for i , sp in enumerate ( spans ) ) return sentence def from_biluov ( biluov , sentence , * , spans = False , drop_remaining = []): Note from_biluov(list('BBULL'), 'A B C D E'.split()) [['C'], ['B', 'D'], ['A', 'E']] entities = [ x for x in discontinuous_match ( biluov , sentence )] for i , ( tag , word ) in enumerate ( zip ( biluov , sentence )): if tag == \"U\" : entities . append ([ word ]) biluov [ i ] = \"O\" elif tag == \"V\" : biluov [ i ] = \"I\" only BILO is left!!! changed = True while changed : changed = False one_shot = enumerate ( zip ( biluov , sentence )) try : i , ( tag , word ) = next ( one_shot ) while True : if tag != \"B\" : i , ( tag , word ) = next ( one_shot ) continue on_build = [( word , i )] i , ( tag , word ) = next ( one_shot ) while tag in ( \"O\" , \"I\" ): if tag == \"I\" : on_build . append ( word ) i , ( tag , word ) = next ( one_shot ) if tag == \"L\" : entities . append ([ x for x , _ in on_build ] + [ word ]) for _ , j in on_build : biluov [ j ] = \"O\" biluov [ i ] = \"O\" on_build . clear () changed = True except StopIteration : pass for i , ( tag , word ) in enumerate ( zip ( biluov , sentence )): if tag != \"O\" and tag not in drop_remaining : entities . append ([ word ]) return ( entities if not spans else [[( t . idx , t . idx + len ( t )) for t in tokens ] for tokens in entities ] ) def discontinuous_match ( biluov , sentence ): Note discontinuous_match(['B','V','L'],['la', 'enfermedad', 'renal']) [['la', 'enfermedad', 'renal'], ['enfermedad']] discontinuous_match(['O','V','I','L','O','I','L'],['el','cancer','de','pulmon','y','de','mama']) [['cancer', 'de', 'pulmon'], ['cancer', 'de', 'mama']] discontinuous_match(['B','O','B','V'],['tejidos','y','organos','humanos']) [['organos', 'humanos'], ['tejidos', 'humanos']] discontinuous_match(['O','V','I','L','O','I','L','O','B','O','B','V'], ['el','cancer','de','pulmon','y','de','mama','y','tejidos','y','organos','humanos']) [['cancer', 'de', 'pulmon'], ['cancer', 'de', 'mama'], ['organos', 'humanos'], ['tejidos', 'humanos']] discontinuous_match(list('BBULL'), 'A B C D E'.split()) [] entities = [] for i , tag in enumerate ( biluov ): if tag != \"V\" : continue for entity_ids in _full_overlap ( biluov , list ( range ( len ( sentence ))), i ): entity = [] for idx in entity_ids : entity . append ( sentence [ idx ]) biluov [ idx ] = \"O\" entities . append ( entity ) return entities def _full_overlap ( biluov , sentence , index , product = False ): Note INDEX TAG MUST BE 'V' _full_overlap(['B','V','L'], list(range(3)), 1) [[0, 1, 2], [1]] _full_overlap(['B','V','V','L'], list(range(4)), 1) [[0, 1, 2, 3], [1, 2]] _full_overlap(['B','V','V','L'], list(range(4)), 2) [[0, 1, 2, 3], [1, 2]] _full_overlap(['B','V','V','V','L'], list(range(5)), 1) [[0, 1, 2, 3, 4], [1, 2, 3]] _full_overlap(['B','V','V','V','L'], list(range(5)), 2) [[0, 1, 2, 3, 4], [1, 2, 3]] _full_overlap(['B','V','V','V','L'], list(range(5)), 3) [[0, 1, 2, 3, 4], [1, 2, 3]] _full_overlap(['B','B','V','L','L'], list(range(5)), 2) [[1, 2, 3], [0, 2, 4]] _full_overlap(['B','I','B','O','V','I','L','O','L'], list(range(9)), 4) [[2, 4, 5, 6], [0, 1, 4, 8]] _full_overlap(['B','I','B','O','V','I','L','O','L'], list(range(9)), 4, True) [[2, 4, 5, 6], [2, 4, 8], [0, 1, 4, 5, 6], [0, 1, 4, 8]] _full_overlap(['0','0','V','L'], list(range(4)), 2) [[2, 3], [2]] _full_overlap(['V','L'], list(range(2)), 0) [[0, 1], [0]] _full_overlap(['B','V','O','O'], list(range(4)), 1) [[0, 1], [1]] _full_overlap(['B','V'], list(range(2)), 1) [[0, 1], [1]] _full_overlap(['0','0','V','O','O'], list(range(5)), 2) [] left = _right_to_left_overlap ( biluov [: index + 1 ], sentence [: index + 1 ]) right = _left_to_right_overlap ( biluov [ index :], sentence [ index :]) full = [] if product : for l in left : for r in right : new = l + r [ 1 :] if len ( l ) > len ( r ) else l [: - 1 ] + r full . append ( new ) else : for l , r in itt . zip_longest ( left , right , fillvalue = []): new = l + r [ 1 :] if len ( l ) > len ( r ) else l [: - 1 ] + r full . append ( new ) return full def _left_to_right_overlap ( biluov , sentence ): Note LEFTMOST TAG MUST BE 'V' _left_to_right_overlap(['V', 'V', 'O', 'V', 'I', 'L', 'O', 'I', 'L'], range(9)) [[0, 1, 3, 4, 5], [0, 1, 3, 7, 8]] _left_to_right_overlap(['V', 'O', 'V', 'O'], range(4)) [] _left_to_right_overlap(['V', 'O', 'V', 'O', 'L'], range(5)) [[0, 2, 4], [0, 2]] _left_to_right_overlap(['V', 'O', 'V', 'O', 'L', 'O', 'L'], range(8)) [[0, 2, 4], [0, 2, 6]] _left_to_right_overlap(['V', 'O', 'V', 'O', 'L', 'I', 'L', 'V', 'L'], range(9)) [[0, 2, 4], [0, 2, 5, 6]] return _build_overlap ( biluov , sentence , \"L\" ) def _right_to_left_overlap ( biluov , sentence ): Note RIGHTMOST TAG MUST BE 'V' _right_to_left_overlap(['B', 'I', 'O', 'B', 'I', 'V', 'O', 'V', 'V'], range(9)) [[3, 4, 5, 7, 8], [0, 1, 5, 7, 8]] _right_to_left_overlap(['O', 'V', 'O', 'V'], range(4)) [] _right_to_left_overlap(['B', 'O', 'V', 'O', 'V'], range(5)) [[0, 2, 4], [2, 4]] _right_to_left_overlap(['B', 'O', 'B', 'O', 'V', 'O', 'V'], range(7)) [[2, 4, 6], [0, 4, 6]] _right_to_left_overlap(['B', 'V', 'B', 'I', 'B', 'O', 'V', 'O', 'V'], range(9)) [[4, 6, 8], [2, 3, 6, 8]] inverse = _build_overlap ( reversed ( biluov ), reversed ( sentence ), \"B\" ) for x in inverse : x . reverse () return inverse def _build_overlap ( biluov , sentence , finisher ): Note LEFTMOST TAG MUST BE 'V' one_shot = zip ( biluov , sentence ) tag , word = next ( one_shot ) prefix = [] complete = [] try : while tag in ( \"V\" , \"O\" ): if tag == \"V\" : prefix . append ( word ) tag , word = next ( one_shot ) on_build = [] while tag in ( \"O\" , \"I\" , \"U\" , finisher ): if tag == \"I\" : on_build . append ( word ) elif tag == finisher : complete . append ( prefix + on_build + [ word ]) on_build . clear () elif tag == \"U\" : complete . append ([ word ]) tag , word = next ( one_shot ) except StopIteration : pass if len ( complete ) == 1 : complete . append ( prefix ) return complete","title":"Autogoal.datasets.ehealthkd20. encoding"},{"location":"api/autogoal.datasets.ehealthkd20._tools/","text":"import bisect import sys def offset ( id ): return id [ 0 ] + str ( int ( id [ 1 :]) + 1000 ) class EntityAnnotation : def __init__ ( self , id , typ , spans , text ): self . id = id self . type = typ self . spans = spans self . text = text @staticmethod def parse ( line ): id , mid , text = line . strip () . split ( \" \\t \" ) typ , spans = mid . split ( \" \" , 1 ) spans = [ tuple ( s . split ()) for s in spans . split ( \";\" )] return EntityAnnotation ( id , typ , spans , text ) def __repr__ ( self ): return \"<Entity(id= %r , type= %r , spans= %r , text= %r )>\" % ( self . id , self . type , self . spans , self . text , ) def offset_id ( self ): self . id = offset ( self . id ) def as_brat ( self ): spans = \";\" . join ( \" \" . join ( s ) for s in self . spans ) return \" %s \\t %s %s \\t %s \" % ( self . id , self . type , spans , self . text ) class RelationAnnotation : def __init__ ( self , id , typ , arg1 , arg2 ): self . id = id self . type = typ self . arg1 = arg1 self . arg2 = arg2 @staticmethod def parse ( line ): id , typ , arg1 , arg2 = line . strip () . split () arg1 = arg1 . split ( \":\" )[ 1 ] arg2 = arg2 . split ( \":\" )[ 1 ] return RelationAnnotation ( id , typ , arg1 , arg2 ) def offset_id ( self ): self . arg1 = offset ( self . arg1 ) self . arg2 = offset ( self . arg2 ) self . id = offset ( self . id ) def __repr__ ( self ): return \"<Relation(id= %r , type= %r , arg1= %r , arg2= %r )>\" % ( self . id , self . type , self . arg1 , self . arg2 , ) def as_brat ( self ): return \" %s \\t %s Arg1: %s Arg2: %s \" % ( self . id , self . type , self . arg1 , self . arg2 ) class SameAsAnnotation : total = 0 def __init__ ( self , id , typ , args ): self . id = id self . type = typ self . args = args @staticmethod def parse ( line ): SameAsAnnotation . total += 1 typ , args = line [ 1 :] . strip () . split ( \" \" , 1 ) id = \"* %d \" % SameAsAnnotation . total args = args . split () return SameAsAnnotation ( id , typ , args ) def offset_id ( self ): self . args = [ offset ( arg ) for arg in self . args ] def __repr__ ( self ): return \"<Relation(id= %r , type= %r , args= %r )>\" % ( self . id , self . type , self . args ) def as_brat ( self ): return \"* \\t %s %s \" % ( self . type , \" \" . join ( self . args )) class EventAnnotation : def __init__ ( self , id , typ , ref , args ): self . id = id self . type = typ self . ref = ref self . args = args @staticmethod def parse ( line ): id , mid = line . strip () . split ( \" \\t \" ) args = mid . split () typ , ref = args [ 0 ] . split ( \":\" ) args = args [ 1 :] args = { arg . split ( \":\" )[ 0 ]: arg . split ( \":\" )[ 1 ] for arg in args } return EventAnnotation ( id , typ , ref , args ) def offset_id ( self ): self . ref = offset ( self . ref ) self . id = offset ( self . id ) for k in self . args : self . args [ k ] = offset ( self . args [ k ]) def __repr__ ( self ): return \"<Event(id= %r , type= %r , ref= %r , args= %r )>\" % ( self . id , self . type , self . ref , self . args , ) def as_brat ( self ): spans = \" \" . join ( k + \":\" + v for k , v in self . args . items ()) return \" %s \\t %s : %s %s \" % ( self . id , self . type , self . ref , spans ) class AttributeAnnotation : def __init__ ( self , id , typ , ref ): self . id = id self . type = typ self . ref = ref @staticmethod def parse ( line ): id , typ , ref = line . strip () . split () return AttributeAnnotation ( id , typ , ref ) def offset_id ( self ): self . ref = offset ( self . ref ) self . id = offset ( self . id ) def __repr__ ( self ): return \"<Attribute(id= %r , type= %r , ref= %r )>\" % ( self . id , self . type , self . ref ) def as_brat ( self ): return \" %s \\t %s %s \" % ( self . id , self . type , self . ref ) class AnnFile : def __init__ ( self ): self . annotations = [] def load ( self , path ): with open ( path ) as fp : for line in fp : ann = self . _parse ( line ) if ann : self . annotations . append ( ann ) return self def annotations_of ( self , type ): for e in self . annotations : if isinstance ( e , type ): yield e def filter_sentences ( self , sentences , order ): skip = 0 sentence = 0 free_space = 0 skipped_space = [] selected_sentence_spans = [] while order : next_sentence = order . pop ( 0 ) - 1 skip_backup = skip while sentence != next_sentence : skip += len ( sentences . pop ( 0 )) + 1 sentence += 1 free_space += skip - skip_backup current_length = len ( sentences [ 0 ]) selected_sentence_spans . append (( skip , skip + current_length )) skipped_space . append ( free_space ) free_space -= current_length + 1 selected_annotations = {} for entity in self . annotations_of ( EntityAnnotation ): min_start = min ( int ( start ) for start , _ in entity . spans ) max_end = max ( int ( end ) for _ , end in entity . spans ) try : sentence = next ( i for i , ( start , end ) in enumerate ( selected_sentence_spans ) if ( start <= min_start and max_end <= end ) ) except StopIteration : continue entity . spans = [ tuple ( str ( int ( x ) - skipped_space [ sentence ]) for x in span ) for span in entity . spans ] selected_annotations [ entity . id ] = entity for ann in self . annotations_of ( EventAnnotation ): if ann . ref in selected_annotations : selected_annotations [ ann . id ] = ann for ann in self . annotations : add = ( isinstance ( ann , SameAsAnnotation ) and ann . args [ 0 ] in selected_annotations ) add |= ( isinstance ( ann , RelationAnnotation ) and ann . arg1 in selected_annotations ) add |= ( isinstance ( ann , AttributeAnnotation ) and ann . ref in selected_annotations ) if add : selected_annotations [ ann . id ] = ann self . annotations = list ( selected_annotations . values ()) def offset_spans ( self , sentences , first ): sentences_offset = self . _compute_sentence_offset ( sentences ) for ann in self . annotations_of ( EntityAnnotation ): locations = list ( set ( [ bisect . bisect_left ( sentences_offset , int ( s )) for span in ann . spans for s in span ] ) ) if len ( locations ) != 1 : raise ValueError () location = locations . pop () offset = sentences_offset [ location ] + 1 if first : offset = sentences_offset [ location - 1 ] + 1 if location > 0 else 0 ann . spans = [ ( str ( int ( span [ 0 ]) + offset ), str ( int ( span [ 1 ]) + offset )) for span in ann . spans ] def _compute_sentence_offset ( self , sentences ): sentences_offset = [ - 1 ] for s in sentences : prev = sentences_offset [ - 1 ] start = prev + 1 end = start + len ( s ) sentences_offset . append ( end ) sentences_offset . pop ( 0 ) return sentences_offset def offset_ids ( self ): for ann in self . annotations : ann . offset_id () def _parse ( self , line ): if line . startswith ( \"T\" ): return EntityAnnotation . parse ( line ) if line . startswith ( \"R\" ): return RelationAnnotation . parse ( line ) if line . startswith ( \"*\" ): return SameAsAnnotation . parse ( line ) if line . startswith ( \"E\" ): return EventAnnotation . parse ( line ) if line . startswith ( \"A\" ): return AttributeAnnotation . parse ( line ) if line . startswith ( \"#\" ): return None raise ValueError ( \"Unknown annotation: %s \" % line ) def merge ( ann1 : str , ann2 : str , text : str ): Note \"\"\"Merge annotations of two different versions of the same file. file1 = AnnFile () . load ( ann1 ) file2 = AnnFile () . load ( ann2 ) sents = open ( text ) . read () . split ( \" \\n \" ) file1 . offset_spans ( sents , first = True ) file2 . offset_spans ( sents , first = False ) file2 . offset_ids () for ann in file1 . annotations : print ( ann . as_brat ()) for ann in file2 . annotations : print ( ann . as_brat ()) def review ( ann : str , text : str , order : str ): Note \"\"\"Process a merged annotation file and outputs the selected annotations. file1 = AnnFile () . load ( ann ) sents = open ( text ) . read () . split ( \" \\n \" ) order = open ( order ) . read () . split ( \" \\n \" ) order = [ int ( line . strip ( \"*\" )) for line in order if line ] file1 . filter_sentences ( sents , order ) for ann in file1 . annotations : print ( ann . as_brat ()) def review_text ( text : str , order : str ): Note \"\"\"Process a merged annotation file and outputs the selected sentences. sents = open ( text ) . read () . split ( \" \\n \" ) order = open ( order ) . read () . split ( \" \\n \" ) order = [ int ( line . strip ( \"*\" )) for line in order if line ] selected = [ sents [ i - 1 ] for i in order ] for sent in selected : print ( sent ) def to_review ( order : str ): order = open ( order ) . read () . split ( \" \\n \" ) for i , o in enumerate ( order ): if o . endswith ( \"*\" ): print ( i + 1 )","title":"Autogoal.datasets.ehealthkd20. tools"},{"location":"api/autogoal.datasets.ehealthkd20._utils/","text":"import bisect import collections import re import warnings from collections import defaultdict from pathlib import Path from typing import List from autogoal.datasets.ehealthkd20._tools import ( AnnFile , AttributeAnnotation , EntityAnnotation , EventAnnotation , RelationAnnotation , SameAsAnnotation , ) ENTITIES = [ \"Concept\" , \"Action\" , \"Predicate\" , \"Reference\" ] RELATIONS = [ \"is-a\" , \"same-as\" , \"part-of\" , \"has-property\" , \"causes\" , \"entails\" , \"in-context\" , \"in-place\" , \"in-time\" , \"subject\" , \"target\" , \"domain\" , \"arg\" , ] class Keyphrase : def __init__ ( self , sentence , label , id , spans ): self . sentence : Sentence = sentence self . label = label self . id = id self . spans = spans self . attributes : List [ Attribute ] = [] def split ( self ): if len ( self . spans ) > 1 : raise TypeError ( \"Cannot split a keyphrase with multiple spans\" ) start , end = self . spans [ 0 ] spans = [] spans . append ( start ) for i , c in enumerate ( self . text ): if c == \" \" : spans . append ( start + i ) spans . append ( start + i + 1 ) spans . append ( end ) self . spans = [( spans [ i ], spans [ i + 1 ]) for i in range ( 0 , len ( spans ), 2 )] def clone ( self , sentence , shallow = False ) -> \"Keyphrase\" : k = Keyphrase ( sentence , self . label , self . id , self . spans ) k . attributes = [ a if shallow else a . clone ( k ) for a in self . attributes ] return k @property def text ( self ): return \" \" . join ( self . sentence . text [ s : e ] for ( s , e ) in self . spans ) def __repr__ ( self ): return \"Keyphrase(text= %r , label= %r , id= %r , attr= %r )\" % ( self . text , self . label , self . id , self . attributes , ) def as_ann ( self , shift ): return \"T {0} \\t {1} {2} \\t {3} \\n \" . format ( self . id , self . label , \";\" . join ( \" {} {} \" . format ( start + shift , end + shift ) for start , end in self . spans ), self . text , ) def matches ( self , other : \"Keyphrase\" , label = None ): return ( isinstance ( other , Keyphrase ) and self . sentence . text == other . sentence . text and self . spans == other . spans and ( ( label is None and self . label == other . label ) or ( label is not None and self . label == label ) ) ) def find_attributes ( self , label ) -> \"Attribute\" : return [ attr for attr in self . attributes if attr . label == label ] class Relation : def __init__ ( self , sentence , origin , destination , label ): self . sentence = sentence self . origin = origin self . destination = destination self . label = label def clone ( self , sentence ) -> \"Relation\" : return Relation ( sentence , self . origin , self . destination , self . label ) @property def from_phrase ( self ) -> Keyphrase : return self . sentence . find_keyphrase ( id = self . origin ) @property def to_phrase ( self ) -> Keyphrase : return self . sentence . find_keyphrase ( id = self . destination ) class _Unk : text = \"UNK\" def __repr__ ( self ): from_phrase = ( self . from_phrase or Relation . _Unk ()) . text to_phrase = ( self . to_phrase or Relation . _Unk ()) . text return \"Relation(from= %r , to= %r , label= %r )\" % ( from_phrase , to_phrase , self . label , ) def as_ann ( self , shift ): if self . label == \"same-as\" : return \"* \\t same-as T {0} T {1} \\n \" . format ( self . origin , self . destination ) else : return \"R {0} \\t {1} Arg1:T {2} Arg2:T {3} \\n \" . format ( shift , self . label , self . origin , self . destination ) def matches ( self , other : \"Relation\" , label = None ): return ( isinstance ( other , Relation ) and self . sentence . text == other . sentence . text and self . from_phrase . matches ( other . from_phrase ) and self . to_phrase . matches ( other . to_phrase ) and ( ( label is None and self . label == other . label ) or ( label is not None and self . label == label ) ) ) class Attribute : def __init__ ( self , keyphrase : Keyphrase , label ): self . keyphrase = keyphrase self . label = label def clone ( self , keyphrase ) -> \"Attribute\" : return Attribute ( keyphrase , self . label ) def __repr__ ( self ): return \"Attribute(label= %r )\" % ( self . label ,) def as_ann ( self , shift ): return \"A {0} \\t {1} T {2} \\n \" . format ( shift , self . label , self . keyphrase . id ) class Sentence : def __init__ ( self , text ): self . text = text self . keyphrases : List [ Keyphrase ] = [] self . relations : List [ Relation ] = [] def clone ( self , shallow = False ) -> \"Sentence\" : s = Sentence ( self . text ) s . keyphrases = [ k if shallow else k . clone ( s ) for k in self . keyphrases ] s . relations = [ r if shallow else r . clone ( s ) for r in self . relations ] return s def fix_ids ( self , start = 1 ): next_id = start copy = self . clone () for k , kc in zip ( self . keyphrases , copy . keyphrases ): for r , rc in zip ( self . relations , copy . relations ): if rc . origin == kc . id : r . origin = next_id if rc . destination == kc . id : r . destination = next_id k . id = next_id next_id += 1 return next_id def overlapping_keyphrases ( self ): result = [] for s1 in self . keyphrases : overlaps = set ([ s1 ]) for s2 in self . keyphrases : if s2 . spans == s1 . spans : overlaps . add ( s2 ) if len ( overlaps ) > 1 and overlaps not in result : result . append ( overlaps ) return result def merge_overlapping_keyphrases ( self ): overlaps = self . overlapping_keyphrases () for keyphrases in overlaps : keyphrases = list ( keyphrases ) first = keyphrases [ 0 ] rest = keyphrases [ 1 :] rest_ids = [ k . id for k in rest ] for relation in self . relations : if relation . origin in rest_ids : print ( \"Changing %r origin from %s to %s \" % ( relation , relation . origin , first . id ) ) relation . origin = first . id if relation . destination in rest_ids : print ( \"Changing %r destination from %s to %s \" % ( relation , relation . destination , first . id ) ) relation . destination = first . id for keyp in rest : self . keyphrases . remove ( keyp ) def dup_relations ( self ): dup_relations = collections . defaultdict ( lambda : []) for r in self . relations : dup_relations [( r . label , r . origin , r . destination )] . append ( r ) return { k : v for k , v in dup_relations . items () if len ( v ) > 1 } def remove_dup_relations ( self ): new_relations = {} for r in self . relations : new_relations [( r . label , r . origin , r . destination )] = r self . relations = list ( new_relations . values ()) def find_first_match ( self , annotation , label = None ): matches = self . find_matches ( annotation , label ) return None if not matches else matches [ 0 ] def find_matches ( self , annotation , label = None ): if isinstance ( annotation , Keyphrase ): return [ k for k in self . keyphrases if k . matches ( annotation , label )] elif isinstance ( annotation , Relation ): return [ r for r in self . relations if r . matches ( annotation , label )] else : raise TypeError ( \"Invalid annotation\" ) def find_keyphrase ( self , id = None , start = None , end = None , spans = None ) -> Keyphrase : if id is not None : return self . _find_keyphrase_by_id ( id ) if spans is None : spans = [( start , end )] return self . _find_keyphrase_by_spans ( spans ) def find_relations ( self , orig , dest ) -> List [ Relation ]: results = [] for r in self . relations : if r . origin == orig and r . destination == dest : results . append ( r ) return results def find_relation ( self , orig , dest , label ) -> Relation : for r in self . relations : if r . origin == orig and r . destination == dest and label == r . label : return r return None def _find_keyphrase_by_id ( self , id ) -> Keyphrase : for k in self . keyphrases : if k . id == id : return k return None def _find_keyphrase_by_spans ( self , spans ) -> Keyphrase : for k in self . keyphrases : if k . spans == spans : return k return None def sort ( self ): self . keyphrases . sort ( key = lambda k : tuple ([ s for s , e in k . spans ] + [ e for s , e in k . spans ]) ) def __len__ ( self ): return len ( self . text ) def __repr__ ( self ): return \"Sentence(text= %r , keyphrases= %r , relations= %r )\" % ( self . text , self . keyphrases , self . relations , ) @staticmethod def load ( finput ) -> \"List[Sentence]\" : return [ Sentence ( s . strip ()) for s in finput . read_text ( encoding = \"utf8\" ) . splitlines () if s ] @property def annotated ( self ): return self . keyphrases or self . relations class Collection : def __init__ ( self , sentences = None ): self . sentences : List [ Sentence ] = sentences or [] def clone ( self , skip_empty = False ) -> \"Collection\" : return Collection ( [ s . clone () for s in self . sentences if not skip_empty or s . annotated ] ) def merge ( self , * collections : \"Collection\" , skip_empty = False ): clone = self . clone ( skip_empty ) sentences = [ s . clone () for c in collections for s in c . sentences if not skip_empty or s . annotated ] clone . sentences . extend ( sentences ) return clone def __len__ ( self ): return len ( self . sentences ) def fix_ids ( self ): next_id = 1 for s in self . sentences : next_id = s . fix_ids ( next_id ) def filter ( self , keyphrase = None , relation = None , attribute = None ,) -> \"Collection\" : sentences = [] for sentence in self . sentences : s = Sentence ( sentence . text ) keyphrases s . keyphrases = [ k . clone ( s ) for k in sentence . keyphrases if keyphrase is None or keyphrase ( k ) ] attributes if attribute is not None : for k in s . keyphrases : k . attributes = [ a for a in k . attributes if attribute is None or attribute ( a ) ] s . keyphrases = [ k for k in s . keyphrases if k . attributes ] relations s . relations = [ r . clone ( s ) for r in sentence . relations if ( relation is None or relation ( r )) and ( keyphrase is None or ( keyphrase ( r . from_phrase ) and keyphrase ( r . to_phrase )) ) and ( attribute is None or ( any ( attribute ( a ) for a in r . from_phrase . attributes ) and any ( attribute ( a ) for a in r . to_phrase . attributes ) ) ) ] sentences . append ( s ) return Collection ( sentences ) def filter_keyphrase ( self , labels ) -> \"Collection\" : return self . filter ( keyphrase = lambda k : k . label in labels ) def filter_relation ( self , labels ) -> \"Collection\" : return self . filter ( relation = lambda r : r . label in labels ) def filter_attribute ( self , labels ) -> \"Collection\" : return self . filter ( attribute = lambda a : a . label in labels ) def find_first_match ( self , text ) -> Sentence : matches = self . find_matches ( text ) return None if not matches else matches [ 0 ] def find_matches ( self , text ) -> List [ Sentence ]: return [ s for s in self . sentences if s . text == text ] def load ( self , finput : Path , * , legacy = True , keyphrases = True , relations = True , attributes = True ) -> \"Collection\" : return CollectionV2Handler . load ( self , finput , legacy = legacy , keyphrases = keyphrases , relations = relations , attributes = attributes , ) def dump ( self , text_file : Path , skip_empty_sentences = True ): return CollectionV2Handler . dump ( self , text_file , skip_empty_sentences ) def load_dir ( self , finput : Path , * , legacy = True , keyphrases = True , relations = True , attributes = True ) -> \"Collection\" : return CollectionV2Handler . load_dir ( self , finput , legacy = legacy , keyphrases = keyphrases , relations = relations , attributes = attributes , ) class CollectionHandler : @classmethod def load_dir ( cls , collection : Collection , finput : Path , ** kargs ) -> Collection : pass @classmethod def load ( cls , collection : Collection , finput : Path , ** kargs ) -> Collection : pass @classmethod def dump ( cls , collection : Collection , text_file : Path , skip_empty_sentences = True ): pass class CollectionV1Handler ( CollectionHandler ): @classmethod def load_dir ( cls , collection : Collection , finput : Path ) -> Collection : for item in finput . iterdir (): if re . fullmatch ( r \".*put_scenario.*\\.txt\" , item . name ): cls . load ( collection , item ) return collection @classmethod def load ( cls , collection : Collection , finput : Path ) -> Collection : input_b_file = finput . parent / ( \"output_b_\" + finput . name . split ( \"_\" )[ 1 ]) sentence_by_id = cls . _load_keyphrases ( collection , finput ) for line in input_b_file . open ( encoding = \"utf8\" ) . readlines (): label , src , dst = line . strip () . split ( \" \\t \" ) src , dst = int ( src ), int ( dst ) the_sentence = sentence_by_id [ src ] if the_sentence != sentence_by_id [ dst ]: warnings . warn ( \"In file ' %s ' relation ' %s ' between %i and %i crosses sentence boundaries and has been ignored.\" % ( finput , label , src , dst ) ) continue assert sentence_by_id [ dst ] == the_sentence the_sentence . relations . append ( Relation ( the_sentence , src , dst , label . lower ()) ) return collection @classmethod def _load_keyphrases ( cls , collection : Collection , finput : Path ): cls . _load_input ( collection , finput ) input_a_file = finput . parent / ( \"output_a_\" + finput . name . split ( \"_\" )[ 1 ]) sentences_length = [ len ( s . text ) for s in collection . sentences ] for i in range ( 1 , len ( sentences_length )): sentences_length [ i ] += sentences_length [ i - 1 ] + 1 sentence_by_id = {} for line in input_a_file . open ( encoding = \"utf8\" ) . readlines (): lid , spans , label , _ = line . strip () . split ( \" \\t \" ) lid = int ( lid ) spans = [ s . split () for s in spans . split ( \";\" )] spans = [( int ( start ), int ( end )) for start , end in spans ] find the sentence where this annotation is i = bisect . bisect ( sentences_length , spans [ 0 ][ 0 ]) correct the annotation spans if i > 0 : spans = [ ( start - sentences_length [ i - 1 ] - 1 , end - sentences_length [ i - 1 ] - 1 , ) for start , end in spans ] spans . sort ( key = lambda t : t [ 0 ]) store the annotation in the corresponding sentence the_sentence = collection . sentences [ i ] keyphrase = Keyphrase ( the_sentence , label , lid , spans ) the_sentence . keyphrases . append ( keyphrase ) if len ( keyphrase . spans ) == 1 : keyphrase . split () sentence_by_id [ lid ] = the_sentence return sentence_by_id @classmethod def _load_input ( cls , collection : Collection , finput : Path ): sentences = [ s . strip () for s in finput . open ( encoding = \"utf8\" ) . readlines () if s ] sentences_obj = [ Sentence ( text ) for text in sentences ] collection . sentences . extend ( sentences_obj ) @classmethod def dump ( cls , collection : Collection , text_file : Path , skip_empty_sentences = True ): collection . fix_ids () input_file = text_file . open ( \"w\" , encoding = \"utf8\" ) output_a_file = ( text_file . parent / ( \"output_a_\" + text_file . name . split ( \"_\" )[ 1 ]) ) . open ( \"w\" , encoding = \"utf8\" ) output_b_file = ( text_file . parent / ( \"output_b_\" + text_file . name . split ( \"_\" )[ 1 ]) ) . open ( \"w\" , encoding = \"utf8\" ) shift = 0 for sentence in collection . sentences : if ( not sentence . keyphrases and not sentence . relations and skip_empty_sentences ): continue input_file . write ( \" {} \\n \" . format ( sentence . text )) for keyphrase in sentence . keyphrases : output_a_file . write ( \" {0} \\t {1} \\t {2} \\t {3} \\n \" . format ( keyphrase . id , \";\" . join ( \" {} {} \" . format ( start + shift , end + shift ) for start , end in keyphrase . spans ), keyphrase . label , keyphrase . text , ) ) for relation in sentence . relations : output_b_file . write ( \" {0} \\t {1} \\t {2} \\n \" . format ( relation . label , relation . origin , relation . destination ) ) shift += len ( sentence ) + 1 class CollectionV2Handler ( CollectionHandler ): @classmethod def load_dir ( cls , collection : Collection , finput : Path , * , legacy = True , keyphrases = True , relations = True , attributes = True ) -> Collection : for item in finput . iterdir (): if item . suffix == \".txt\" : cls . load ( collection , item , legacy = legacy , keyphrases = keyphrases , relations = relations , attributes = attributes , ) return collection @classmethod def load ( cls , collection : Collection , finput : Path , * , legacy = True , keyphrases = True , relations = True , attributes = True ) -> \"Collection\" : add sentences from input .txt to Collection sentences = cls . _load_input ( collection , finput ) if keyphrases won't be loaded finish right there if not keyphrases : return collection else, parse .ann file to start the annotation of sentences ann_file = cls . _load_ann ( finput ) def add_relation ( source_id , destination_id , ann_type , id_to_keyphrase ): source = id_to_keyphrase [ source_id ] destination = id_to_keyphrase [ destination_id ] if source . sentence != destination . sentence : warnings . warn ( \"In file ' %s ' relation ' %s ' between %i and %i crosses sentence boundaries and has been ignored.\" % ( finput , ann_type , source_id , destination_id ) ) else : relation = Relation ( source . sentence , source . id , destination . id , ann_type ) source . sentence . relations . append ( relation ) def legacy_load ( ann_file , sentences , id_to_keyphrase ): for ann in ann_file . annotations : if isinstance ( ann , EventAnnotation ): id_to_keyphrase [ ann . id ] = id_to_keyphrase [ ann . ref ] for ann in ann_file . annotations : if not isinstance ( ann , EventAnnotation ): continue for label , destination in ann . args . items (): label = \"\" . join ( i for i in label if not i . isdigit ()) . lower () add_relation ( ann . ref , destination , label , id_to_keyphrase ) compute sentences' boundaries sentences_length = [ len ( s ) for s in sentences ] for i in range ( 1 , len ( sentences_length )): sentences_length [ i ] += sentences_length [ i - 1 ] + 1 load keyphrases from Entity Annotations id_to_keyphrase = {} for ann in ann_file . annotations : if isinstance ( ann , EntityAnnotation ): tid = int ( ann . id [ 1 :]) spans = [( int ( start ), int ( end )) for start , end in ann . spans ] sid , spans = cls . _get_relative_ann ( spans , sentences_length ) sentence = sentences [ sid ] keyphrase = Keyphrase ( sentence , ann . type , tid , spans ) sentence . keyphrases . append ( keyphrase ) if len ( keyphrase . spans ) == 1 : keyphrase . split () id_to_keyphrase [ ann . id ] = keyphrase load relations from Event Annotations (legacy support) if legacy and relations : legacy_load ( ann_file , sentences , id_to_keyphrase ) load standard relations and attributes for ann in ann_file . annotations : if isinstance ( ann , RelationAnnotation ) and relations : add_relation ( ann . arg1 , ann . arg2 , ann . type , id_to_keyphrase ) elif isinstance ( ann , SameAsAnnotation ) and relations : source = ann . args [ 0 ] for destination in ann . args [ 1 :]: add_relation ( source , destination , ann . type , id_to_keyphrase ) elif isinstance ( ann , AttributeAnnotation ) and attributes : keyphrase = id_to_keyphrase [ ann . ref ] attribute = Attribute ( keyphrase , ann . type ) keyphrase . attributes . append ( attribute ) elif not ( isinstance ( ann , EntityAnnotation ) or legacy and isinstance ( ann , EventAnnotation ) ): warnings . warn ( \"In file ' %s ' annotation ' %s ' has been ignored.\" % ( finput , ann ) ) for s in sentences : s . sort () return collection @classmethod def _load_input ( cls , collection : Collection , finput : Path ) -> List [ Sentence ]: sentences = Sentence . load ( finput ) collection . sentences . extend ( sentences ) return sentences @classmethod def _load_ann ( cls , finput : Path ) -> AnnFile : ann_path : Path = finput . parent / ( finput . stem + \".ann\" ) return AnnFile () . load ( ann_path ) @classmethod def _get_relative_ann ( cls , spans , sentences_length : List [ int ]) -> int : find the sentence where this annotation is i = bisect . bisect ( sentences_length , spans [ 0 ][ 0 ]) correct the annotation spans if i > 0 : spans = [ ( start - sentences_length [ i - 1 ] - 1 , end - sentences_length [ i - 1 ] - 1 , ) for start , end in spans ] spans . sort ( key = lambda t : t [ 0 ]) return i , spans @classmethod def dump ( cls , collection : Collection , text_file , skip_empty_sentences = True ): ann_path : Path = text_file . parent / ( text_file . stem + \".ann\" ) cls . _dump_input ( collection , text_file , skip_empty_sentences ) cls . _dump_ann ( collection , ann_path , skip_empty_sentences ) @classmethod def _dump_input ( cls , collection : Collection , text_file : Path , skip_empty_sentences = True ): text_file . parent . mkdir ( parents = True , exist_ok = True ) text_file . write_text ( \" \\n \" . join ( sentence . text for sentence in collection . sentences if not skip_empty_sentences or sentence . keyphrases or sentence . relations ), encoding = \"utf8\" , ) @classmethod def _dump_ann ( cls , collection : Collection , ann_path : Path , skip_empty_sentences = True ): collection . fix_ids () aid = 0 rid = 0 shift = 0 with ann_path . open ( \"w\" , encoding = \"utf8\" ) as ann_file : for sentence in collection . sentences : if ( skip_empty_sentences and not sentence . keyphrases and not sentence . relations ): continue for keyphrase in sentence . keyphrases : ann_file . write ( keyphrase . as_ann ( shift )) for attribute in keyphrase . attributes : ann_file . write ( attribute . as_ann ( aid )) aid += 1 for relation in sentence . relations : ann_file . write ( relation . as_ann ( rid )) if relation . label != \"same-as\" : rid += 1 shift += len ( sentence ) + 1 class DisjointSet : def __init__ ( self , * items ): self . nodes = { x : DisjointNode ( x ) for x in items } def merge ( self , items ): items = ( self . nodes [ x ] for x in items ) try : head , * others = items for other in others : head . merge ( other ) except ValueError : pass @property def representatives ( self ): return { n . representative for n in self . nodes . values ()} @property def groups ( self ): return [ [ n for n in self . nodes . values () if n . representative == r ] for r in self . representatives ] def __len__ ( self ): return len ( self . representatives ) def __getitem__ ( self , item ): return self . nodes [ item ] def __call__ ( self , item1 , item2 ): return self [ item1 ] . representative == self [ item2 ] . representative def __str__ ( self ): return str ( self . groups ) def __repr__ ( self ): return str ( self ) class DisjointNode : def __init__ ( self , value ): self . value = value self . parent = self @property def representative ( self ): if self . parent != self : self . parent = self . parent . representative return self . parent def merge ( self , other ): other . representative . parent = self . representative def __str__ ( self ): return str ( self . value ) def __repr__ ( self ): return str ( self )","title":"Autogoal.datasets.ehealthkd20. utils"},{"location":"api/autogoal.datasets.german_credit/","text":"import os import numpy as np from autogoal.datasets import download , datapath from sklearn.feature_extraction import DictVectorizer def _parse ( x ): return int ( x ) if x . isdigit () else x def load ( max_examples = None ): download ( \"german_credit\" ) f = open ( datapath ( \"german_credit\" ) / \"german.data\" , \"r\" ) X = [] y = [] for i in f . readlines (): if max_examples and len ( X ) >= max_examples : break clean_line = i . strip () . split () line = { \"feature_ %i \" % i : _parse ( v ) for i , v in enumerate ( clean_line [: - 1 ])} X . append ( line ) y . append ( int ( clean_line [ - 1 ]) == 2 ) return DictVectorizer ( sparse = False ) . fit_transform ( X ), np . asarray ( y )","title":"Autogoal.datasets.german credit"},{"location":"api/autogoal.datasets.gisette/","text":"import os import numpy as np from scipy import sparse as sp from autogoal.datasets import datapath , download def load (): Note Loads train and valid datasets from Gisette uci dataset . Examples \u00b6 ```python X_train, y_train, X_valid, y_valid = load() X_train.shape, X_valid.shape ((6000, 5000), (1000, 5000)) len(y_train), len(y_valid) (6000, 1000) ``` try : download ( \"gisette\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise train_data = open ( datapath ( \"gisette\" ) / \"gisette_train.data\" , \"r\" ) train_labels = open ( datapath ( \"gisette\" ) / \"gisette_train.labels\" , \"r\" ) valid_data = open ( datapath ( \"gisette\" ) / \"gisette_valid.data\" , \"r\" ) valid_labels = open ( datapath ( \"gisette\" ) / \"gisette_valid.labels\" , \"r\" ) Xtrain = sp . lil_matrix (( 6000 , 5000 )) ytrain = [] Xvalid = sp . lil_matrix (( 1000 , 5000 )) yvalid = [] for i , line in enumerate ( train_data ): for j , value in enumerate ( line . split ()): value = int ( value ) if value > 0 : Xtrain [ i , j ] = value for i , line in enumerate ( valid_data ): for j , value in enumerate ( line . split ()): value = int ( value ) if value > 0 : Xvalid [ i , j ] = value for line in train_labels : ytrain . append ( int ( line ) > 0 ) for line in valid_labels : yvalid . append ( int ( line ) > 0 ) return Xtrain . tocsr (), np . asarray ( ytrain ), Xvalid . tocsr (), np . asarray ( yvalid )","title":"Autogoal.datasets.gisette"},{"location":"api/autogoal.datasets.gisette/#examples","text":"```python X_train, y_train, X_valid, y_valid = load() X_train.shape, X_valid.shape ((6000, 5000), (1000, 5000)) len(y_train), len(y_valid) (6000, 1000) ``` try : download ( \"gisette\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise train_data = open ( datapath ( \"gisette\" ) / \"gisette_train.data\" , \"r\" ) train_labels = open ( datapath ( \"gisette\" ) / \"gisette_train.labels\" , \"r\" ) valid_data = open ( datapath ( \"gisette\" ) / \"gisette_valid.data\" , \"r\" ) valid_labels = open ( datapath ( \"gisette\" ) / \"gisette_valid.labels\" , \"r\" ) Xtrain = sp . lil_matrix (( 6000 , 5000 )) ytrain = [] Xvalid = sp . lil_matrix (( 1000 , 5000 )) yvalid = [] for i , line in enumerate ( train_data ): for j , value in enumerate ( line . split ()): value = int ( value ) if value > 0 : Xtrain [ i , j ] = value for i , line in enumerate ( valid_data ): for j , value in enumerate ( line . split ()): value = int ( value ) if value > 0 : Xvalid [ i , j ] = value for line in train_labels : ytrain . append ( int ( line ) > 0 ) for line in valid_labels : yvalid . append ( int ( line ) > 0 ) return Xtrain . tocsr (), np . asarray ( ytrain ), Xvalid . tocsr (), np . asarray ( yvalid )","title":"Examples"},{"location":"api/autogoal.datasets.haha/","text":"import os import pandas as pd import numpy as np from autogoal.datasets import datapath , download def load_raw ( max_examples = None ): Note Loads the train and test datasets for the HAHA 2019 corpus as Pandas dataframes. Examples \u00b6 ```python train, test = load_raw() len(train), len(test) (24000, 6000) train.columns Index(['id', 'text', 'is_humor', 'votes_no', 'votes_1', 'votes_2', 'votes_3', 'votes_4', 'votes_5', 'funniness_average'], dtype='object') train[\"funniness_average\"].mean() 2.0464498676235694 ``` download ( \"haha_2019\" ) train_df = pd . read_csv ( datapath ( \"haha_2019\" ) / \"haha_2019_train.csv\" ) test_df = pd . read_csv ( datapath ( \"haha_2019\" ) / \"haha_2019_test_gold.csv\" ) if max_examples is not None : train_df = train_df [: max_examples ] test_df = test_df [: max_examples ] return train_df , test_df def load ( target = \"is_humor\" , max_examples = None ): Note Loads the train and test datasets for the HAHA 2019 corpus as lists of texts and target values. Arguments \u00b6 target : Which column to use for target. Default is \"is_humor\" which can be used for binary classification. Another option is \"funniness_average\" which can be used for regression. Examples \u00b6 Loading with classification targets: ```python X_train, y_train, X_test, y_test = load() print(X_train[13]) Le\u00ed que la falta de sexo trae consigo una notable mejora en el l\u00e9xico. Me quedo absorto ante tal afirmaci\u00f3n carente de raciocinio. y_train[13] 1 ``` Loading with regression targets: ```python X_train, y_train, X_test, y_test = load(target=\"funniness_average\") print(X_train[13]) Le\u00ed que la falta de sexo trae consigo una notable mejora en el l\u00e9xico. Me quedo absorto ante tal afirmaci\u00f3n carente de raciocinio. y_train[13] 3.25 ``` Loading a subset of the dataset: ```python Xtrain, Xtest, ytrain, ytest = load(max_examples=100) len(Xtrain), len(Xtest), len(ytrain), len(ytest) (100, 100, 100, 100) ``` train_df , test_df = load_raw ( max_examples ) X_train = list ( train_df [ \"text\" ]) y_train = list ( train_df [ target ]) X_test = list ( test_df [ \"text\" ]) y_test = list ( test_df [ target ]) return X_train , np . asarray ( y_train ), X_test , np . asarray ( y_test )","title":"Autogoal.datasets.haha"},{"location":"api/autogoal.datasets.haha/#examples","text":"```python train, test = load_raw() len(train), len(test) (24000, 6000) train.columns Index(['id', 'text', 'is_humor', 'votes_no', 'votes_1', 'votes_2', 'votes_3', 'votes_4', 'votes_5', 'funniness_average'], dtype='object') train[\"funniness_average\"].mean() 2.0464498676235694 ``` download ( \"haha_2019\" ) train_df = pd . read_csv ( datapath ( \"haha_2019\" ) / \"haha_2019_train.csv\" ) test_df = pd . read_csv ( datapath ( \"haha_2019\" ) / \"haha_2019_test_gold.csv\" ) if max_examples is not None : train_df = train_df [: max_examples ] test_df = test_df [: max_examples ] return train_df , test_df def load ( target = \"is_humor\" , max_examples = None ): Note Loads the train and test datasets for the HAHA 2019 corpus as lists of texts and target values.","title":"Examples"},{"location":"api/autogoal.datasets.haha/#arguments","text":"target : Which column to use for target. Default is \"is_humor\" which can be used for binary classification. Another option is \"funniness_average\" which can be used for regression.","title":"Arguments"},{"location":"api/autogoal.datasets.haha/#examples_1","text":"Loading with classification targets: ```python X_train, y_train, X_test, y_test = load() print(X_train[13]) Le\u00ed que la falta de sexo trae consigo una notable mejora en el l\u00e9xico. Me quedo absorto ante tal afirmaci\u00f3n carente de raciocinio. y_train[13] 1 ``` Loading with regression targets: ```python X_train, y_train, X_test, y_test = load(target=\"funniness_average\") print(X_train[13]) Le\u00ed que la falta de sexo trae consigo una notable mejora en el l\u00e9xico. Me quedo absorto ante tal afirmaci\u00f3n carente de raciocinio. y_train[13] 3.25 ``` Loading a subset of the dataset: ```python Xtrain, Xtest, ytrain, ytest = load(max_examples=100) len(Xtrain), len(Xtest), len(ytrain), len(ytest) (100, 100, 100, 100) ``` train_df , test_df = load_raw ( max_examples ) X_train = list ( train_df [ \"text\" ]) y_train = list ( train_df [ target ]) X_test = list ( test_df [ \"text\" ]) y_test = list ( test_df [ target ]) return X_train , np . asarray ( y_train ), X_test , np . asarray ( y_test )","title":"Examples"},{"location":"api/autogoal.datasets.meddocan/","text":"import numpy as np import os from autogoal.datasets import datapath , download def load ( max_examples = None ): Note Loads train and test datasets from MEDDOCAN iberleaf 2018 . Examples \u00b6 ```python X_train, y_train, X_valid, y_valid = load() len(X_train), len(X_valid) (25622, 8432) len(y_train), len(y_valid) (25622, 8432) ``` try : download ( \"meddocan_2018\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise train_path = datapath ( \"meddocan_2018\" ) / \"train/brat\" dev_path = datapath ( \"meddocan_2018\" ) / \"dev/brat\" test_path = datapath ( \"meddocan_2018\" ) / \"test/brat\" X_train = [] X_test = [] y_train = [] y_test = [] total = 0 success = 0 failed = 0 for file in os . scandir ( train_path ): if file . name . split ( \".\" )[ 1 ] == \"ann\" : text , phi = parse_text_and_tags ( file . path ) brat_corpora , text , ibo_corpora = get_tagged_tokens ( text , phi ) if compare_tags ( brat_corpora , phi ): X_train . extend ( text ) y_train . extend ( ibo_corpora ) for file in os . scandir ( dev_path ): if file . name . split ( \".\" )[ 1 ] == \"ann\" : text , phi = parse_text_and_tags ( file . path ) brat_corpora , text , ibo_corpora = get_tagged_tokens ( text , phi ) if compare_tags ( brat_corpora , phi ): X_train . extend ( text ) y_train . extend ( ibo_corpora ) for file in os . scandir ( test_path ): if file . name . split ( \".\" )[ 1 ] == \"ann\" : text , phi = parse_text_and_tags ( file . path ) brat_corpora , text , ibo_corpora = get_tagged_tokens ( text , phi ) if compare_tags ( brat_corpora , phi ): X_test . extend ( text ) y_test . extend ( ibo_corpora ) if max_examples is not None : X_train = X_train [: max_examples ] X_test = X_test [: max_examples ] y_train = y_train [: max_examples ] y_test = y_test [: max_examples ] return X_train , y_train , X_test , y_test def parse_text_and_tags ( file_name = None ): Note Given a file representing an annotated text in Brat format returns the text and tags annotated. text = \"\" phi = [] if file_name is not None : text = open ( os . path . splitext ( file_name )[ 0 ] + \".txt\" , \"r\" ) . read () for row in open ( file_name , \"r\" ): line = row . strip () if line . startswith ( \"T\" ): # Lines is a Brat TAG try : label = line . split ( \" \\t \" )[ 1 ] . split () tag = label [ 0 ] start = int ( label [ 1 ]) end = int ( label [ 2 ]) value = text [ start : end ] phi . append (( tag , start , end , value )) except IndexError : print ( \"ERROR! Index error while splitting sentence '\" + line + \"' in document '\" + file_name + \"'!\" ) else : # Line is a Brat comment print ( \" \\t Skipping line (comment): \\t \" + line ) return ( text , phi ) def get_tagged_tokens ( text , tags ): Note convert a given text and annotations in brat format to IOB tag format Parameters: - text: raw text - tags: tags annotated on text with brat format output: tuple of identified tags in brat format from text and list of tokens tagged in IOB format tags . sort ( key = lambda x : x [ 1 ]) offset = 0 tagged_tokens = [] current_tag = \"\" current_tag_end = 0 current_tag_init = 0 processing_token = False token = \"\" tag = \"\" itag = 0 next_tag_init = tags [ itag ][ 1 ] sentences = [[]] for char in text : if processing_token and current_tag_end == offset : tagged_tokens . append (( current_tag , current_tag_init , offset , token )) tokens = token . split () if len ( tokens ) > 1 : sentences [ - 1 ] . append (( tokens [ 0 ], tag )) for tok in tokens [ 1 :]: sentences [ - 1 ] . append (( tok , \"I-\" + current_tag )) else : sentences [ - 1 ] . append (( token , tag )) token = \"\" current_tag = \"\" processing_token = False if not processing_token and char in [ \" \\n \" , \" \" , \",\" , \".\" , \";\" , \":\" , \"!\" , \"?\" , \"(\" , \")\" , ]: if token : sentences [ - 1 ] . append (( token , tag )) if char in [ \" \\n \" , \".\" , \"!\" , \" ?\" ] and len ( sentences [ - 1 ]) > 1 : sentences . append ([]) token = \"\" offset += 1 continue if offset == next_tag_init : if token : if char in [ \" \\n \" , \" \" , \",\" , \".\" , \";\" , \":\" , \"!\" , \"?\" , \"(\" , \")\" ]: sentences [ - 1 ] . append (( token , tag )) else : token += char sentences [ - 1 ] . append (( token , tag )) token = \"\" current_tag = tags [ itag ][ 0 ] current_tag_init = tags [ itag ][ 1 ] current_tag_end = tags [ itag ][ 2 ] processing_token = True itag += 1 next_tag_init = tags [ itag ][ 1 ] if itag < len ( tags ) else - 1 if processing_token and current_tag : if not token : tag = \"B-\" + current_tag else : tag = \"O\" token += char offset += 1 raw_sentences = [ [ word for word , _ in sentence ] for sentence in sentences if sentence ] raw_tags = [[ tag for _ , tag in sentence ] for sentence in sentences if sentence ] return tagged_tokens , raw_sentences , raw_tags def compare_tags ( tag_list , other_tag_list ): Note compare two tags lists with the same tag format: ( tag_name , start_offset , end_offset , value ) tags_amount = len ( tag_list ) if tags_amount != len ( other_tag_list ): print ( \"missmatch of amount of tags %d vs %d \" % ( tags_amount , len ( other_tag_list )) ) return False tag_list . sort ( key = lambda x : x [ 1 ]) other_tag_list . sort ( key = lambda x : x [ 1 ]) for i in range ( tags_amount ): if len ( tag_list [ i ]) != len ( other_tag_list [ i ]): print ( \"missmatch of tags format\" ) return False for j in range ( len ( tag_list [ i ])): if tag_list [ i ][ j ] != other_tag_list [ i ][ j ]: print ( \"missmatch of tags %s vs %s \" % ( tag_list [ i ][ j ], other_tag_list [ i ][ j ]) ) return False return True def get_qvals ( y , predicted ): tp = 0 fp = 0 fn = 0 total_sentences = 0 for i in range ( len ( y )): for j in range ( len ( y [ i ])): tag = y [ i ][ j ] predicted_tag = predicted [ i ][ j ] if tag != \"O\" : if tag == predicted_tag : tp += 1 else : fn += 1 elif tag != predicted_tag : fp += 1 total_sentences += 1 return tp , fp , fn , total_sentences def leak ( y , predicted ): Note leak evaluation function from MEDDOCAN iberleaf 2018 tp , fp , fn , total_sentences = get_qvals ( y , predicted ) try : return float ( fn / total_sentences ) except ZeroDivisionError : return 0.0 def precision ( y , predicted ): Note precision evaluation function from MEDDOCAN iberleaf 2018 tp , fp , fn , total_sentences = get_qvals ( y , predicted ) try : return tp / float ( tp + fp ) except ZeroDivisionError : return 0.0 def recall ( y , predicted ): Note recall evaluation function from MEDDOCAN iberleaf 2018 tp , fp , fn , total_sentences = get_qvals ( y , predicted ) try : return tp / float ( tp + fn ) except ZeroDivisionError : return 0.0 def F1_beta ( y , predicted , beta = 1 ): Note F1 evaluation function from MEDDOCAN iberleaf 2018 p = precision ( predicted , y ) r = recall ( predicted , y ) try : return ( 1 + beta ** 2 ) * (( p * r ) / ( p + r )) except ZeroDivisionError : return 0.0 pass def basic_fn ( y , predicted ): correct = 0 total = 0 for i in range ( len ( y )): for j in range ( len ( y [ i ])): total += 1 _ , tag = y [ i ][ j ] _ , predicted_tag = predicted [ i ][ j ] correct += 1 if tag == predicted_tag else 0 return correct / total def test_meddocan (): X_train , X_valid , y_train , y_valid = load () assert len ( X_train ) == len ( y_train ) assert len ( X_valid ) == len ( y_valid ) assert all ( isinstance ( x , list ) for x in X_train ) assert all ( isinstance ( x , list ) for x in X_valid ) assert all ( len ( x ) > 0 for x in X_train ) assert all ( len ( x ) > 0 for x in X_valid ) for xi , yi in zip ( X_train , y_train ): assert len ( xi ) == len ( yi ) for xi , yi in zip ( X_valid , y_valid ): assert len ( xi ) == len ( yi ) load ()","title":"Autogoal.datasets.meddocan"},{"location":"api/autogoal.datasets.meddocan/#examples","text":"```python X_train, y_train, X_valid, y_valid = load() len(X_train), len(X_valid) (25622, 8432) len(y_train), len(y_valid) (25622, 8432) ``` try : download ( \"meddocan_2018\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise train_path = datapath ( \"meddocan_2018\" ) / \"train/brat\" dev_path = datapath ( \"meddocan_2018\" ) / \"dev/brat\" test_path = datapath ( \"meddocan_2018\" ) / \"test/brat\" X_train = [] X_test = [] y_train = [] y_test = [] total = 0 success = 0 failed = 0 for file in os . scandir ( train_path ): if file . name . split ( \".\" )[ 1 ] == \"ann\" : text , phi = parse_text_and_tags ( file . path ) brat_corpora , text , ibo_corpora = get_tagged_tokens ( text , phi ) if compare_tags ( brat_corpora , phi ): X_train . extend ( text ) y_train . extend ( ibo_corpora ) for file in os . scandir ( dev_path ): if file . name . split ( \".\" )[ 1 ] == \"ann\" : text , phi = parse_text_and_tags ( file . path ) brat_corpora , text , ibo_corpora = get_tagged_tokens ( text , phi ) if compare_tags ( brat_corpora , phi ): X_train . extend ( text ) y_train . extend ( ibo_corpora ) for file in os . scandir ( test_path ): if file . name . split ( \".\" )[ 1 ] == \"ann\" : text , phi = parse_text_and_tags ( file . path ) brat_corpora , text , ibo_corpora = get_tagged_tokens ( text , phi ) if compare_tags ( brat_corpora , phi ): X_test . extend ( text ) y_test . extend ( ibo_corpora ) if max_examples is not None : X_train = X_train [: max_examples ] X_test = X_test [: max_examples ] y_train = y_train [: max_examples ] y_test = y_test [: max_examples ] return X_train , y_train , X_test , y_test def parse_text_and_tags ( file_name = None ): Note Given a file representing an annotated text in Brat format returns the text and tags annotated. text = \"\" phi = [] if file_name is not None : text = open ( os . path . splitext ( file_name )[ 0 ] + \".txt\" , \"r\" ) . read () for row in open ( file_name , \"r\" ): line = row . strip () if line . startswith ( \"T\" ): # Lines is a Brat TAG try : label = line . split ( \" \\t \" )[ 1 ] . split () tag = label [ 0 ] start = int ( label [ 1 ]) end = int ( label [ 2 ]) value = text [ start : end ] phi . append (( tag , start , end , value )) except IndexError : print ( \"ERROR! Index error while splitting sentence '\" + line + \"' in document '\" + file_name + \"'!\" ) else : # Line is a Brat comment print ( \" \\t Skipping line (comment): \\t \" + line ) return ( text , phi ) def get_tagged_tokens ( text , tags ): Note convert a given text and annotations in brat format to IOB tag format Parameters: - text: raw text - tags: tags annotated on text with brat format output: tuple of identified tags in brat format from text and list of tokens tagged in IOB format tags . sort ( key = lambda x : x [ 1 ]) offset = 0 tagged_tokens = [] current_tag = \"\" current_tag_end = 0 current_tag_init = 0 processing_token = False token = \"\" tag = \"\" itag = 0 next_tag_init = tags [ itag ][ 1 ] sentences = [[]] for char in text : if processing_token and current_tag_end == offset : tagged_tokens . append (( current_tag , current_tag_init , offset , token )) tokens = token . split () if len ( tokens ) > 1 : sentences [ - 1 ] . append (( tokens [ 0 ], tag )) for tok in tokens [ 1 :]: sentences [ - 1 ] . append (( tok , \"I-\" + current_tag )) else : sentences [ - 1 ] . append (( token , tag )) token = \"\" current_tag = \"\" processing_token = False if not processing_token and char in [ \" \\n \" , \" \" , \",\" , \".\" , \";\" , \":\" , \"!\" , \"?\" , \"(\" , \")\" , ]: if token : sentences [ - 1 ] . append (( token , tag )) if char in [ \" \\n \" , \".\" , \"!\" , \" ?\" ] and len ( sentences [ - 1 ]) > 1 : sentences . append ([]) token = \"\" offset += 1 continue if offset == next_tag_init : if token : if char in [ \" \\n \" , \" \" , \",\" , \".\" , \";\" , \":\" , \"!\" , \"?\" , \"(\" , \")\" ]: sentences [ - 1 ] . append (( token , tag )) else : token += char sentences [ - 1 ] . append (( token , tag )) token = \"\" current_tag = tags [ itag ][ 0 ] current_tag_init = tags [ itag ][ 1 ] current_tag_end = tags [ itag ][ 2 ] processing_token = True itag += 1 next_tag_init = tags [ itag ][ 1 ] if itag < len ( tags ) else - 1 if processing_token and current_tag : if not token : tag = \"B-\" + current_tag else : tag = \"O\" token += char offset += 1 raw_sentences = [ [ word for word , _ in sentence ] for sentence in sentences if sentence ] raw_tags = [[ tag for _ , tag in sentence ] for sentence in sentences if sentence ] return tagged_tokens , raw_sentences , raw_tags def compare_tags ( tag_list , other_tag_list ): Note compare two tags lists with the same tag format: ( tag_name , start_offset , end_offset , value ) tags_amount = len ( tag_list ) if tags_amount != len ( other_tag_list ): print ( \"missmatch of amount of tags %d vs %d \" % ( tags_amount , len ( other_tag_list )) ) return False tag_list . sort ( key = lambda x : x [ 1 ]) other_tag_list . sort ( key = lambda x : x [ 1 ]) for i in range ( tags_amount ): if len ( tag_list [ i ]) != len ( other_tag_list [ i ]): print ( \"missmatch of tags format\" ) return False for j in range ( len ( tag_list [ i ])): if tag_list [ i ][ j ] != other_tag_list [ i ][ j ]: print ( \"missmatch of tags %s vs %s \" % ( tag_list [ i ][ j ], other_tag_list [ i ][ j ]) ) return False return True def get_qvals ( y , predicted ): tp = 0 fp = 0 fn = 0 total_sentences = 0 for i in range ( len ( y )): for j in range ( len ( y [ i ])): tag = y [ i ][ j ] predicted_tag = predicted [ i ][ j ] if tag != \"O\" : if tag == predicted_tag : tp += 1 else : fn += 1 elif tag != predicted_tag : fp += 1 total_sentences += 1 return tp , fp , fn , total_sentences def leak ( y , predicted ): Note leak evaluation function from MEDDOCAN iberleaf 2018 tp , fp , fn , total_sentences = get_qvals ( y , predicted ) try : return float ( fn / total_sentences ) except ZeroDivisionError : return 0.0 def precision ( y , predicted ): Note precision evaluation function from MEDDOCAN iberleaf 2018 tp , fp , fn , total_sentences = get_qvals ( y , predicted ) try : return tp / float ( tp + fp ) except ZeroDivisionError : return 0.0 def recall ( y , predicted ): Note recall evaluation function from MEDDOCAN iberleaf 2018 tp , fp , fn , total_sentences = get_qvals ( y , predicted ) try : return tp / float ( tp + fn ) except ZeroDivisionError : return 0.0 def F1_beta ( y , predicted , beta = 1 ): Note F1 evaluation function from MEDDOCAN iberleaf 2018 p = precision ( predicted , y ) r = recall ( predicted , y ) try : return ( 1 + beta ** 2 ) * (( p * r ) / ( p + r )) except ZeroDivisionError : return 0.0 pass def basic_fn ( y , predicted ): correct = 0 total = 0 for i in range ( len ( y )): for j in range ( len ( y [ i ])): total += 1 _ , tag = y [ i ][ j ] _ , predicted_tag = predicted [ i ][ j ] correct += 1 if tag == predicted_tag else 0 return correct / total def test_meddocan (): X_train , X_valid , y_train , y_valid = load () assert len ( X_train ) == len ( y_train ) assert len ( X_valid ) == len ( y_valid ) assert all ( isinstance ( x , list ) for x in X_train ) assert all ( isinstance ( x , list ) for x in X_valid ) assert all ( len ( x ) > 0 for x in X_train ) assert all ( len ( x ) > 0 for x in X_valid ) for xi , yi in zip ( X_train , y_train ): assert len ( xi ) == len ( yi ) for xi , yi in zip ( X_valid , y_valid ): assert len ( xi ) == len ( yi ) load ()","title":"Examples"},{"location":"api/autogoal.datasets.movie_reviews/","text":"import random from autogoal.datasets import download , datapath def load ( max_examples = None ): try : download ( \"movie_reviews\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise sentences = [] classes = [] path = datapath ( \"movie_reviews\" ) ids = list ( path . rglob ( \"*.txt\" )) random . shuffle ( ids ) for fd in ids : if \"neg/\" in str ( fd ): cls = \"neg\" else : cls = \"pos\" with fd . open () as fp : sentences . append ( fp . read ()) classes . append ( cls ) if max_examples and len ( classes ) >= max_examples : break return sentences , classes def make_fn ( test_size = 0.25 , examples = None ): from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score X , y = load ( examples ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size ) def fitness_fn ( pipeline ): pipeline . fit ( X_train , y_train ) y_pred = pipeline . predict ( X_test ) return accuracy_score ( y_test , y_pred ) return fitness_fn","title":"Autogoal.datasets.movie reviews"},{"location":"api/autogoal.datasets.shuttle/","text":"import numpy as np import os from autogoal.datasets import datapath , download from sklearn.feature_extraction import DictVectorizer def load ( max_examples = None ): Note Loads train and valid datasets from Shuttle uci dataset . Examples \u00b6 ```python X_train, y_train, X_valid, y_valid = load() X_train.shape, X_valid.shape ((43500, 9), (14500, 9)) len(y_train), len(y_valid) (43500, 14500) ``` try : download ( \"shuttle\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise train_data = open ( datapath ( \"shuttle\" ) / \"shuttle.trn\" , \"r\" ) test_data = open ( datapath ( \"shuttle\" ) / \"shuttle.tst\" , \"r\" ) X_train = [] X_test = [] y_train = [] y_test = [] for i in train_data . readlines (): clean_line = i . strip () . split () temp = {} temp [ \"1\" ] = int ( clean_line [ 0 ]) temp [ \"2\" ] = int ( clean_line [ 1 ]) temp [ \"3\" ] = int ( clean_line [ 2 ]) temp [ \"4\" ] = int ( clean_line [ 3 ]) temp [ \"5\" ] = int ( clean_line [ 4 ]) temp [ \"6\" ] = int ( clean_line [ 5 ]) temp [ \"7\" ] = int ( clean_line [ 6 ]) temp [ \"8\" ] = int ( clean_line [ 7 ]) temp [ \"9\" ] = int ( clean_line [ 8 ]) X_train . append ( temp ) y_train . append ( clean_line [ 9 ]) if max_examples and len ( X_train ) >= max_examples : break for i in test_data . readlines (): clean_line = i . strip () . split () temp = {} temp [ \"1\" ] = int ( clean_line [ 0 ]) temp [ \"2\" ] = int ( clean_line [ 1 ]) temp [ \"3\" ] = int ( clean_line [ 2 ]) temp [ \"4\" ] = int ( clean_line [ 3 ]) temp [ \"5\" ] = int ( clean_line [ 4 ]) temp [ \"6\" ] = int ( clean_line [ 5 ]) temp [ \"7\" ] = int ( clean_line [ 6 ]) temp [ \"8\" ] = int ( clean_line [ 7 ]) temp [ \"9\" ] = int ( clean_line [ 8 ]) X_test . append ( temp ) y_test . append ( clean_line [ 9 ]) if max_examples and len ( X_test ) >= max_examples : break X_train , y_train = _load_onehot ( X_train , y_train ) X_test , y_test = _load_onehot ( X_test , y_test ) return X_train , y_train , X_test , y_test def _load_onehot ( X , y ): vec = DictVectorizer ( sparse = False ) return vec . fit_transform ( X ), np . asarray ( y )","title":"Autogoal.datasets.shuttle"},{"location":"api/autogoal.datasets.shuttle/#examples","text":"```python X_train, y_train, X_valid, y_valid = load() X_train.shape, X_valid.shape ((43500, 9), (14500, 9)) len(y_train), len(y_valid) (43500, 14500) ``` try : download ( \"shuttle\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise train_data = open ( datapath ( \"shuttle\" ) / \"shuttle.trn\" , \"r\" ) test_data = open ( datapath ( \"shuttle\" ) / \"shuttle.tst\" , \"r\" ) X_train = [] X_test = [] y_train = [] y_test = [] for i in train_data . readlines (): clean_line = i . strip () . split () temp = {} temp [ \"1\" ] = int ( clean_line [ 0 ]) temp [ \"2\" ] = int ( clean_line [ 1 ]) temp [ \"3\" ] = int ( clean_line [ 2 ]) temp [ \"4\" ] = int ( clean_line [ 3 ]) temp [ \"5\" ] = int ( clean_line [ 4 ]) temp [ \"6\" ] = int ( clean_line [ 5 ]) temp [ \"7\" ] = int ( clean_line [ 6 ]) temp [ \"8\" ] = int ( clean_line [ 7 ]) temp [ \"9\" ] = int ( clean_line [ 8 ]) X_train . append ( temp ) y_train . append ( clean_line [ 9 ]) if max_examples and len ( X_train ) >= max_examples : break for i in test_data . readlines (): clean_line = i . strip () . split () temp = {} temp [ \"1\" ] = int ( clean_line [ 0 ]) temp [ \"2\" ] = int ( clean_line [ 1 ]) temp [ \"3\" ] = int ( clean_line [ 2 ]) temp [ \"4\" ] = int ( clean_line [ 3 ]) temp [ \"5\" ] = int ( clean_line [ 4 ]) temp [ \"6\" ] = int ( clean_line [ 5 ]) temp [ \"7\" ] = int ( clean_line [ 6 ]) temp [ \"8\" ] = int ( clean_line [ 7 ]) temp [ \"9\" ] = int ( clean_line [ 8 ]) X_test . append ( temp ) y_test . append ( clean_line [ 9 ]) if max_examples and len ( X_test ) >= max_examples : break X_train , y_train = _load_onehot ( X_train , y_train ) X_test , y_test = _load_onehot ( X_test , y_test ) return X_train , y_train , X_test , y_test def _load_onehot ( X , y ): vec = DictVectorizer ( sparse = False ) return vec . fit_transform ( X ), np . asarray ( y )","title":"Examples"},{"location":"api/autogoal.datasets.wine_quality/","text":"import numpy as np import os from autogoal.datasets import download , datapath def load ( white = True , red = True , max_examples = None ): if not red and not white : raise ValueError ( \"Either red or white must be selected\" ) download ( \"wine_quality\" ) f_white = open ( datapath ( \"wine_quality\" ) / \"winequality-white.csv\" , \"r\" ) f_red = open ( datapath ( \"wine_quality\" ) / \"winequality-red.csv\" , \"r\" ) X = [] y = [] if white : title_line = True for i in f_white . readlines (): if max_examples and len ( X ) >= max_examples : break if title_line == True : title_line = False continue clean_line = i . strip () . split ( \";\" ) X . append ([ 1 , 0 ] + [ float ( i ) for i in clean_line [: - 1 ]]) y . append ( float ( clean_line [ - 1 ])) if red : title_line = True for i in f_red . readlines (): if max_examples and len ( X ) >= max_examples : break if title_line == True : title_line = False continue clean_line = i . strip () . split ( \";\" ) X . append ([ 0 , 1 ] + [ float ( i ) for i in clean_line [: - 1 ]]) y . append ( float ( clean_line [ - 1 ])) return np . asarray ( X ), np . asarray ( y )","title":"Autogoal.datasets.wine quality"},{"location":"api/autogoal.datasets.yeast/","text":"import numpy as np import os from autogoal.datasets import datapath , download from sklearn.feature_extraction import DictVectorizer def load (): Note Loads corpora from Yeast uci dataset . Examples \u00b6 ```python X, y = load() X.shape (1484, 8) len(y) 1484 ``` try : download ( \"yeast\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise f = open ( datapath ( \"yeast\" ) / \"yeast.data\" , \"r\" ) X = [] y = [] for i in f : clean_line = i . strip () . split () temp = {} temp [ \"1\" ] = float ( clean_line [ 1 ]) temp [ \"2\" ] = float ( clean_line [ 2 ]) temp [ \"3\" ] = float ( clean_line [ 3 ]) temp [ \"4\" ] = float ( clean_line [ 4 ]) temp [ \"5\" ] = float ( clean_line [ 5 ]) temp [ \"6\" ] = float ( clean_line [ 6 ]) temp [ \"7\" ] = float ( clean_line [ 7 ]) temp [ \"8\" ] = float ( clean_line [ 8 ]) X . append ( temp ) y . append ( clean_line [ 9 ]) return _load_onehot ( X , y ) def _load_onehot ( X , y ): vec = DictVectorizer ( sparse = False ) return vec . fit_transform ( X ), np . asarray ( y )","title":"Autogoal.datasets.yeast"},{"location":"api/autogoal.datasets.yeast/#examples","text":"```python X, y = load() X.shape (1484, 8) len(y) 1484 ``` try : download ( \"yeast\" ) except : print ( \"Error loading data. This may be caused due to bad connection. Please delete badly downloaded data and retry\" ) raise f = open ( datapath ( \"yeast\" ) / \"yeast.data\" , \"r\" ) X = [] y = [] for i in f : clean_line = i . strip () . split () temp = {} temp [ \"1\" ] = float ( clean_line [ 1 ]) temp [ \"2\" ] = float ( clean_line [ 2 ]) temp [ \"3\" ] = float ( clean_line [ 3 ]) temp [ \"4\" ] = float ( clean_line [ 4 ]) temp [ \"5\" ] = float ( clean_line [ 5 ]) temp [ \"6\" ] = float ( clean_line [ 6 ]) temp [ \"7\" ] = float ( clean_line [ 7 ]) temp [ \"8\" ] = float ( clean_line [ 8 ]) X . append ( temp ) y . append ( clean_line [ 9 ]) return _load_onehot ( X , y ) def _load_onehot ( X , y ): vec = DictVectorizer ( sparse = False ) return vec . fit_transform ( X ), np . asarray ( y )","title":"Examples"},{"location":"api/autogoal.exceptions/","text":"class InterfaceIncompatibleError ( ValueError ): def __init__ ( self , cls ) -> None : super () . __init__ ( f \"Cannot find compatible implementations for <class { cls } >\" ) self . cls = cls","title":"Autogoal.exceptions"},{"location":"api/autogoal.experimental.__init__/","text":"import warnings warnings . warn ( \"Using experimental features is not recommended in production. These features are not properly tested.\" )","title":"Autogoal.experimental.  init  "},{"location":"api/autogoal.experimental.stacking/","text":"from autogoal.contrib import ensemble import numpy as np from sklearn.svm import SVC from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier class StackingEnsemble : def __init__ ( self , layers = None , final = None ): if layers == None : self . layers = [[ SVC (), LogisticRegression ()], [ DecisionTreeClassifier ()]] else : self . layers = layers if final == None : self . final = GaussianNB () else : self . final = final self . network = [] def network_constructor ( self ): Note Creates a network containing layers of estimators. network = self . network layers = self . layers final = self . final network . append ( layers ) network . append ( final ) return network def forward_pass ( self , X , y ): Note Do a forward pass of the stacked network network = self . network_constructor () output = y input_current_layer = [] input_next_layer = [] for index , layer in enumerate ( network ): if index == 0 : input_current_layer = X for estimator in layer : estimator . fit ( input_current_layer , output ) input_next_layer . append ( estimator . predict ( input_current_layer )) else : input_current_layer = input_next_layer input_next_layer = [] for estimator in layer : estimator . fit ( input_current_layer , output ) input_next_layer . append ( estimator . predict ( input_current_layer )) return network def fit ( self , X , y ): input_length = len ( X ) target_lenght = len ( y ) if input_length == target_lenght : return self . forward_pass ( X , y ) else : raise ValueError ( \"X and y must have the same length\" ) def predict ( self , X ): Note Do a prediction for a test data network = self . network prediction_current_layer = np . array ([]) input_current_layer = [] for index , layer in enumerate ( network ): if index == 0 : input_current_layer = X for estimator in layer : prediction_current_layer = np . concatenate ( ( prediction_current_layer , estimator . predict ( input_current_layer ), ) ) prediction_current_layer = np . reshape ( prediction_current_layer , ( 1 , 2 )) else : input_current_layer = prediction_current_layer prediction_current_layer = np . array ([]) for estimator in layer : prediction_current_layer = np . concatenate ( ( prediction_current_layer , estimator . predict ( input_current_layer ), ) ) return prediction_current_layer if __name__ == \"__main__\" : X_train = [[ 0 , 0 ], [ 1 , 1 ]] y_train = [ 0 , 1 ] X_test = [[ 2.0 , 2.0 ]] y_test = [ 1 ] ensemble = StackingEnsemble ([ SVC (), DecisionTreeClassifier ()], [ SVC ()]) ensemble . fit ( X_train , y_train ) y_pred = ensemble . predict ( X_test )","title":"Autogoal.experimental.stacking"},{"location":"api/autogoal.grammar.__init__/","text":"from ._base import Grammar , Sampler from ._cfg import ( generate_cfg , ContextFreeGrammar , DiscreteValue , ContinuousValue , CategoricalValue , BooleanValue , Union , Symbol , CfgInitializer , Empty , Symbol , Subset , ) from ._graph import GraphGrammar , Path , Block , Graph , GraphSpace , Epsilon","title":"autogoal.grammar"},{"location":"api/autogoal.grammar._base/","text":"import random from autogoal.sampling import Sampler class Grammar : def __init__ ( self , start ): self . _start = start def sample ( self , * , max_iterations : int = 100 , sampler : Sampler = None ): if sampler is None : sampler = Sampler () return self . _sample ( symbol = self . _start , max_iterations = max_iterations , sampler = sampler ) def __call__ ( self , sampler : Sampler = None ): return self . sample ( sampler = sampler ) def _sample ( self , symbol , max_iterations , sampler ): raise NotImplementedError ()","title":"Autogoal.grammar. base"},{"location":"api/autogoal.grammar._cfg/","text":"import inspect from typing import List , Dict , Set from ._base import Grammar class Symbol : def __init__ ( self , name ): self . name = name def __eq__ ( self , other ): return self . name == other . name def __hash__ ( self ): return hash ( self . name ) def __repr__ ( self ): return \"Symbol(name= %r )\" % ( self . name ) class Production : def __init__ ( self , head : Symbol , grammar : \"ContextFreeGrammar\" ): self . head = head self . grammar = grammar def to_string ( self , code : List [ str ], visited : Set [ Symbol ], max_symbol_length : int , ): raise NotImplementedError () def sample ( self , sampler , namespace , max_iterations ): raise NotImplementedError () class Empty ( Production ): def __repr__ ( self ): return \"Empty()\" def to_string ( self , code : List [ str ], visited : Set [ Symbol ], max_symbol_length : int , ): code . append ( \" %s := %s \" % (( \"< %s >\" % self . head . name ) . ljust ( max_symbol_length ), \"<empty>\" ) ) visited . add ( self . head ) def sample ( self , sampler , namespace , max_iterations ): return None class OneOf ( Production ): def __init__ ( self , head , grammar , * options ): super () . __init__ ( head , grammar ) self . _options : List [ Symbol ] = list ( options ) @property def options ( self ) -> List [ Symbol ]: return self . _options def __repr__ ( self ): return \"OneOf(options= %r )\" % self . _options def to_string ( self , code : List [ str ], visited : Set [ Symbol ], max_symbol_length : int , ): lhs = ( \"< %s >\" % self . head . name ) . ljust ( max_symbol_length ) rhs = [ \"< %s >\" % option . name for option in self . _options ] code . append ( \" %s := %s \" % ( lhs , \" | \" . join ( rhs ))) visited . add ( self . head ) for symbol in self . _options : if symbol in visited : continue self . grammar [ symbol ] . to_string ( code , visited , max_symbol_length ) def sample ( self , sampler , namespace , max_iterations ): if max_iterations <= 0 : raise ValueError ( \"Max iterations exceeded\" ) option = sampler . choice ( self . options , handle = self . head . name ) return self . grammar [ option ] . sample ( sampler , namespace , max_iterations - 1 ) class SubsetOf ( Production ): def __init__ ( self , head , grammar , * options , allow_empty = False ): super () . __init__ ( head , grammar ) self . _options : List [ object ] = list ( options ) self . _allow_empty = allow_empty @property def options ( self ) -> List [ object ]: return self . _options def __repr__ ( self ): return \"SubsetOf(options= %r )\" % self . _options def to_string ( self , code : List [ str ], visited : Set [ Symbol ], max_symbol_length : int , ): lhs = ( \"< %s >\" % self . head . name ) . ljust ( max_symbol_length ) rhs = [ ( \"< %s >\" % option . name if hasattr ( option , \"name\" ) else repr ( option )) for option in self . _options ] code . append ( \" %s := { %s }\" % ( lhs , \" , \" . join ( rhs ))) visited . add ( self . head ) for symbol in self . _options : if symbol in visited : continue if symbol not in self . grammar : continue self . grammar [ symbol ] . to_string ( code , visited , max_symbol_length ) def sample ( self , sampler , namespace , max_iterations ): if max_iterations <= 0 : raise ValueError ( \"Max iterations exceeded\" ) while True : selected = [] for option in self . options : if hasattr ( option , \"name\" ): handle = self . head . name + \"_\" + option . name sample = self . grammar [ option ] . sample ( sampler , namespace , max_iterations - 1 ) else : handle = self . head . name + \"_\" + repr ( option ) sample = option if sampler . boolean ( handle = handle ): selected . append ( sample ) if len ( selected ) > 0 or self . _allow_empty : break return selected class Callable ( Production ): def __init__ ( self , head , grammar , name , ** parameters ): super () . __init__ ( head , grammar ) self . _name = name self . _parameters = parameters def __repr__ ( self ): return \"Callable(name= %r , parameters= %r )\" % ( self . _name , self . _parameters ) def to_string ( self , code : List [ str ], visited : Set [ Symbol ], max_symbol_length : int , ): lhs = ( \"< %s >\" % self . head . name ) . ljust ( max_symbol_length ) rhs = [ \" %s = %s \" % ( key , ( \"< %s >\" % value . name ) if hasattr ( value , \"name\" ) else value ) for key , value in self . _parameters . items () ] code . append ( \" %s := %s ( %s )\" % ( lhs , self . _name , \", \" . join ( rhs ))) visited . add ( self . head ) for _ , symbol in self . _parameters . items (): if not isinstance ( symbol , Symbol ): continue if symbol in visited : continue if symbol not in self . grammar : continue self . grammar [ symbol ] . to_string ( code , visited , max_symbol_length ) def sample ( self , sampler , namespace , max_iterations ): if max_iterations <= 0 : raise ValueError ( \"Max iterations exceeded\" ) kwargs = {} for arg , symbol in self . _parameters . items (): if isinstance ( symbol , Symbol ): arg_value = self . grammar [ symbol ] . sample ( sampler , namespace , max_iterations - 1 ) else : arg_value = symbol kwargs [ arg ] = arg_value obj = namespace [ self . _name ]( ** kwargs ) if hasattr ( obj , \"sample\" ) and callable ( obj . sample ): obj . sample ( sampler , max_iterations = max_iterations ) return obj class Distribution ( Callable ): def __repr__ ( self ): return \"Distribution(name= %r , parameters= %r )\" % ( self . _name , self . _parameters ) def sample ( self , sampler , namespace , max_iterations ): return sampler . distribution ( self . _name , handle = self . head . name , ** self . _parameters ) class ContextFreeGrammar ( Grammar ): Note Represents a CFG grammar. def __init__ ( self , start : Symbol , namespace : Dict [ str , type ] = None ): super ( ContextFreeGrammar , self ) . __init__ ( start ) self . _namespace = {} if namespace is None else namespace self . _productions : Dict [ Symbol , Production ] = {} @property def namespace ( self ): return self . _namespace def add ( self , symbol : Symbol , production : Production ) -> None : if symbol in self : raise ValueError ( \"Cannot add more than once, call Grammar.replace() instead.\" ) self . _productions [ symbol ] = production def replace ( self , symbol : Symbol , production : Production ) -> None : if symbol not in self : raise ValueError ( \"Symbol is not defined, call Grammar.add() instead.\" ) self . _productions [ symbol ] = production def __contains__ ( self , symbol : Symbol ): return symbol in self . _productions def __getitem__ ( self , symbol : Symbol ) -> Production : return self . _productions [ symbol ] def __repr__ ( self ): return \"Grammar(start= %r , productions= %r )\" % ( self . _start , self . _productions ,) def __str__ ( self ): code = [] max_symbol_length = max ( len ( symbol . name ) for symbol in self . _productions ) + 2 self [ self . _start ] . to_string ( code , set (), max_symbol_length ) return \" \\n \" . join ( code ) def _sample ( self , symbol , max_iterations , sampler ): production = self [ symbol ] return production . sample ( sampler , self . _namespace , max_iterations ) def generate_cfg ( cls , registry = None ) -> ContextFreeGrammar : Note Generates a ContextFreeGrammar from an annotated callable (class or function). Parameters \u00b6 cls : class or function with annotated arguments. Returns \u00b6 ContextFreeGrammar : the generated grammar. Examples \u00b6 ```python class MyClass: ... def init (self, x: DiscreteValue(1,3), y: ContinuousValue(0,1)): ... pass grammar = generate_cfg(MyClass) print(grammar) := MyClass (x= , y= ) := discrete (min=1, max=3) := continuous (min=0, max=1) ``` return _generate_cfg ( cls , registry = registry ) def _generate_cfg ( cls , grammar : ContextFreeGrammar = None , head : Symbol = None , registry = None ) -> ContextFreeGrammar : symbol = head or Symbol ( cls . __name__ ) if grammar is None : grammar = ContextFreeGrammar ( start = symbol ) pre-register all classes that are already given if registry : for clss in registry : grammar . namespace [ clss . __name__ ] = clss elif symbol in grammar : return grammar grammar . namespace [ symbol . name ] = cls if hasattr ( cls , \"generate_cfg\" ): return cls . generate_cfg ( grammar , symbol ) grammar . add ( symbol , Empty ( symbol , grammar )) parameters = {} if inspect . isclass ( cls ): signature = inspect . signature ( cls . __init__ ) elif inspect . isfunction ( cls ): signature = inspect . signature ( cls ) else : raise ValueError ( \"Unable to obtain signature for %r \" % cls ) for param_name , param_obj in signature . parameters . items (): if param_name in [ \"self\" , \"args\" , \"kwargs\" ]: continue annotation_cls = param_obj . annotation if annotation_cls == inspect . Parameter . empty : if param_obj . default == inspect . Parameter . empty : raise TypeError ( \"In < %s >: Couldn't find annotation type for %r \" % ( cls . __name__ , param_obj ) ) continue if annotation_cls == \"self\" : annotation_cls = cls if isinstance ( annotation_cls , str ): try : annotation_cls = grammar . namespace [ annotation_cls ] except KeyError : raise ValueError ( \"To use strings for annotations, make sure recursion hits the corresponding class first.\" ) if hasattr ( annotation_cls , \"__name__\" ): param_symbol = Symbol ( annotation_cls . __name__ ) else : param_symbol = Symbol ( \" %s _ %s \" % ( cls . __name__ , param_name )) _generate_cfg ( annotation_cls , grammar , param_symbol ) parameters [ param_name ] = param_symbol grammar . replace ( symbol , Callable ( symbol , grammar , cls . __name__ , ** parameters )) return grammar class DiscreteValue : def __init__ ( self , min , max ): self . min = min self . max = max def __repr__ ( self ): return \"Discrete(min= %r , max= %r )\" % ( self . min , self . max ) def generate_cfg ( self , grammar , head ): grammar . add ( head , Distribution ( head , grammar , \"discrete\" , min = self . min , max = self . max ) ) return grammar class ContinuousValue ( DiscreteValue ): def __repr__ ( self ): return \"Continuous(min= %r , max= %r )\" % ( self . min , self . max ) def generate_cfg ( self , grammar , head ): grammar . add ( head , Distribution ( head , grammar , \"continuous\" , min = self . min , max = self . max ) ) return grammar class CategoricalValue : def __init__ ( self , * options ): self . options = list ( options ) def __repr__ ( self ): options = \", \" . join ( repr ( o ) for o in self . options ) return f \"Categorical( { options } )\" def generate_cfg ( self , grammar , head ): grammar . add ( head , Distribution ( head , grammar , \"categorical\" , options = self . options ) ) return grammar class BooleanValue : def __repr__ ( self ): return f \"Boolean()\" @staticmethod def generate_cfg ( grammar , head ): grammar . add ( head , Distribution ( head , grammar , \"boolean\" )) return grammar class Union : def __init__ ( self , name , * clss ): self . __name__ = name self . clss = list ( clss ) def __repr__ ( self ): args = \", \".join(str(s) for s in self.clss) return self . __name__ def generate_cfg ( self , grammar , head ): symbol = head or Symbol ( self . __name__ ) if symbol in grammar : return grammar grammar . add ( symbol , Empty ( symbol , grammar )) children = [] for child in self . clss : child_symbol = Symbol ( child . __name__ ) children . append ( child_symbol ) _generate_cfg ( child , grammar , child_symbol ) grammar . replace ( symbol , OneOf ( symbol , grammar , * children )) return grammar class Subset : def __init__ ( self , name , * members , allow_empty = False ): self . __name__ = name self . members = list ( members ) self . allow_empty = allow_empty def __repr__ ( self ): args = \", \".join(str(s) for s in self.clss) return self . __name__ def generate_cfg ( self , grammar , head ): symbol = head or Symbol ( self . __name__ ) if symbol in grammar : return grammar grammar . add ( symbol , Empty ( symbol , grammar )) children = [] for child in self . members : if type ( child ) is Callable : child_symbol = Symbol ( child . __name__ ) children . append ( child_symbol ) _generate_cfg ( child , grammar , child_symbol ) else : children . append ( child ) grammar . replace ( symbol , SubsetOf ( symbol , grammar , * children , allow_empty = self . allow_empty ) ) return grammar class CfgInitializer : def __init__ ( self , registry = None ): self . _grammars = {} self . _registry = registry def __call__ ( self , cls , sampler = None ): if cls not in self . _grammars : self . _grammars [ cls ] = generate_cfg ( cls , self . _registry ) return self . _grammars [ cls ] . sample ( sampler = sampler )","title":"Autogoal.grammar. cfg"},{"location":"api/autogoal.grammar._cfg/#parameters","text":"cls : class or function with annotated arguments.","title":"Parameters"},{"location":"api/autogoal.grammar._cfg/#returns","text":"ContextFreeGrammar : the generated grammar.","title":"Returns"},{"location":"api/autogoal.grammar._cfg/#examples","text":"```python class MyClass: ... def init (self, x: DiscreteValue(1,3), y: ContinuousValue(0,1)): ... pass grammar = generate_cfg(MyClass) print(grammar) := MyClass (x= , y= ) := discrete (min=1, max=3) := continuous (min=0, max=1) ``` return _generate_cfg ( cls , registry = registry ) def _generate_cfg ( cls , grammar : ContextFreeGrammar = None , head : Symbol = None , registry = None ) -> ContextFreeGrammar : symbol = head or Symbol ( cls . __name__ ) if grammar is None : grammar = ContextFreeGrammar ( start = symbol ) pre-register all classes that are already given if registry : for clss in registry : grammar . namespace [ clss . __name__ ] = clss elif symbol in grammar : return grammar grammar . namespace [ symbol . name ] = cls if hasattr ( cls , \"generate_cfg\" ): return cls . generate_cfg ( grammar , symbol ) grammar . add ( symbol , Empty ( symbol , grammar )) parameters = {} if inspect . isclass ( cls ): signature = inspect . signature ( cls . __init__ ) elif inspect . isfunction ( cls ): signature = inspect . signature ( cls ) else : raise ValueError ( \"Unable to obtain signature for %r \" % cls ) for param_name , param_obj in signature . parameters . items (): if param_name in [ \"self\" , \"args\" , \"kwargs\" ]: continue annotation_cls = param_obj . annotation if annotation_cls == inspect . Parameter . empty : if param_obj . default == inspect . Parameter . empty : raise TypeError ( \"In < %s >: Couldn't find annotation type for %r \" % ( cls . __name__ , param_obj ) ) continue if annotation_cls == \"self\" : annotation_cls = cls if isinstance ( annotation_cls , str ): try : annotation_cls = grammar . namespace [ annotation_cls ] except KeyError : raise ValueError ( \"To use strings for annotations, make sure recursion hits the corresponding class first.\" ) if hasattr ( annotation_cls , \"__name__\" ): param_symbol = Symbol ( annotation_cls . __name__ ) else : param_symbol = Symbol ( \" %s _ %s \" % ( cls . __name__ , param_name )) _generate_cfg ( annotation_cls , grammar , param_symbol ) parameters [ param_name ] = param_symbol grammar . replace ( symbol , Callable ( symbol , grammar , cls . __name__ , ** parameters )) return grammar class DiscreteValue : def __init__ ( self , min , max ): self . min = min self . max = max def __repr__ ( self ): return \"Discrete(min= %r , max= %r )\" % ( self . min , self . max ) def generate_cfg ( self , grammar , head ): grammar . add ( head , Distribution ( head , grammar , \"discrete\" , min = self . min , max = self . max ) ) return grammar class ContinuousValue ( DiscreteValue ): def __repr__ ( self ): return \"Continuous(min= %r , max= %r )\" % ( self . min , self . max ) def generate_cfg ( self , grammar , head ): grammar . add ( head , Distribution ( head , grammar , \"continuous\" , min = self . min , max = self . max ) ) return grammar class CategoricalValue : def __init__ ( self , * options ): self . options = list ( options ) def __repr__ ( self ): options = \", \" . join ( repr ( o ) for o in self . options ) return f \"Categorical( { options } )\" def generate_cfg ( self , grammar , head ): grammar . add ( head , Distribution ( head , grammar , \"categorical\" , options = self . options ) ) return grammar class BooleanValue : def __repr__ ( self ): return f \"Boolean()\" @staticmethod def generate_cfg ( grammar , head ): grammar . add ( head , Distribution ( head , grammar , \"boolean\" )) return grammar class Union : def __init__ ( self , name , * clss ): self . __name__ = name self . clss = list ( clss ) def __repr__ ( self ): args = \", \".join(str(s) for s in self.clss) return self . __name__ def generate_cfg ( self , grammar , head ): symbol = head or Symbol ( self . __name__ ) if symbol in grammar : return grammar grammar . add ( symbol , Empty ( symbol , grammar )) children = [] for child in self . clss : child_symbol = Symbol ( child . __name__ ) children . append ( child_symbol ) _generate_cfg ( child , grammar , child_symbol ) grammar . replace ( symbol , OneOf ( symbol , grammar , * children )) return grammar class Subset : def __init__ ( self , name , * members , allow_empty = False ): self . __name__ = name self . members = list ( members ) self . allow_empty = allow_empty def __repr__ ( self ): args = \", \".join(str(s) for s in self.clss) return self . __name__ def generate_cfg ( self , grammar , head ): symbol = head or Symbol ( self . __name__ ) if symbol in grammar : return grammar grammar . add ( symbol , Empty ( symbol , grammar )) children = [] for child in self . members : if type ( child ) is Callable : child_symbol = Symbol ( child . __name__ ) children . append ( child_symbol ) _generate_cfg ( child , grammar , child_symbol ) else : children . append ( child ) grammar . replace ( symbol , SubsetOf ( symbol , grammar , * children , allow_empty = self . allow_empty ) ) return grammar class CfgInitializer : def __init__ ( self , registry = None ): self . _grammars = {} self . _registry = registry def __call__ ( self , cls , sampler = None ): if cls not in self . _grammars : self . _grammars [ cls ] = generate_cfg ( cls , self . _registry ) return self . _grammars [ cls ] . sample ( sampler = sampler )","title":"Examples"},{"location":"api/autogoal.grammar._functional/","text":"from autogoal.grammar._base import Grammar class FunctionalGrammar ( Grammar ): def _sample ( self , symbol , max_iterations , sampler ): pass","title":"Autogoal.grammar. functional"},{"location":"api/autogoal.grammar._graph/","text":"import networkx as nx import random import types from typing import List , Dict from autogoal.utils import nice_repr from ._base import Grammar , Sampler class Graph ( nx . DiGraph ): Note Represents a graph in AutoGOAL that can be expanded with graph grammars. def __init__ ( self , ** attrs ): super ( Graph , self ) . __init__ ( ** attrs ) def build_order ( self ): Note Returns a iterable of (node, in_nodes) in topological order. In detail, returns every node and the list of nodes that are incoming edges to node . This way, you can call a method on each node whose args are the previous nodes. for node in nx . topological_sort ( self ): in_nodes = [ u for u , v in self . in_edges ( node )] yield ( node , in_nodes ) def apply ( self , function ): Note Applies a function to all nodes in build_order 's order. The function receives the current three arguments: - The current node instance - A list of incoming node instances - A list of the result of the application of the function in the previous node instances. Returns the last of the values computed. previous_values = {} final_values = [] for node , in_nodes in self . build_order (): in_values = [ previous_values [ n ] for n in in_nodes ] value = function ( node , in_nodes , in_values ) if not list ( self . neighbors ( node )): final_values . append ( value ) previous_values [ node ] = value return final_values def contains_any ( self , * items ): return any (( node in items ) for node in self ) def uniform_selection ( items ): return random . choice ( items ) def first_selection ( items ): return items [ 0 ] def default_initializer ( cls , sampler = None ): return cls () Holds a string to class map for automatically generated classes _GENERATED_CLASSES : Dict [ str , type ] = {} def _get_generated_class ( name ): if name in _GENERATED_CLASSES : return _GENERATED_CLASSES [ name ] clss = types . new_class ( name ) _GENERATED_CLASSES [ name ] = clss return clss @nice_repr class Production : def __init__ ( self , production_id , pattern , replacement , * , initializer = None ): if not isinstance ( pattern , Graph ): obj = pattern pattern = Graph () pattern . add_node ( obj ) if not isinstance ( replacement , GraphPattern ): replacement = Node ( replacement ) self . production_id = production_id self . pattern = pattern self . replacement = replacement self . _initializer = initializer or default_initializer def _matches ( self , graph : Graph ): TODO: Generalizar a permitir cualquier tipo de grafo como patr\u00f3n, no solo un nodo pattern_node = list ( self . pattern . nodes )[ 0 ] for node in graph . nodes : if node . __class__ == pattern_node : yield node def match ( self , graph : Graph ): Note Returns True if it finds a subgraph in graph that matches the pattern. NOTE: Right now only works if pattern is a single node. for _ in self . _matches ( graph ): return True return False def apply ( self , graph : Graph , pattern_selection = uniform_selection ) -> Graph : Note Applies a production in a graph and returns the modified graph. matches = list ( self . _matches ( graph )) node = pattern_selection ( matches ) in_edges = graph . in_edges ( node ) out_edges = graph . out_edges ( node ) in_nodes = [ u for u , v in in_edges ] out_nodes = [ v for u , v in out_edges ] graph . remove_node ( node ) self . replacement . build ( graph , in_nodes = in_nodes , out_nodes = out_nodes , initializer = self . _initializer ) return graph class GraphPattern : def build ( self , graph : Graph , * , in_nodes = [], out_nodes = [], initializer = default_initializer , ): raise NotImplementedError () def _add_in_nodes ( self , graph , in_nodes , node ): for in_node in in_nodes : graph . add_edge ( in_node , node ) def _add_out_nodes ( self , graph , out_nodes , node ): for out_node in out_nodes : graph . add_edge ( node , out_node ) def make ( self , * , initializer = default_initializer ) -> Graph : graph = Graph () self . build ( graph , in_nodes = [], out_nodes = [], initializer = initializer ) return graph class Node ( GraphPattern ): def __init__ ( self , cls ): self . cls = _get_generated_class ( cls ) if isinstance ( cls , str ) else cls def build ( self , graph : Graph , * , in_nodes = [], out_nodes = [], initializer = default_initializer , ): obj = initializer ( self . cls ) graph . add_node ( obj ) self . _add_in_nodes ( graph , in_nodes , obj ) self . _add_out_nodes ( graph , out_nodes , obj ) class Path ( GraphPattern ): def __init__ ( self , * items ): self . items = [ ( _get_generated_class ( i ) if isinstance ( i , str ) else i ) for i in items ] def build ( self , graph : Graph , * , in_nodes = [], out_nodes = [], initializer = default_initializer , ): items = [ initializer ( cls ) for cls in self . items ] graph . add_nodes_from ( items ) for i , j in zip ( items , items [ 1 :]): graph . add_edge ( i , j ) self . _add_in_nodes ( graph , in_nodes , items [ 0 ]) self . _add_out_nodes ( graph , out_nodes , items [ - 1 ]) class Block ( GraphPattern ): def __init__ ( self , * items ): self . items = [ ( _get_generated_class ( i ) if isinstance ( i , str ) else i ) for i in items ] def build ( self , graph : Graph , * , in_nodes = [], out_nodes = [], initializer = default_initializer , ): items = [ initializer ( cls ) for cls in self . items ] graph . add_nodes_from ( items ) for item in items : self . _add_in_nodes ( graph , in_nodes , item ) self . _add_out_nodes ( graph , out_nodes , item ) class Epsilon ( GraphPattern ): def build ( self , graph : Graph , * , in_nodes = [], out_nodes = [], initializer = default_initializer , ): for u in in_nodes : for v in out_nodes : graph . add_edge ( u , v ) class GraphGrammar ( Grammar ): def __init__ ( self , start , * , initializer = None , non_terminals = None ): if isinstance ( start , str ): start = Node ( start ) if isinstance ( start , GraphPattern ): start = start . make () super ( GraphGrammar , self ) . __init__ ( start ) self . _productions : List [ Production ] = [] self . _non_terminals = set ( non_terminals or []) self . _initializer = initializer or default_initializer def add ( self , pattern , replacement : GraphPattern , * , initializer = None , kwargs = None ): if initializer is None : initializer = self . _initializer if kwargs is not None : initializer = lambda cls : cls ( ** kwargs ) if isinstance ( pattern , str ): pattern = _get_generated_class ( pattern ) self . _non_terminals . add ( pattern ) self . _productions . append ( Production ( \"production_ %i \" % len ( self . _productions ), pattern , replacement , initializer = initializer , ) ) def _sample ( self , symbol : Graph , max_iterations : int , sampler : Sampler ): if symbol is None : raise ValueError ( \"`symbol` cannot be `None`\" ) symbol = symbol . copy () while max_iterations > 0 : valid_productions = [ p for p in self . _productions if p . match ( symbol )] if self . _non_terminals : non_terminal_productions = [ p for p in valid_productions if p . pattern . contains_any ( * self . _non_terminals ) ] if non_terminal_productions : valid_productions = non_terminal_productions if not valid_productions : return symbol production_ids = { p . production_id : p for p in valid_productions } production = production_ids [ sampler . choice ( list ( production_ids ))] symbol = production . apply ( symbol ) max_iterations -= 1 return symbol def __repr__ ( self ): return repr ( self . _productions ) @nice_repr class Start : __slots__ = () __name__ = \"Start\" def __eq__ ( self , other ): return isinstance ( other , Start ) def __hash__ ( self ): return 0 @nice_repr class End : __slots__ = () __name__ = \"End\" def __eq__ ( self , other ): return isinstance ( other , End ) def __hash__ ( self ): return 0 class GraphSpace ( Grammar ): Start = Start () End = End () def __init__ ( self , graph : Graph , initializer = None ): self . graph = graph self . initializer = initializer or default_initializer @property def _start ( self ): return [ GraphSpace . Start ] def _sample ( self , symbol , max_iterations , sampler ): path = symbol visited = set () while max_iterations > 0 : last_node = path [ - 1 ] if last_node == GraphSpace . End : break next_nodes = list(set(self.graph.neighbors(last_node)) - visited) next_nodes = list ( self . graph . neighbors ( last_node )) if not next_nodes : raise ValueError ( \"Cannot continue sampling. Graph is disconnected.\" ) names_values = { x . __name__ : x for x in next_nodes } next_node = names_values [ sampler . choice ( list ( names_values ))] path . append ( next_node ) visited . add ( next_node ) else : raise ValueError ( \"Reached maximum iterations\" ) return [ self . initializer ( node , sampler = sampler ) for node in path [ 1 : - 1 ]]","title":"Autogoal.grammar. graph"},{"location":"api/autogoal.kb.__init__/","text":"from autogoal.kb._algorithm import * from autogoal.kb._semantics import * from autogoal.kb._data import Distinct","title":"Autogoal.kb.  init  "},{"location":"api/autogoal.kb._algorithm/","text":"from collections import namedtuple import inspect import abc import types import warnings from typing import Any , Dict , List , Set , Tuple , Type import types import networkx as nx from autogoal.utils import nice_repr from autogoal.grammar import Graph , GraphSpace , generate_cfg from autogoal.kb._semantics import SemanticType , Seq class Supervised ( SemanticType ): Note \"\"\"Represents a supervised version of some type X. It is considered a subclass of X for semantic purposes, but not the other way around: >>> issubclass(Supervised[Vector], Vector) \u00b6 True \u00b6 >>> issubclass(Vector, Supervised[Vector]) \u00b6 False \u00b6 >>> issubclass(Supervised[Seq[Vector]], Seq[Vector]) \u00b6 True \u00b6 >>> issubclass(Seq[Vector], Supervised[Seq[Vector]]) \u00b6 False \u00b6 __internal_types = {} @classmethod def _specialize ( cls , internal_type ): try : return cls . __internal_types [ internal_type ] except KeyError : pass class SupervisedImp ( Supervised ): __internal = internal_type @classmethod def _name ( cls ): return f \"Supervised[ { cls . __internal } ]\" @classmethod def _reduce ( cls ): return Supervised . _specialize , ( internal_type ,) cls . __internal_types [ internal_type ] = SupervisedImp return SupervisedImp def algorithm ( * annotations ): from autogoal.grammar import Union , Symbol * inputs , output = annotations def match ( cls ): if not hasattr ( cls , \"run\" ): return False signature = inspect . signature ( cls . run ) input_types = [ v . annotation for k , v in signature . parameters . items () if k != \"self\" ] output_type = signature . return_annotation if len ( inputs ) != len ( input_types ): return False for expected , real in zip ( inputs , input_types ): if not issubclass ( expected , real ): return False if not issubclass ( output_type , output ): return False return True @classmethod def is_compatible ( cls , other ): return match ( other ) @classmethod def generate_cfg ( cls , grammar , head ): symbol = head or Symbol ( cls . __name__ ) compatible = [] for _ , other_cls in grammar . namespace . items (): if cls . is_compatible ( other_cls ): compatible . append ( other_cls ) if not compatible : raise ValueError ( f \"Cannot find any suitable implementation of algorithms with inputs: { inputs } and output: { output } \" ) return Union ( symbol . name , * compatible ) . generate_cfg ( grammar , symbol ) def build ( ns ): ns [ \"generate_cfg\" ] = generate_cfg ns [ \"is_compatible\" ] = is_compatible return types . new_class ( f \"Algorithm[ { inputs } , { output } ]\" , bases = (), exec_body = build ) class Algorithm ( abc . ABC ): Note \"\"\"Represents an abstract algorithm with a run method. Provides introspection for the expected semantic input and output types. Users should inherit from AlgorithmBase instead of this class. @abc . abstractclassmethod def input_types ( cls ) -> Tuple [ type ]: Note \"\"\"Returns an ordered list of the expected semantic input types of the run method. pass @abc . abstractclassmethod def input_args ( cls ) -> Tuple [ str ]: Note \"\"\"Returns an ordered tuple of the names of the arguments in the run method. pass @abc . abstractclassmethod def output_type ( cls ) -> type : Note \"\"\"Returns an ordered list of the expected semantic output type of the run method. pass @abc . abstractmethod def run ( self , * args ): Note \"\"\"Executes the algorithm. pass @classmethod def is_compatible_with ( cls : \"Algorithm\" , input_types ): Note Determines if the current algorithm is compatible with a set of input types, i.e., if among those types we can find all the necessary inputs for this algorithm. class A(AlgorithmBase): ... def run(self, x:int) -> float: ... pass A.is_compatible_with([int]) True my_inputs = cls . input_types () for needed in my_inputs : for having in input_types : if issubclass ( having , needed ): break else : return False return True class AlgorithmBase ( Algorithm ): Note \"\"\"Represents an algorithm, Automatically implements the input and output introspection methods using the inspect module. Users inheriting from this class must provide type annotations in the run method. @classmethod def input_types ( cls ) -> Tuple [ type ]: if not hasattr(cls, \" run_signature \"): cls. run_signature = inspect.signature(cls.run) return tuple ( param . annotation for name , param in inspect . signature ( cls . run ) . parameters . items () if name != \"self\" ) @classmethod def input_args ( cls ) -> Tuple [ str ]: if not hasattr(cls, \" run_signature \"): cls. run_signature = inspect.signature(cls.run) return tuple ( name for name in inspect . signature ( cls . run ) . parameters if name != \"self\" ) @classmethod def output_type ( cls ) -> type : if not hasattr(cls, \" run_signature \"): cls. run_signature = inspect.signature(cls.run) return inspect . signature ( cls . run ) . return_annotation def build_input_args ( algorithm : Algorithm , values : Dict [ type , Any ]): Note \"\"\"Buils the correct input mapping for algorithm using the provided values mapping types to objects. The input can be a class that inherits from Algorithm or an instance of such a class. class A(AlgorithmBase): ... def run(self, a:int, b:str): ... pass values = { str:\"hello\", float:3.0, int:42 } build_input_args(A, values) {'a': 42, 'b': 'hello'} build_input_args(A(), values) {'a': 42, 'b': 'hello'} result = {} for name , type in zip ( algorithm . input_args (), algorithm . input_types ()): try : result [ name ] = values [ type ] except KeyError : for key in values : if issubclass ( key , type ): result [ name ] = values [ key ] break else : raise TypeError ( f \"Cannot find compatible input value for { type } \" ) return result @nice_repr class Pipeline : Note \"\"\"Represents a sequence of algorithms. Each algorithm must have a run method declaring it's input and output type. The pipeline instance also receives the input and output types. def __init__ ( self , algorithms : List [ Algorithm ], input_types : List [ Type [ SemanticType ]] ) -> None : self . algorithms = algorithms self . input_types = input_types def run ( self , * inputs ): data = {} for i , t in zip ( inputs , self . input_types ): data [ t ] = i for algorithm in self . algorithms : args = build_input_args ( algorithm , data ) output = algorithm . run ( ** args ) output_type = algorithm . output_type () data [ output_type ] = output return data [ self . algorithms [ - 1 ] . output_type ()] def send ( self , msg : str , * args , ** kwargs ): found = False for step in self . algorithms : if hasattr ( step , msg ): getattr ( step , msg )( * args , ** kwargs ) found = True elif hasattr ( step , \"send\" ): step . send ( msg , * args , ** kwargs ) found = True if not found : warnings . warn ( f \"No step answered message { msg } .\" ) def make_seq_algorithm ( algorithm : Algorithm ): Note \"\"\"Lift an algorithm with input types T1, T2, Tn to a meta-algorithm with types Seq[T1], Seq[T2], ... The generated class correctly defines the input and output types. These implementations are compatible with build_input_args : class A(AlgorithmBase): ... def init (self, alpha): ... self.alpha = 0.5 ... def run(self, x:int, y:str) -> float: ... return self.alpha * (x + len(y)) ... def repr (self): ... return f\"A({self.alpha})\" B = make_seq_algorithm(A) b = B(0.5) b SeqAlgorithm[A(0.5)] b.run([1, 2], y=[\"A\", \"BC\"]) [1.0, 2.0] B.input_types() (Seq[ ], Seq[ ]) B.input_args() ('x', 'y') b.output_type() Seq[ ] build_input_args(B, {Seq[int]: [1, 2], Seq[str]: [\"hello\", \"world\"]}) {'x': [1, 2], 'y': ['hello', 'world']} output_type = algorithm . output_type () name = f \"SeqAlgorithm[ { algorithm . __name__ } ]\" def init_method ( self , * args , ** kwargs ): self . inner = algorithm ( * args , ** kwargs ) def run_method ( self , * args , ** kwargs ) -> Seq [ output_type ]: args_kwargs = _make_list_args_and_kwargs ( * args , ** kwargs ) return [ self . inner . run ( * t . args , ** t . kwargs ) for t in args_kwargs ] def repr_method ( self ): return f \"SeqAlgorithm[ { repr ( self . inner ) } ]\" def getattr_method ( self , attr ): return getattr ( self . inner , attr ) @classmethod def input_types_method ( cls ): return tuple ( Seq [ t ] for t in algorithm . input_types ()) @classmethod def input_args_method ( cls ): return algorithm . input_args () @classmethod def output_types_method ( cls ): return Seq [ algorithm . output_type ()] def body ( ns ): ns [ \"__init__\" ] = init_method ns [ \"run\" ] = run_method ns [ \"__repr__\" ] = repr_method ns [ \"__getattr__\" ] = getattr_method ns [ \"input_types\" ] = input_types_method ns [ \"input_args\" ] = input_args_method ns [ \"output_type\" ] = output_types_method return types . new_class ( name = name , bases = ( Algorithm ,), exec_body = body ) Akw = namedtuple ( \"Akw\" , [ \"args\" , \"kwargs\" ]) def _make_list_args_and_kwargs ( * args , ** kwargs ): Note \"\"\"Transforms a list of args into individual args and kwargs for an internal algorithm. To be used by `make_seq_algorithm\" _make_list_args_and_kwargs([1,2], [4,5]) [Akw(args=(1, 4), kwargs={}), Akw(args=(2, 5), kwargs={})] _make_list_args_and_kwargs(x=[1,2], y=[4,5]) [Akw(args=(), kwargs={'x': 1, 'y': 4}), Akw(args=(), kwargs={'x': 2, 'y': 5})] _make_list_args_and_kwargs([1,2], y=[4,5]) [Akw(args=(1,), kwargs={'y': 4}), Akw(args=(2,), kwargs={'y': 5})] lengths = set ( len ( v ) for v in kwargs . values ()) | set ( len ( v ) for v in args ) if len ( lengths ) != 1 : raise ValueError ( \"All args and kwargs must be sequences of the same length.\" ) length = lengths . pop () inner_args = [] for i in range ( length ): inner_args . append ( tuple ([ xs [ i ] for xs in args ])) inner_kwargs = [] for i in range ( length ): inner_kwargs . append ({ k : v [ i ] for k , v in kwargs . items ()}) return [ Akw ( xs , ks ) for xs , ks in zip ( inner_args , inner_kwargs )] class PipelineNode : def __init__ ( self , algorithm , input_types , output_types , registry = None ) -> None : self . algorithm = algorithm self . input_types = set ( input_types ) self . output_types = set ( output_types ) self . grammar = generate_cfg ( self . algorithm , registry = registry ) def sample ( self , sampler ): return self . grammar . sample ( sampler = sampler ) @property def __name__ ( self ): return self . algorithm . __name__ def __eq__ ( self , o : object ) -> bool : return isinstance ( o , PipelineNode ) and all ( [ o . algorithm == self . algorithm , o . input_types == self . input_types ,] ) def __repr__ ( self ) -> str : return f \"<PipelineNode(algorithm= { self . algorithm . __name__ } ,input_types= { [ i . __name__ for i in self . input_types ] } ,output_types= { [ o . __name__ for o in self . output_types ] } )>\" def __hash__ ( self ) -> int : return hash ( repr ( self )) class PipelineSpace ( GraphSpace ): def __init__ ( self , graph : Graph , input_types ): super () . __init__ ( graph , initializer = self . _initialize ) self . input_types = input_types def _initialize ( self , item : PipelineNode , sampler ): return item . sample ( sampler ) def nodes ( self ) -> Set [ Type [ Algorithm ]]: Note \"\"\"Returns a list of all algorithms (types) that exist in the graph. return set ( node . algorithm for node in self . graph . nodes if isinstance ( node , PipelineNode ) ) def sample ( self , * args , ** kwargs ): path = super () . sample ( * args , ** kwargs ) return Pipeline ( path , input_types = self . input_types ) def build_pipeline_graph ( input_types : List [ type ], output_type : type , registry : List [ type ], max_list_depth : int = 3 , ) -> PipelineSpace : Note \"\"\"Build a graph of algorithms. Every node in the graph corresponds to a that generates an instance of a class with a run method. Each run method must declare input and output types in the form: def run(self, a: type_1, b: type_2, ...) -> type_n: ... \u00b6 if not isinstance ( input_types , ( list , tuple )): input_types = [ input_types ] We start by enlarging the registry with all Seq[...] algorithms pool = set ( registry ) for algorithm in registry : for _ in range ( max_list_depth ): algorithm = make_seq_algorithm ( algorithm ) pool . add ( algorithm ) For building the graph, we'll keep at each node the guaranteed output types We start by collecting all the possible input nodes, those that can process a subset of the input_types open_nodes : List [ PipelineNode ] = [] for algorithm in pool : if not algorithm . is_compatible_with ( input_types ): continue open_nodes . append ( PipelineNode ( algorithm = algorithm , input_types = input_types , output_types = set ( input_types ) | set ([ algorithm . output_type ()]), registry = registry , ) ) G = Graph () for node in open_nodes : G . add_edge ( GraphSpace . Start , node ) We'll make a BFS exploration of the pipeline space. For every open node we will add to the graph every node to which it can connect. closed_nodes = set () while open_nodes : node = open_nodes . pop ( 0 ) These are the types that are available at this node guaranteed_types = node . output_types Here are all the algorithms that could be added new at this point in the graph for algorithm in registry : if not algorithm . is_compatible_with ( guaranteed_types ): continue We never want to apply the same exact algorithm twice if algorithm == node . algorithm : continue And we never want an algorithm that doesn't provide a novel output type... if ( algorithm . output_type () in guaranteed_types and ... unless it is an idempotent algorithm [ algorithm . output_type ()] != algorithm . input_types () ): continue p = PipelineNode ( algorithm = algorithm , input_types = guaranteed_types , output_types = guaranteed_types | set ([ algorithm . output_type ()]), registry = registry , ) G . add_edge ( node , p ) if p not in closed_nodes and p not in open_nodes : open_nodes . append ( p ) Now we check to see if this node is a possible output if issubclass ( node . algorithm . output_type (), output_type ): G . add_edge ( node , GraphSpace . End ) closed_nodes . add ( node ) Remove all nodes that are not connected to the end node try : reachable_from_end = set ( nx . dfs_preorder_nodes ( G . reverse ( False ), GraphSpace . End ) ) unreachable_nodes = set ( G . nodes ) - reachable_from_end G . remove_nodes_from ( unreachable_nodes ) except KeyError : raise TypeError ( \"No pipelines can be found!\" ) return PipelineSpace ( G , input_types = input_types ) __all__ = [ \"AlgorithmBase\" , \"Supervised\" , \"Pipeline\" , \"build_pipeline_graph\" , \"algorithm\" , ] Finally, we run doctest in the module for easy testing of the functional API. if __name__ == \"__main__\" : import doctest doctest . testmod ()","title":"Autogoal.kb. algorithm"},{"location":"api/autogoal.kb._algorithm/#issubclasssupervisedvector-vector","text":"","title":"&gt;&gt;&gt; issubclass(Supervised[Vector], Vector)"},{"location":"api/autogoal.kb._algorithm/#true","text":"","title":"True"},{"location":"api/autogoal.kb._algorithm/#issubclassvector-supervisedvector","text":"","title":"&gt;&gt;&gt; issubclass(Vector, Supervised[Vector])"},{"location":"api/autogoal.kb._algorithm/#false","text":"","title":"False"},{"location":"api/autogoal.kb._algorithm/#issubclasssupervisedseqvector-seqvector","text":"","title":"&gt;&gt;&gt; issubclass(Supervised[Seq[Vector]], Seq[Vector])"},{"location":"api/autogoal.kb._algorithm/#true_1","text":"","title":"True"},{"location":"api/autogoal.kb._algorithm/#issubclassseqvector-supervisedseqvector","text":"","title":"&gt;&gt;&gt; issubclass(Seq[Vector], Supervised[Seq[Vector]])"},{"location":"api/autogoal.kb._algorithm/#false_1","text":"__internal_types = {} @classmethod def _specialize ( cls , internal_type ): try : return cls . __internal_types [ internal_type ] except KeyError : pass class SupervisedImp ( Supervised ): __internal = internal_type @classmethod def _name ( cls ): return f \"Supervised[ { cls . __internal } ]\" @classmethod def _reduce ( cls ): return Supervised . _specialize , ( internal_type ,) cls . __internal_types [ internal_type ] = SupervisedImp return SupervisedImp def algorithm ( * annotations ): from autogoal.grammar import Union , Symbol * inputs , output = annotations def match ( cls ): if not hasattr ( cls , \"run\" ): return False signature = inspect . signature ( cls . run ) input_types = [ v . annotation for k , v in signature . parameters . items () if k != \"self\" ] output_type = signature . return_annotation if len ( inputs ) != len ( input_types ): return False for expected , real in zip ( inputs , input_types ): if not issubclass ( expected , real ): return False if not issubclass ( output_type , output ): return False return True @classmethod def is_compatible ( cls , other ): return match ( other ) @classmethod def generate_cfg ( cls , grammar , head ): symbol = head or Symbol ( cls . __name__ ) compatible = [] for _ , other_cls in grammar . namespace . items (): if cls . is_compatible ( other_cls ): compatible . append ( other_cls ) if not compatible : raise ValueError ( f \"Cannot find any suitable implementation of algorithms with inputs: { inputs } and output: { output } \" ) return Union ( symbol . name , * compatible ) . generate_cfg ( grammar , symbol ) def build ( ns ): ns [ \"generate_cfg\" ] = generate_cfg ns [ \"is_compatible\" ] = is_compatible return types . new_class ( f \"Algorithm[ { inputs } , { output } ]\" , bases = (), exec_body = build ) class Algorithm ( abc . ABC ): Note \"\"\"Represents an abstract algorithm with a run method. Provides introspection for the expected semantic input and output types. Users should inherit from AlgorithmBase instead of this class. @abc . abstractclassmethod def input_types ( cls ) -> Tuple [ type ]: Note \"\"\"Returns an ordered list of the expected semantic input types of the run method. pass @abc . abstractclassmethod def input_args ( cls ) -> Tuple [ str ]: Note \"\"\"Returns an ordered tuple of the names of the arguments in the run method. pass @abc . abstractclassmethod def output_type ( cls ) -> type : Note \"\"\"Returns an ordered list of the expected semantic output type of the run method. pass @abc . abstractmethod def run ( self , * args ): Note \"\"\"Executes the algorithm. pass @classmethod def is_compatible_with ( cls : \"Algorithm\" , input_types ): Note Determines if the current algorithm is compatible with a set of input types, i.e., if among those types we can find all the necessary inputs for this algorithm. class A(AlgorithmBase): ... def run(self, x:int) -> float: ... pass A.is_compatible_with([int]) True my_inputs = cls . input_types () for needed in my_inputs : for having in input_types : if issubclass ( having , needed ): break else : return False return True class AlgorithmBase ( Algorithm ): Note \"\"\"Represents an algorithm, Automatically implements the input and output introspection methods using the inspect module. Users inheriting from this class must provide type annotations in the run method. @classmethod def input_types ( cls ) -> Tuple [ type ]: if not hasattr(cls, \" run_signature \"): cls. run_signature = inspect.signature(cls.run) return tuple ( param . annotation for name , param in inspect . signature ( cls . run ) . parameters . items () if name != \"self\" ) @classmethod def input_args ( cls ) -> Tuple [ str ]: if not hasattr(cls, \" run_signature \"): cls. run_signature = inspect.signature(cls.run) return tuple ( name for name in inspect . signature ( cls . run ) . parameters if name != \"self\" ) @classmethod def output_type ( cls ) -> type : if not hasattr(cls, \" run_signature \"): cls. run_signature = inspect.signature(cls.run) return inspect . signature ( cls . run ) . return_annotation def build_input_args ( algorithm : Algorithm , values : Dict [ type , Any ]): Note \"\"\"Buils the correct input mapping for algorithm using the provided values mapping types to objects. The input can be a class that inherits from Algorithm or an instance of such a class. class A(AlgorithmBase): ... def run(self, a:int, b:str): ... pass values = { str:\"hello\", float:3.0, int:42 } build_input_args(A, values) {'a': 42, 'b': 'hello'} build_input_args(A(), values) {'a': 42, 'b': 'hello'} result = {} for name , type in zip ( algorithm . input_args (), algorithm . input_types ()): try : result [ name ] = values [ type ] except KeyError : for key in values : if issubclass ( key , type ): result [ name ] = values [ key ] break else : raise TypeError ( f \"Cannot find compatible input value for { type } \" ) return result @nice_repr class Pipeline : Note \"\"\"Represents a sequence of algorithms. Each algorithm must have a run method declaring it's input and output type. The pipeline instance also receives the input and output types. def __init__ ( self , algorithms : List [ Algorithm ], input_types : List [ Type [ SemanticType ]] ) -> None : self . algorithms = algorithms self . input_types = input_types def run ( self , * inputs ): data = {} for i , t in zip ( inputs , self . input_types ): data [ t ] = i for algorithm in self . algorithms : args = build_input_args ( algorithm , data ) output = algorithm . run ( ** args ) output_type = algorithm . output_type () data [ output_type ] = output return data [ self . algorithms [ - 1 ] . output_type ()] def send ( self , msg : str , * args , ** kwargs ): found = False for step in self . algorithms : if hasattr ( step , msg ): getattr ( step , msg )( * args , ** kwargs ) found = True elif hasattr ( step , \"send\" ): step . send ( msg , * args , ** kwargs ) found = True if not found : warnings . warn ( f \"No step answered message { msg } .\" ) def make_seq_algorithm ( algorithm : Algorithm ): Note \"\"\"Lift an algorithm with input types T1, T2, Tn to a meta-algorithm with types Seq[T1], Seq[T2], ... The generated class correctly defines the input and output types. These implementations are compatible with build_input_args : class A(AlgorithmBase): ... def init (self, alpha): ... self.alpha = 0.5 ... def run(self, x:int, y:str) -> float: ... return self.alpha * (x + len(y)) ... def repr (self): ... return f\"A({self.alpha})\" B = make_seq_algorithm(A) b = B(0.5) b SeqAlgorithm[A(0.5)] b.run([1, 2], y=[\"A\", \"BC\"]) [1.0, 2.0] B.input_types() (Seq[ ], Seq[ ]) B.input_args() ('x', 'y') b.output_type() Seq[ ] build_input_args(B, {Seq[int]: [1, 2], Seq[str]: [\"hello\", \"world\"]}) {'x': [1, 2], 'y': ['hello', 'world']} output_type = algorithm . output_type () name = f \"SeqAlgorithm[ { algorithm . __name__ } ]\" def init_method ( self , * args , ** kwargs ): self . inner = algorithm ( * args , ** kwargs ) def run_method ( self , * args , ** kwargs ) -> Seq [ output_type ]: args_kwargs = _make_list_args_and_kwargs ( * args , ** kwargs ) return [ self . inner . run ( * t . args , ** t . kwargs ) for t in args_kwargs ] def repr_method ( self ): return f \"SeqAlgorithm[ { repr ( self . inner ) } ]\" def getattr_method ( self , attr ): return getattr ( self . inner , attr ) @classmethod def input_types_method ( cls ): return tuple ( Seq [ t ] for t in algorithm . input_types ()) @classmethod def input_args_method ( cls ): return algorithm . input_args () @classmethod def output_types_method ( cls ): return Seq [ algorithm . output_type ()] def body ( ns ): ns [ \"__init__\" ] = init_method ns [ \"run\" ] = run_method ns [ \"__repr__\" ] = repr_method ns [ \"__getattr__\" ] = getattr_method ns [ \"input_types\" ] = input_types_method ns [ \"input_args\" ] = input_args_method ns [ \"output_type\" ] = output_types_method return types . new_class ( name = name , bases = ( Algorithm ,), exec_body = body ) Akw = namedtuple ( \"Akw\" , [ \"args\" , \"kwargs\" ]) def _make_list_args_and_kwargs ( * args , ** kwargs ): Note \"\"\"Transforms a list of args into individual args and kwargs for an internal algorithm. To be used by `make_seq_algorithm\" _make_list_args_and_kwargs([1,2], [4,5]) [Akw(args=(1, 4), kwargs={}), Akw(args=(2, 5), kwargs={})] _make_list_args_and_kwargs(x=[1,2], y=[4,5]) [Akw(args=(), kwargs={'x': 1, 'y': 4}), Akw(args=(), kwargs={'x': 2, 'y': 5})] _make_list_args_and_kwargs([1,2], y=[4,5]) [Akw(args=(1,), kwargs={'y': 4}), Akw(args=(2,), kwargs={'y': 5})] lengths = set ( len ( v ) for v in kwargs . values ()) | set ( len ( v ) for v in args ) if len ( lengths ) != 1 : raise ValueError ( \"All args and kwargs must be sequences of the same length.\" ) length = lengths . pop () inner_args = [] for i in range ( length ): inner_args . append ( tuple ([ xs [ i ] for xs in args ])) inner_kwargs = [] for i in range ( length ): inner_kwargs . append ({ k : v [ i ] for k , v in kwargs . items ()}) return [ Akw ( xs , ks ) for xs , ks in zip ( inner_args , inner_kwargs )] class PipelineNode : def __init__ ( self , algorithm , input_types , output_types , registry = None ) -> None : self . algorithm = algorithm self . input_types = set ( input_types ) self . output_types = set ( output_types ) self . grammar = generate_cfg ( self . algorithm , registry = registry ) def sample ( self , sampler ): return self . grammar . sample ( sampler = sampler ) @property def __name__ ( self ): return self . algorithm . __name__ def __eq__ ( self , o : object ) -> bool : return isinstance ( o , PipelineNode ) and all ( [ o . algorithm == self . algorithm , o . input_types == self . input_types ,] ) def __repr__ ( self ) -> str : return f \"<PipelineNode(algorithm= { self . algorithm . __name__ } ,input_types= { [ i . __name__ for i in self . input_types ] } ,output_types= { [ o . __name__ for o in self . output_types ] } )>\" def __hash__ ( self ) -> int : return hash ( repr ( self )) class PipelineSpace ( GraphSpace ): def __init__ ( self , graph : Graph , input_types ): super () . __init__ ( graph , initializer = self . _initialize ) self . input_types = input_types def _initialize ( self , item : PipelineNode , sampler ): return item . sample ( sampler ) def nodes ( self ) -> Set [ Type [ Algorithm ]]: Note \"\"\"Returns a list of all algorithms (types) that exist in the graph. return set ( node . algorithm for node in self . graph . nodes if isinstance ( node , PipelineNode ) ) def sample ( self , * args , ** kwargs ): path = super () . sample ( * args , ** kwargs ) return Pipeline ( path , input_types = self . input_types ) def build_pipeline_graph ( input_types : List [ type ], output_type : type , registry : List [ type ], max_list_depth : int = 3 , ) -> PipelineSpace : Note \"\"\"Build a graph of algorithms. Every node in the graph corresponds to a that generates an instance of a class with a run method. Each run method must declare input and output types in the form: def run(self, a: type_1, b: type_2, ...) -> type_n:","title":"False"},{"location":"api/autogoal.kb._algorithm/#_1","text":"if not isinstance ( input_types , ( list , tuple )): input_types = [ input_types ] We start by enlarging the registry with all Seq[...] algorithms pool = set ( registry ) for algorithm in registry : for _ in range ( max_list_depth ): algorithm = make_seq_algorithm ( algorithm ) pool . add ( algorithm ) For building the graph, we'll keep at each node the guaranteed output types We start by collecting all the possible input nodes, those that can process a subset of the input_types open_nodes : List [ PipelineNode ] = [] for algorithm in pool : if not algorithm . is_compatible_with ( input_types ): continue open_nodes . append ( PipelineNode ( algorithm = algorithm , input_types = input_types , output_types = set ( input_types ) | set ([ algorithm . output_type ()]), registry = registry , ) ) G = Graph () for node in open_nodes : G . add_edge ( GraphSpace . Start , node ) We'll make a BFS exploration of the pipeline space. For every open node we will add to the graph every node to which it can connect. closed_nodes = set () while open_nodes : node = open_nodes . pop ( 0 ) These are the types that are available at this node guaranteed_types = node . output_types Here are all the algorithms that could be added new at this point in the graph for algorithm in registry : if not algorithm . is_compatible_with ( guaranteed_types ): continue We never want to apply the same exact algorithm twice if algorithm == node . algorithm : continue And we never want an algorithm that doesn't provide a novel output type... if ( algorithm . output_type () in guaranteed_types and ... unless it is an idempotent algorithm [ algorithm . output_type ()] != algorithm . input_types () ): continue p = PipelineNode ( algorithm = algorithm , input_types = guaranteed_types , output_types = guaranteed_types | set ([ algorithm . output_type ()]), registry = registry , ) G . add_edge ( node , p ) if p not in closed_nodes and p not in open_nodes : open_nodes . append ( p ) Now we check to see if this node is a possible output if issubclass ( node . algorithm . output_type (), output_type ): G . add_edge ( node , GraphSpace . End ) closed_nodes . add ( node ) Remove all nodes that are not connected to the end node try : reachable_from_end = set ( nx . dfs_preorder_nodes ( G . reverse ( False ), GraphSpace . End ) ) unreachable_nodes = set ( G . nodes ) - reachable_from_end G . remove_nodes_from ( unreachable_nodes ) except KeyError : raise TypeError ( \"No pipelines can be found!\" ) return PipelineSpace ( G , input_types = input_types ) __all__ = [ \"AlgorithmBase\" , \"Supervised\" , \"Pipeline\" , \"build_pipeline_graph\" , \"algorithm\" , ] Finally, we run doctest in the module for easy testing of the functional API. if __name__ == \"__main__\" : import doctest doctest . testmod ()","title":"..."},{"location":"api/autogoal.kb._data/","text":"from autogoal.exceptions import InterfaceIncompatibleError import types import inspect import pprint from typing import Mapping from autogoal.grammar import Symbol , Union , Empty , Subset from scipy.sparse.base import spmatrix from numpy import ndarray def algorithm(input_type, output_type): def run_method(self, input: input_type) -> output_type: pass def body ( ns ) : ns [ \" run \" ] = run_method return types . new_class ( name = \" Algorithm[%s, %s] \" % ( input_type , output_type ) , bases = ( Interface , ) , exec_body = body , ) class Interface : @classmethod def is_compatible ( cls , other_cls ): own_methods = _get_annotations ( cls , ignore = [ \"generate_cfg\" , \"is_compatible\" ]) if not inspect . isclass ( other_cls ): return False if issubclass ( other_cls , Interface ): return False type_methods = _get_annotations ( other_cls ) return _compatible_annotations ( own_methods , type_methods ) @classmethod def generate_cfg ( cls , grammar , head ): symbol = head or Symbol ( cls . __name__ ) compatible = [] for _ , other_cls in grammar . namespace . items (): if cls . is_compatible ( other_cls ): compatible . append ( other_cls ) if not compatible : raise InterfaceIncompatibleError ( cls . __name__ ) return Union ( symbol . name , * compatible ) . generate_cfg ( grammar , symbol ) class Distinct : def __init__ ( self , interface : Interface , exceptions = None ): self . interface = interface self . exceptions = exceptions def generate_cfg ( self , grammar , head ): symbol = head or Symbol ( self . __class__ . __name__ ) compatible = [] for _ , other_cls in grammar . namespace . items (): if other_cls in self . exceptions : continue if hasattr ( other_cls , \"__name__\" ) and other_cls . __name__ in self . exceptions : continue if self . interface . is_compatible ( other_cls ): compatible . append ( other_cls ) if not compatible : raise ValueError ( \"Cannot find compatible implementations for <class %s >\" % self . interface ) return Subset ( symbol . name , * compatible ) . generate_cfg ( grammar , symbol ) def conforms(type1, type2): if inspect.isclass(type1) and inspect.isclass(type2): return issubclass(type1, type2) if hasattr ( type1 , \" __conforms__ \" ) and type1 . __conforms__ ( type2 ) : return True if hasattr ( type2 , \" __rconforms__ \" ) and type2 . __rconforms__ ( type1 ) : return True return False def _compatible_annotations ( methods_if : Mapping [ str , inspect . Signature ], methods_im : Mapping [ str , inspect . Signature ], ): for name , mif in methods_if . items (): if not name in methods_im : return False mim = methods_im [ name ] for name , param_if in mif . parameters . items (): if not name in mim . parameters : return False param_im = mim . parameters [ name ] ann_if = param_if . annotation if ann_if == inspect . Parameter . empty : continue ann_im = param_im . annotation if not conforms ( ann_if , ann_im ): return False return_if = mif . return_annotation if return_if == inspect . Parameter . empty : continue return_im = mim . return_annotation if not conforms ( return_im , return_if ): return False return True def _get_annotations ( clss , ignore = []): Note Computes the annotations of all public methods in type clss . Examples \u00b6 ```python class A: ... def f(self, input: int) -> float: ... pass _get_annotations(A) {'f': float>} ``` methods = inspect . getmembers ( clss , lambda m : inspect . ismethod ( m ) or inspect . isfunction ( m ) ) signatures = { name : inspect . signature ( method ) for name , method in methods if not name . startswith ( \"_\" ) } for name in ignore : signatures . pop ( name , None ) return signatures def make_list_wrapper(algorithm): \u00b6 from autogoal.kb._algorithm import _get_annotations \u00b6 input_type, output_type = _get_annotations(algorithm) \u00b6 name = f\"List[{algorithm. name }]\" \u00b6 def wrap_list(types): \u00b6 if isinstance(types, Tuple): \u00b6 return Tuple(*(List(t) for t in types.inner)) \u00b6 return List(types) \u00b6 def init_method(self, inner: algorithm): \u00b6 self.inner = inner \u00b6 def run_method(self, input: wrap_list(input_type)) -> wrap_list(output_type): \u00b6 return [self.inner.run(x) for x in xs] \u00b6 def repr_method(self): \u00b6 return f\"{name}(inner={repr(self.inner)})\" \u00b6 def getattr_method(self, attr): \u00b6 return getattr(self.inner, attr) \u00b6 def body(ns): \u00b6 ns[\" init \"] = init_method \u00b6 ns[\"run\"] = run_method \u00b6 ns[\" repr \"] = repr_method \u00b6 ns[\" getattr \"] = getattr_method \u00b6 return types.new_class(name=name, bases=(), exec_body=body) \u00b6 def build_composite_list(input_type, output_type, depth=1): \u00b6 def wrap(t, d): \u00b6 if d == 0: \u00b6 return t \u00b6 return List(wrap(t, d - 1)) \u00b6 input_wrapper = wrap(input_type, depth) \u00b6 output_wrapper = wrap(output_type, depth) \u00b6 # name = \"ListAlgorithm\" # % (input_wrapper, output_wrapper) \u00b6 name = \"ListAlgorithm\" #[%s, %s]\" % (input_wrapper, output_wrapper) \u00b6 def init_method(self, inner: algorithm(input_type, output_type)): \u00b6 self.inner = inner \u00b6 def run_method(self, input: input_wrapper) -> output_wrapper: \u00b6 def wrap_run(xs, d): \u00b6 if d == 0: \u00b6 return self.inner.run(xs) \u00b6 return [wrap_run(x, d - 1) for x in xs] \u00b6 return wrap_run(input, depth) \u00b6 def repr_method(self): \u00b6 return f\"{name}(inner={repr(self.inner)})\" \u00b6 def getattr_method(self, attr): \u00b6 return getattr(self.inner, attr) \u00b6 def reduce_method(self): \u00b6 return ( \u00b6 build_composite_list_instance, \u00b6 (input_type, output_type, self.inner) \u00b6 ) \u00b6 def body(ns): \u00b6 ns[\" init \"] = init_method \u00b6 ns[\"run\"] = run_method \u00b6 ns[\" repr \"] = repr_method \u00b6 ns[\" getattr \"] = getattr_method \u00b6 ns[\" reduce \"] = reduce_method \u00b6 return types.new_class(name=name, bases=(), exec_body=body) \u00b6 def build_composite_list_instance(input_type, output_type, inner_algorithm): \u00b6 \"\"\" \u00b6 Build a ListAlgorithm[...] type and instantiate it directly on a given algorithm. \u00b6 \"\"\" \u00b6 return build_composite_list(input_type, output_type)(inner_algorithm) \u00b6 def build_composite_tuple(index, input_type: \"Tuple\", output_type: \"Tuple\"): \u00b6 \"\"\" \u00b6 Dynamically generate a class TupleAlgorithm that wraps \u00b6 another algorithm to receive a Tuple but pass only one of the \u00b6 parameters to the internal algorithm. \u00b6 \"\"\" \u00b6 internal_input = input_type.inner[index] \u00b6 internal_output = output_type.inner[index] \u00b6 name = \"TupleAlgorithm\" #[%s, %s]\" % (input_type, output_type) \u00b6 def init_method(self, inner: algorithm(internal_input, internal_output)): \u00b6 self.inner = inner \u00b6 def run_method(self, input: input_type) -> output_type: \u00b6 elements = list(input) \u00b6 elements[index] = self.inner.run(elements[index]) \u00b6 return tuple(elements) \u00b6 def repr_method(self): \u00b6 return f\"{name}(inner={repr(self.inner)})\" \u00b6 def getattr_method(self, attr): \u00b6 return getattr(self.inner, attr) \u00b6 def reduce_method(self): \u00b6 return ( \u00b6 build_composite_tuple_instance, \u00b6 (index, input_type, output_type, self.inner) \u00b6 ) \u00b6 def body(ns): \u00b6 ns[\" init \"] = init_method \u00b6 ns[\"run\"] = run_method \u00b6 ns[\" repr \"] = repr_method \u00b6 ns[\" getattr \"] = getattr_method \u00b6 ns[\" reduce \"] = reduce_method \u00b6 return types.new_class(name=name, bases=(), exec_body=body) \u00b6 def build_composite_tuple_instance(index, input_type, output_type, inner_algorithm): \u00b6 \"\"\" \u00b6 Build a TupleAlgorithm[...] type and instantiate it directly on a given algorithm. \u00b6 \"\"\" \u00b6 return build_composite_tuple(index, input_type, output_type)(inner_algorithm) \u00b6 class DataType: \u00b6 def init (self, **tags): \u00b6 self.tags = tags \u00b6 def get_tag(self, tag): \u00b6 return self.tags.get(tag, None) \u00b6 def repr (self): \u00b6 # tags = \", \".join( \u00b6 # f\"{key}={value}\" \u00b6 # for key, value in sorted(self.tags.items(), key=lambda t: t[0]) \u00b6 # ) \u00b6 return f\"{self. class . name }()\" #({tags})\" \u00b6 def eq (self, other): \u00b6 return repr(self) == repr(other) \u00b6 def hash (self): \u00b6 return hash(repr(self)) \u00b6 @property \u00b6 def name (self): \u00b6 return self. class . name \u00b6 def conforms (self, other): \u00b6 return issubclass(self. class , other. class ) \u00b6 def infer_type(obj): \u00b6 \"\"\" \u00b6 Attempts to automatically infer the most precise semantic type for obj . \u00b6 ##### Parameters \u00b6 * obj : Object to detect its semantic type. \u00b6 ##### Raises \u00b6 * TypeError : if no valid semantic type was found that matched obj . \u00b6 ##### Examples \u00b6 * Natural language \u00b6 ```python \u00b6 >>> infer_type(\"hello\") \u00b6 Word() \u00b6 >>> infer_type(\"hello world\") \u00b6 Sentence() \u00b6 >>> infer_type(\"Hello Word. It is raining.\") \u00b6 Document() \u00b6 ``` \u00b6 * Vectors \u00b6 ``` \u00b6 >>> import numpy as np \u00b6 >>> infer_type(np.asarray([\"A\", \"B\", \"C\", \"D\"])) \u00b6 CategoricalVector() \u00b6 >>> infer_type(np.asarray([0.0, 1.1, 2.1, 0.2])) \u00b6 ContinuousVector() \u00b6 >>> infer_type(np.asarray([0, 1, 1, 0])) \u00b6 DiscreteVector() \u00b6 ``` \u00b6 * Matrices \u00b6 ``` \u00b6 >>> import numpy as np \u00b6 >>> infer_type(np.random.randn(10,10)) \u00b6 MatrixContinuousDense() \u00b6 >>> import scipy.sparse as sp \u00b6 >>> infer_type(sp.coo_matrix((10,10))) \u00b6 MatrixContinuousSparse() \u00b6 ``` \u00b6 \"\"\" \u00b6 if isinstance(obj, str): \u00b6 if \" \" not in obj: \u00b6 return Word() \u00b6 if \".\" not in obj: \u00b6 return Sentence() \u00b6 return Document() \u00b6 if isinstance(obj, list): \u00b6 internal_types = set([infer_type(x) for x in obj]) \u00b6 for test_type in [Document(), Sentence(), Word()]: \u00b6 if test_type in internal_types: \u00b6 return List(test_type) \u00b6 if hasattr(obj, \"shape\"): \u00b6 if len(obj.shape) == 1: \u00b6 if isinstance(obj, ndarray): \u00b6 if obj.dtype.kind == \"U\": \u00b6 return CategoricalVector() \u00b6 if obj.dtype.kind == \"i\": \u00b6 return DiscreteVector() \u00b6 if obj.dtype.kind == \"f\": \u00b6 return ContinuousVector() \u00b6 if len(obj.shape) == 2: \u00b6 if isinstance(obj, spmatrix): \u00b6 return MatrixContinuousSparse() \u00b6 if isinstance(obj, ndarray): \u00b6 if obj.dtype.kind == \"O\": \u00b6 return MatrixCategorical() \u00b6 else: \u00b6 return MatrixContinuousDense() \u00b6 raise TypeError(\"Cannot infer type for %r\" % obj) \u00b6 class Text(DataType): \u00b6 pass \u00b6 class Word(Text): \u00b6 pass \u00b6 class Stem(DataType): \u00b6 pass \u00b6 class Sentence(Text): \u00b6 pass \u00b6 class Document(Text): \u00b6 pass \u00b6 class Category(DataType): \u00b6 pass \u00b6 class Vector(DataType): \u00b6 pass \u00b6 class Matrix(DataType): \u00b6 pass \u00b6 class DenseMatrix(Matrix): \u00b6 pass \u00b6 class SparseMatrix(Matrix): \u00b6 pass \u00b6 class ContinuousVector(Vector): \u00b6 pass \u00b6 class DiscreteVector(Vector): \u00b6 pass \u00b6 class CategoricalVector(Vector): \u00b6 pass \u00b6 class MatrixContinuous(Matrix): \u00b6 pass \u00b6 class MatrixCategorical(Matrix): \u00b6 pass \u00b6 class MatrixContinuousDense(MatrixContinuous, DenseMatrix): \u00b6 pass \u00b6 class MatrixContinuousSparse(MatrixContinuous, SparseMatrix): \u00b6 pass \u00b6 class Entity(DataType): \u00b6 pass \u00b6 class Summary(Document): \u00b6 pass \u00b6 class Sentiment(DataType): \u00b6 pass \u00b6 class Synset(DataType): \u00b6 pass \u00b6 class Postag(DataType): \u00b6 pass \u00b6 class Chunktag(DataType): \u00b6 pass \u00b6 class Tensor3(DataType): \u00b6 pass \u00b6 class Tensor4(DataType): \u00b6 pass \u00b6 class Flags(DataType): \u00b6 pass \u00b6 class List(DataType): \u00b6 def init (self, inner): \u00b6 self.inner = inner \u00b6 # super(). init (**inner.tags) \u00b6 def depth(self): \u00b6 if not isinstance(self.inner, List): \u00b6 return 1 \u00b6 return 1 + self.inner.depth() \u00b6 def conforms (self, other): \u00b6 return isinstance(other, List) and conforms(self.inner, other.inner) \u00b6 def repr (self): \u00b6 return \"List(%r)\" % self.inner \u00b6 class Tuple(DataType): \u00b6 def init (self, *inner): \u00b6 self.inner = inner \u00b6 # super(). init (**inner[0].tags) \u00b6 def repr (self): \u00b6 items = \", \".join(repr(s) for s in self.inner) \u00b6 return \"Tuple(%s)\" % items \u00b6 def conforms (self, other): \u00b6 if not isinstance(other, Tuple): \u00b6 return False \u00b6 if len(self.inner) != len(other.inner): \u00b6 return False \u00b6 for x, y in zip(self.inner, other.inner): \u00b6 if not conforms(x, y): \u00b6 return False \u00b6 return True \u00b6 DATA_TYPES = frozenset( \u00b6 [ \u00b6 Text, \u00b6 Word, \u00b6 Stem, \u00b6 Sentence, \u00b6 Document, \u00b6 Category, \u00b6 Vector, \u00b6 Matrix, \u00b6 DenseMatrix, \u00b6 SparseMatrix, \u00b6 ContinuousVector, \u00b6 DiscreteVector, \u00b6 CategoricalVector, \u00b6 MatrixContinuous, \u00b6 MatrixCategorical, \u00b6 MatrixContinuousDense, \u00b6 MatrixContinuousSparse, \u00b6 Entity, \u00b6 Summary, \u00b6 Sentiment, \u00b6 Synset, \u00b6 Postag, \u00b6 Chunktag, \u00b6 Tensor3, \u00b6 List, \u00b6 Tuple, \u00b6 Flags, \u00b6 ] \u00b6 ) \u00b6 def draw_data_hierarchy(output_file): \u00b6 \"\"\" \u00b6 Creates an SVG representation of the DataType hierarchy, \u00b6 for documentation purposes. \u00b6 \"\"\" \u00b6 import pydot \u00b6 classes = list(DATA_TYPES) + [DataType] \u00b6 graph = pydot.Dot(direction=\"TB\") \u00b6 for clss in classes: \u00b6 graph.add_node(pydot.Node(clss. name )) \u00b6 for clss in classes: \u00b6 for base in clss. bases : \u00b6 if base not in classes: \u00b6 continue \u00b6 graph.add_edge(pydot.Edge(base. name , clss. name )) \u00b6 graph.write(output_file + \".svg\", format=\"svg\") \u00b6 graph.write(output_file + \".png\", format=\"png\") \u00b6","title":"Autogoal.kb. data"},{"location":"api/autogoal.kb._data/#examples","text":"```python class A: ... def f(self, input: int) -> float: ... pass _get_annotations(A) {'f': float>} ``` methods = inspect . getmembers ( clss , lambda m : inspect . ismethod ( m ) or inspect . isfunction ( m ) ) signatures = { name : inspect . signature ( method ) for name , method in methods if not name . startswith ( \"_\" ) } for name in ignore : signatures . pop ( name , None ) return signatures","title":"Examples"},{"location":"api/autogoal.kb._data/#def-make_list_wrapperalgorithm","text":"","title":"def make_list_wrapper(algorithm):"},{"location":"api/autogoal.kb._data/#from-autogoalkb_algorithm-import-_get_annotations","text":"","title":"from autogoal.kb._algorithm import _get_annotations"},{"location":"api/autogoal.kb._data/#input_type-output_type-_get_annotationsalgorithm","text":"","title":"input_type, output_type = _get_annotations(algorithm)"},{"location":"api/autogoal.kb._data/#name-flistalgorithmname","text":"","title":"name = f\"List[{algorithm.name}]\""},{"location":"api/autogoal.kb._data/#def-wrap_listtypes","text":"","title":"def wrap_list(types):"},{"location":"api/autogoal.kb._data/#if-isinstancetypes-tuple","text":"","title":"if isinstance(types, Tuple):"},{"location":"api/autogoal.kb._data/#return-tuplelistt-for-t-in-typesinner","text":"","title":"return Tuple(*(List(t) for t in types.inner))"},{"location":"api/autogoal.kb._data/#return-listtypes","text":"","title":"return List(types)"},{"location":"api/autogoal.kb._data/#def-init_methodself-inner-algorithm","text":"","title":"def init_method(self, inner: algorithm):"},{"location":"api/autogoal.kb._data/#selfinner-inner","text":"","title":"self.inner = inner"},{"location":"api/autogoal.kb._data/#def-run_methodself-input-wrap_listinput_type-wrap_listoutput_type","text":"","title":"def run_method(self, input: wrap_list(input_type)) -&gt; wrap_list(output_type):"},{"location":"api/autogoal.kb._data/#return-selfinnerrunx-for-x-in-xs","text":"","title":"return [self.inner.run(x) for x in xs]"},{"location":"api/autogoal.kb._data/#def-repr_methodself","text":"","title":"def repr_method(self):"},{"location":"api/autogoal.kb._data/#return-fnameinnerreprselfinner","text":"","title":"return f\"{name}(inner={repr(self.inner)})\""},{"location":"api/autogoal.kb._data/#def-getattr_methodself-attr","text":"","title":"def getattr_method(self, attr):"},{"location":"api/autogoal.kb._data/#return-getattrselfinner-attr","text":"","title":"return getattr(self.inner, attr)"},{"location":"api/autogoal.kb._data/#def-bodyns","text":"","title":"def body(ns):"},{"location":"api/autogoal.kb._data/#nsinit-init_method","text":"","title":"ns[\"init\"] = init_method"},{"location":"api/autogoal.kb._data/#nsrun-run_method","text":"","title":"ns[\"run\"] = run_method"},{"location":"api/autogoal.kb._data/#nsrepr-repr_method","text":"","title":"ns[\"repr\"] = repr_method"},{"location":"api/autogoal.kb._data/#nsgetattr-getattr_method","text":"","title":"ns[\"getattr\"] = getattr_method"},{"location":"api/autogoal.kb._data/#return-typesnew_classnamename-bases-exec_bodybody","text":"","title":"return types.new_class(name=name, bases=(), exec_body=body)"},{"location":"api/autogoal.kb._data/#def-build_composite_listinput_type-output_type-depth1","text":"","title":"def build_composite_list(input_type, output_type, depth=1):"},{"location":"api/autogoal.kb._data/#def-wrapt-d","text":"","title":"def wrap(t, d):"},{"location":"api/autogoal.kb._data/#if-d-0","text":"","title":"if d == 0:"},{"location":"api/autogoal.kb._data/#return-t","text":"","title":"return t"},{"location":"api/autogoal.kb._data/#return-listwrapt-d-1","text":"","title":"return List(wrap(t, d - 1))"},{"location":"api/autogoal.kb._data/#input_wrapper-wrapinput_type-depth","text":"","title":"input_wrapper = wrap(input_type, depth)"},{"location":"api/autogoal.kb._data/#output_wrapper-wrapoutput_type-depth","text":"","title":"output_wrapper = wrap(output_type, depth)"},{"location":"api/autogoal.kb._data/#name-listalgorithm-input_wrapper-output_wrapper","text":"","title":"# name = \"ListAlgorithm\"  # % (input_wrapper, output_wrapper)"},{"location":"api/autogoal.kb._data/#name-listalgorithm-s-s-input_wrapper-output_wrapper","text":"","title":"name = \"ListAlgorithm\" #[%s, %s]\" % (input_wrapper, output_wrapper)"},{"location":"api/autogoal.kb._data/#def-init_methodself-inner-algorithminput_type-output_type","text":"","title":"def init_method(self, inner: algorithm(input_type, output_type)):"},{"location":"api/autogoal.kb._data/#selfinner-inner_1","text":"","title":"self.inner = inner"},{"location":"api/autogoal.kb._data/#def-run_methodself-input-input_wrapper-output_wrapper","text":"","title":"def run_method(self, input: input_wrapper) -&gt; output_wrapper:"},{"location":"api/autogoal.kb._data/#def-wrap_runxs-d","text":"","title":"def wrap_run(xs, d):"},{"location":"api/autogoal.kb._data/#if-d-0_1","text":"","title":"if d == 0:"},{"location":"api/autogoal.kb._data/#return-selfinnerrunxs","text":"","title":"return self.inner.run(xs)"},{"location":"api/autogoal.kb._data/#return-wrap_runx-d-1-for-x-in-xs","text":"","title":"return [wrap_run(x, d - 1) for x in xs]"},{"location":"api/autogoal.kb._data/#return-wrap_runinput-depth","text":"","title":"return wrap_run(input, depth)"},{"location":"api/autogoal.kb._data/#def-repr_methodself_1","text":"","title":"def repr_method(self):"},{"location":"api/autogoal.kb._data/#return-fnameinnerreprselfinner_1","text":"","title":"return f\"{name}(inner={repr(self.inner)})\""},{"location":"api/autogoal.kb._data/#def-getattr_methodself-attr_1","text":"","title":"def getattr_method(self, attr):"},{"location":"api/autogoal.kb._data/#return-getattrselfinner-attr_1","text":"","title":"return getattr(self.inner, attr)"},{"location":"api/autogoal.kb._data/#def-reduce_methodself","text":"","title":"def reduce_method(self):"},{"location":"api/autogoal.kb._data/#return","text":"","title":"return ("},{"location":"api/autogoal.kb._data/#build_composite_list_instance","text":"","title":"build_composite_list_instance,"},{"location":"api/autogoal.kb._data/#input_type-output_type-selfinner","text":"","title":"(input_type, output_type, self.inner)"},{"location":"api/autogoal.kb._data/#_1","text":"","title":")"},{"location":"api/autogoal.kb._data/#def-bodyns_1","text":"","title":"def body(ns):"},{"location":"api/autogoal.kb._data/#nsinit-init_method_1","text":"","title":"ns[\"init\"] = init_method"},{"location":"api/autogoal.kb._data/#nsrun-run_method_1","text":"","title":"ns[\"run\"] = run_method"},{"location":"api/autogoal.kb._data/#nsrepr-repr_method_1","text":"","title":"ns[\"repr\"] = repr_method"},{"location":"api/autogoal.kb._data/#nsgetattr-getattr_method_1","text":"","title":"ns[\"getattr\"] = getattr_method"},{"location":"api/autogoal.kb._data/#nsreduce-reduce_method","text":"","title":"ns[\"reduce\"] = reduce_method"},{"location":"api/autogoal.kb._data/#return-typesnew_classnamename-bases-exec_bodybody_1","text":"","title":"return types.new_class(name=name, bases=(), exec_body=body)"},{"location":"api/autogoal.kb._data/#def-build_composite_list_instanceinput_type-output_type-inner_algorithm","text":"","title":"def build_composite_list_instance(input_type, output_type, inner_algorithm):"},{"location":"api/autogoal.kb._data/#_2","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#build-a-listalgorithm-type-and-instantiate-it-directly-on-a-given-algorithm","text":"","title":"Build a ListAlgorithm[...] type and instantiate it directly on a given algorithm."},{"location":"api/autogoal.kb._data/#_3","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#return-build_composite_listinput_type-output_typeinner_algorithm","text":"","title":"return build_composite_list(input_type, output_type)(inner_algorithm)"},{"location":"api/autogoal.kb._data/#def-build_composite_tupleindex-input_type-tuple-output_type-tuple","text":"","title":"def build_composite_tuple(index, input_type: \"Tuple\", output_type: \"Tuple\"):"},{"location":"api/autogoal.kb._data/#_4","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#dynamically-generate-a-class-tuplealgorithm-that-wraps","text":"","title":"Dynamically generate a class TupleAlgorithm that wraps"},{"location":"api/autogoal.kb._data/#another-algorithm-to-receive-a-tuple-but-pass-only-one-of-the","text":"","title":"another algorithm to receive a Tuple but pass only one of the"},{"location":"api/autogoal.kb._data/#parameters-to-the-internal-algorithm","text":"","title":"parameters to the internal algorithm."},{"location":"api/autogoal.kb._data/#_5","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#internal_input-input_typeinnerindex","text":"","title":"internal_input = input_type.inner[index]"},{"location":"api/autogoal.kb._data/#internal_output-output_typeinnerindex","text":"","title":"internal_output = output_type.inner[index]"},{"location":"api/autogoal.kb._data/#name-tuplealgorithm-s-s-input_type-output_type","text":"","title":"name = \"TupleAlgorithm\" #[%s, %s]\" % (input_type, output_type)"},{"location":"api/autogoal.kb._data/#def-init_methodself-inner-algorithminternal_input-internal_output","text":"","title":"def init_method(self, inner: algorithm(internal_input, internal_output)):"},{"location":"api/autogoal.kb._data/#selfinner-inner_2","text":"","title":"self.inner = inner"},{"location":"api/autogoal.kb._data/#def-run_methodself-input-input_type-output_type","text":"","title":"def run_method(self, input: input_type) -&gt; output_type:"},{"location":"api/autogoal.kb._data/#elements-listinput","text":"","title":"elements = list(input)"},{"location":"api/autogoal.kb._data/#elementsindex-selfinnerrunelementsindex","text":"","title":"elements[index] = self.inner.run(elements[index])"},{"location":"api/autogoal.kb._data/#return-tupleelements","text":"","title":"return tuple(elements)"},{"location":"api/autogoal.kb._data/#def-repr_methodself_2","text":"","title":"def repr_method(self):"},{"location":"api/autogoal.kb._data/#return-fnameinnerreprselfinner_2","text":"","title":"return f\"{name}(inner={repr(self.inner)})\""},{"location":"api/autogoal.kb._data/#def-getattr_methodself-attr_2","text":"","title":"def getattr_method(self, attr):"},{"location":"api/autogoal.kb._data/#return-getattrselfinner-attr_2","text":"","title":"return getattr(self.inner, attr)"},{"location":"api/autogoal.kb._data/#def-reduce_methodself_1","text":"","title":"def reduce_method(self):"},{"location":"api/autogoal.kb._data/#return_1","text":"","title":"return ("},{"location":"api/autogoal.kb._data/#build_composite_tuple_instance","text":"","title":"build_composite_tuple_instance,"},{"location":"api/autogoal.kb._data/#index-input_type-output_type-selfinner","text":"","title":"(index, input_type, output_type, self.inner)"},{"location":"api/autogoal.kb._data/#_6","text":"","title":")"},{"location":"api/autogoal.kb._data/#def-bodyns_2","text":"","title":"def body(ns):"},{"location":"api/autogoal.kb._data/#nsinit-init_method_2","text":"","title":"ns[\"init\"] = init_method"},{"location":"api/autogoal.kb._data/#nsrun-run_method_2","text":"","title":"ns[\"run\"] = run_method"},{"location":"api/autogoal.kb._data/#nsrepr-repr_method_2","text":"","title":"ns[\"repr\"] = repr_method"},{"location":"api/autogoal.kb._data/#nsgetattr-getattr_method_2","text":"","title":"ns[\"getattr\"] = getattr_method"},{"location":"api/autogoal.kb._data/#nsreduce-reduce_method_1","text":"","title":"ns[\"reduce\"] = reduce_method"},{"location":"api/autogoal.kb._data/#return-typesnew_classnamename-bases-exec_bodybody_2","text":"","title":"return types.new_class(name=name, bases=(), exec_body=body)"},{"location":"api/autogoal.kb._data/#def-build_composite_tuple_instanceindex-input_type-output_type-inner_algorithm","text":"","title":"def build_composite_tuple_instance(index, input_type, output_type, inner_algorithm):"},{"location":"api/autogoal.kb._data/#_7","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#build-a-tuplealgorithm-type-and-instantiate-it-directly-on-a-given-algorithm","text":"","title":"Build a TupleAlgorithm[...] type and instantiate it directly on a given algorithm."},{"location":"api/autogoal.kb._data/#_8","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#return-build_composite_tupleindex-input_type-output_typeinner_algorithm","text":"","title":"return build_composite_tuple(index, input_type, output_type)(inner_algorithm)"},{"location":"api/autogoal.kb._data/#class-datatype","text":"","title":"class DataType:"},{"location":"api/autogoal.kb._data/#def-initself-tags","text":"","title":"def init(self, **tags):"},{"location":"api/autogoal.kb._data/#selftags-tags","text":"","title":"self.tags = tags"},{"location":"api/autogoal.kb._data/#def-get_tagself-tag","text":"","title":"def get_tag(self, tag):"},{"location":"api/autogoal.kb._data/#return-selftagsgettag-none","text":"","title":"return self.tags.get(tag, None)"},{"location":"api/autogoal.kb._data/#def-reprself","text":"","title":"def repr(self):"},{"location":"api/autogoal.kb._data/#tags-join","text":"","title":"# tags = \", \".join("},{"location":"api/autogoal.kb._data/#fkeyvalue","text":"","title":"#     f\"{key}={value}\""},{"location":"api/autogoal.kb._data/#for-key-value-in-sortedselftagsitems-keylambda-t-t0","text":"","title":"#     for key, value in sorted(self.tags.items(), key=lambda t: t[0])"},{"location":"api/autogoal.kb._data/#_9","text":"","title":"# )"},{"location":"api/autogoal.kb._data/#return-fselfclassname-tags","text":"","title":"return f\"{self.class.name}()\" #({tags})\""},{"location":"api/autogoal.kb._data/#def-eqself-other","text":"","title":"def eq(self, other):"},{"location":"api/autogoal.kb._data/#return-reprself-reprother","text":"","title":"return repr(self) == repr(other)"},{"location":"api/autogoal.kb._data/#def-hashself","text":"","title":"def hash(self):"},{"location":"api/autogoal.kb._data/#return-hashreprself","text":"","title":"return hash(repr(self))"},{"location":"api/autogoal.kb._data/#property","text":"","title":"@property"},{"location":"api/autogoal.kb._data/#def-nameself","text":"","title":"def name(self):"},{"location":"api/autogoal.kb._data/#return-selfclassname","text":"","title":"return self.class.name"},{"location":"api/autogoal.kb._data/#def-conformsself-other","text":"","title":"def conforms(self, other):"},{"location":"api/autogoal.kb._data/#return-issubclassselfclass-otherclass","text":"","title":"return issubclass(self.class, other.class)"},{"location":"api/autogoal.kb._data/#def-infer_typeobj","text":"","title":"def infer_type(obj):"},{"location":"api/autogoal.kb._data/#_10","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#attempts-to-automatically-infer-the-most-precise-semantic-type-for-obj","text":"","title":"Attempts to automatically infer the most precise semantic type for obj."},{"location":"api/autogoal.kb._data/#parameters","text":"","title":"##### Parameters"},{"location":"api/autogoal.kb._data/#obj-object-to-detect-its-semantic-type","text":"","title":"* obj: Object to detect its semantic type."},{"location":"api/autogoal.kb._data/#raises","text":"","title":"##### Raises"},{"location":"api/autogoal.kb._data/#typeerror-if-no-valid-semantic-type-was-found-that-matched-obj","text":"","title":"* TypeError: if no valid semantic type was found that matched obj."},{"location":"api/autogoal.kb._data/#examples_1","text":"","title":"##### Examples"},{"location":"api/autogoal.kb._data/#natural-language","text":"","title":"* Natural language"},{"location":"api/autogoal.kb._data/#python","text":"","title":"```python"},{"location":"api/autogoal.kb._data/#infer_typehello","text":"","title":"&gt;&gt;&gt; infer_type(\"hello\")"},{"location":"api/autogoal.kb._data/#word","text":"","title":"Word()"},{"location":"api/autogoal.kb._data/#infer_typehello-world","text":"","title":"&gt;&gt;&gt; infer_type(\"hello world\")"},{"location":"api/autogoal.kb._data/#sentence","text":"","title":"Sentence()"},{"location":"api/autogoal.kb._data/#infer_typehello-word-it-is-raining","text":"","title":"&gt;&gt;&gt; infer_type(\"Hello Word. It is raining.\")"},{"location":"api/autogoal.kb._data/#document","text":"","title":"Document()"},{"location":"api/autogoal.kb._data/#_11","text":"","title":"```"},{"location":"api/autogoal.kb._data/#vectors","text":"","title":"* Vectors"},{"location":"api/autogoal.kb._data/#_12","text":"","title":"```"},{"location":"api/autogoal.kb._data/#import-numpy-as-np","text":"","title":"&gt;&gt;&gt; import numpy as np"},{"location":"api/autogoal.kb._data/#infer_typenpasarraya-b-c-d","text":"","title":"&gt;&gt;&gt; infer_type(np.asarray([\"A\", \"B\", \"C\", \"D\"]))"},{"location":"api/autogoal.kb._data/#categoricalvector","text":"","title":"CategoricalVector()"},{"location":"api/autogoal.kb._data/#infer_typenpasarray00-11-21-02","text":"","title":"&gt;&gt;&gt; infer_type(np.asarray([0.0, 1.1, 2.1, 0.2]))"},{"location":"api/autogoal.kb._data/#continuousvector","text":"","title":"ContinuousVector()"},{"location":"api/autogoal.kb._data/#infer_typenpasarray0-1-1-0","text":"","title":"&gt;&gt;&gt; infer_type(np.asarray([0, 1, 1, 0]))"},{"location":"api/autogoal.kb._data/#discretevector","text":"","title":"DiscreteVector()"},{"location":"api/autogoal.kb._data/#_13","text":"","title":"```"},{"location":"api/autogoal.kb._data/#matrices","text":"","title":"* Matrices"},{"location":"api/autogoal.kb._data/#_14","text":"","title":"```"},{"location":"api/autogoal.kb._data/#import-numpy-as-np_1","text":"","title":"&gt;&gt;&gt; import numpy as np"},{"location":"api/autogoal.kb._data/#infer_typenprandomrandn1010","text":"","title":"&gt;&gt;&gt; infer_type(np.random.randn(10,10))"},{"location":"api/autogoal.kb._data/#matrixcontinuousdense","text":"","title":"MatrixContinuousDense()"},{"location":"api/autogoal.kb._data/#import-scipysparse-as-sp","text":"","title":"&gt;&gt;&gt; import scipy.sparse as sp"},{"location":"api/autogoal.kb._data/#infer_typespcoo_matrix1010","text":"","title":"&gt;&gt;&gt; infer_type(sp.coo_matrix((10,10)))"},{"location":"api/autogoal.kb._data/#matrixcontinuoussparse","text":"","title":"MatrixContinuousSparse()"},{"location":"api/autogoal.kb._data/#_15","text":"","title":"```"},{"location":"api/autogoal.kb._data/#_16","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#if-isinstanceobj-str","text":"","title":"if isinstance(obj, str):"},{"location":"api/autogoal.kb._data/#if-not-in-obj","text":"","title":"if \" \" not in obj:"},{"location":"api/autogoal.kb._data/#return-word","text":"","title":"return Word()"},{"location":"api/autogoal.kb._data/#if-not-in-obj_1","text":"","title":"if \".\" not in obj:"},{"location":"api/autogoal.kb._data/#return-sentence","text":"","title":"return Sentence()"},{"location":"api/autogoal.kb._data/#return-document","text":"","title":"return Document()"},{"location":"api/autogoal.kb._data/#if-isinstanceobj-list","text":"","title":"if isinstance(obj, list):"},{"location":"api/autogoal.kb._data/#internal_types-setinfer_typex-for-x-in-obj","text":"","title":"internal_types = set([infer_type(x) for x in obj])"},{"location":"api/autogoal.kb._data/#for-test_type-in-document-sentence-word","text":"","title":"for test_type in [Document(), Sentence(), Word()]:"},{"location":"api/autogoal.kb._data/#if-test_type-in-internal_types","text":"","title":"if test_type in internal_types:"},{"location":"api/autogoal.kb._data/#return-listtest_type","text":"","title":"return List(test_type)"},{"location":"api/autogoal.kb._data/#if-hasattrobj-shape","text":"","title":"if hasattr(obj, \"shape\"):"},{"location":"api/autogoal.kb._data/#if-lenobjshape-1","text":"","title":"if len(obj.shape) == 1:"},{"location":"api/autogoal.kb._data/#if-isinstanceobj-ndarray","text":"","title":"if isinstance(obj, ndarray):"},{"location":"api/autogoal.kb._data/#if-objdtypekind-u","text":"","title":"if obj.dtype.kind == \"U\":"},{"location":"api/autogoal.kb._data/#return-categoricalvector","text":"","title":"return CategoricalVector()"},{"location":"api/autogoal.kb._data/#if-objdtypekind-i","text":"","title":"if obj.dtype.kind == \"i\":"},{"location":"api/autogoal.kb._data/#return-discretevector","text":"","title":"return DiscreteVector()"},{"location":"api/autogoal.kb._data/#if-objdtypekind-f","text":"","title":"if obj.dtype.kind == \"f\":"},{"location":"api/autogoal.kb._data/#return-continuousvector","text":"","title":"return ContinuousVector()"},{"location":"api/autogoal.kb._data/#if-lenobjshape-2","text":"","title":"if len(obj.shape) == 2:"},{"location":"api/autogoal.kb._data/#if-isinstanceobj-spmatrix","text":"","title":"if isinstance(obj, spmatrix):"},{"location":"api/autogoal.kb._data/#return-matrixcontinuoussparse","text":"","title":"return MatrixContinuousSparse()"},{"location":"api/autogoal.kb._data/#if-isinstanceobj-ndarray_1","text":"","title":"if isinstance(obj, ndarray):"},{"location":"api/autogoal.kb._data/#if-objdtypekind-o","text":"","title":"if obj.dtype.kind == \"O\":"},{"location":"api/autogoal.kb._data/#return-matrixcategorical","text":"","title":"return MatrixCategorical()"},{"location":"api/autogoal.kb._data/#else","text":"","title":"else:"},{"location":"api/autogoal.kb._data/#return-matrixcontinuousdense","text":"","title":"return MatrixContinuousDense()"},{"location":"api/autogoal.kb._data/#raise-typeerrorcannot-infer-type-for-r-obj","text":"","title":"raise TypeError(\"Cannot infer type for %r\" % obj)"},{"location":"api/autogoal.kb._data/#class-textdatatype","text":"","title":"class Text(DataType):"},{"location":"api/autogoal.kb._data/#pass","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-wordtext","text":"","title":"class Word(Text):"},{"location":"api/autogoal.kb._data/#pass_1","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-stemdatatype","text":"","title":"class Stem(DataType):"},{"location":"api/autogoal.kb._data/#pass_2","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-sentencetext","text":"","title":"class Sentence(Text):"},{"location":"api/autogoal.kb._data/#pass_3","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-documenttext","text":"","title":"class Document(Text):"},{"location":"api/autogoal.kb._data/#pass_4","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-categorydatatype","text":"","title":"class Category(DataType):"},{"location":"api/autogoal.kb._data/#pass_5","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-vectordatatype","text":"","title":"class Vector(DataType):"},{"location":"api/autogoal.kb._data/#pass_6","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-matrixdatatype","text":"","title":"class Matrix(DataType):"},{"location":"api/autogoal.kb._data/#pass_7","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-densematrixmatrix","text":"","title":"class DenseMatrix(Matrix):"},{"location":"api/autogoal.kb._data/#pass_8","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-sparsematrixmatrix","text":"","title":"class SparseMatrix(Matrix):"},{"location":"api/autogoal.kb._data/#pass_9","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-continuousvectorvector","text":"","title":"class ContinuousVector(Vector):"},{"location":"api/autogoal.kb._data/#pass_10","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-discretevectorvector","text":"","title":"class DiscreteVector(Vector):"},{"location":"api/autogoal.kb._data/#pass_11","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-categoricalvectorvector","text":"","title":"class CategoricalVector(Vector):"},{"location":"api/autogoal.kb._data/#pass_12","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-matrixcontinuousmatrix","text":"","title":"class MatrixContinuous(Matrix):"},{"location":"api/autogoal.kb._data/#pass_13","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-matrixcategoricalmatrix","text":"","title":"class MatrixCategorical(Matrix):"},{"location":"api/autogoal.kb._data/#pass_14","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-matrixcontinuousdensematrixcontinuous-densematrix","text":"","title":"class MatrixContinuousDense(MatrixContinuous, DenseMatrix):"},{"location":"api/autogoal.kb._data/#pass_15","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-matrixcontinuoussparsematrixcontinuous-sparsematrix","text":"","title":"class MatrixContinuousSparse(MatrixContinuous, SparseMatrix):"},{"location":"api/autogoal.kb._data/#pass_16","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-entitydatatype","text":"","title":"class Entity(DataType):"},{"location":"api/autogoal.kb._data/#pass_17","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-summarydocument","text":"","title":"class Summary(Document):"},{"location":"api/autogoal.kb._data/#pass_18","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-sentimentdatatype","text":"","title":"class Sentiment(DataType):"},{"location":"api/autogoal.kb._data/#pass_19","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-synsetdatatype","text":"","title":"class Synset(DataType):"},{"location":"api/autogoal.kb._data/#pass_20","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-postagdatatype","text":"","title":"class Postag(DataType):"},{"location":"api/autogoal.kb._data/#pass_21","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-chunktagdatatype","text":"","title":"class Chunktag(DataType):"},{"location":"api/autogoal.kb._data/#pass_22","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-tensor3datatype","text":"","title":"class Tensor3(DataType):"},{"location":"api/autogoal.kb._data/#pass_23","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-tensor4datatype","text":"","title":"class Tensor4(DataType):"},{"location":"api/autogoal.kb._data/#pass_24","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-flagsdatatype","text":"","title":"class Flags(DataType):"},{"location":"api/autogoal.kb._data/#pass_25","text":"","title":"pass"},{"location":"api/autogoal.kb._data/#class-listdatatype","text":"","title":"class List(DataType):"},{"location":"api/autogoal.kb._data/#def-initself-inner","text":"","title":"def init(self, inner):"},{"location":"api/autogoal.kb._data/#selfinner-inner_3","text":"","title":"self.inner = inner"},{"location":"api/autogoal.kb._data/#superinitinnertags","text":"","title":"# super().init(**inner.tags)"},{"location":"api/autogoal.kb._data/#def-depthself","text":"","title":"def depth(self):"},{"location":"api/autogoal.kb._data/#if-not-isinstanceselfinner-list","text":"","title":"if not isinstance(self.inner, List):"},{"location":"api/autogoal.kb._data/#return-1","text":"","title":"return 1"},{"location":"api/autogoal.kb._data/#return-1-selfinnerdepth","text":"","title":"return 1 + self.inner.depth()"},{"location":"api/autogoal.kb._data/#def-conformsself-other_1","text":"","title":"def conforms(self, other):"},{"location":"api/autogoal.kb._data/#return-isinstanceother-list-and-conformsselfinner-otherinner","text":"","title":"return isinstance(other, List) and conforms(self.inner, other.inner)"},{"location":"api/autogoal.kb._data/#def-reprself_1","text":"","title":"def repr(self):"},{"location":"api/autogoal.kb._data/#return-listr-selfinner","text":"","title":"return \"List(%r)\" % self.inner"},{"location":"api/autogoal.kb._data/#class-tupledatatype","text":"","title":"class Tuple(DataType):"},{"location":"api/autogoal.kb._data/#def-initself-inner_1","text":"","title":"def init(self, *inner):"},{"location":"api/autogoal.kb._data/#selfinner-inner_4","text":"","title":"self.inner = inner"},{"location":"api/autogoal.kb._data/#superinitinner0tags","text":"","title":"# super().init(**inner[0].tags)"},{"location":"api/autogoal.kb._data/#def-reprself_2","text":"","title":"def repr(self):"},{"location":"api/autogoal.kb._data/#items-joinreprs-for-s-in-selfinner","text":"","title":"items = \", \".join(repr(s) for s in self.inner)"},{"location":"api/autogoal.kb._data/#return-tuples-items","text":"","title":"return \"Tuple(%s)\" % items"},{"location":"api/autogoal.kb._data/#def-conformsself-other_2","text":"","title":"def conforms(self, other):"},{"location":"api/autogoal.kb._data/#if-not-isinstanceother-tuple","text":"","title":"if not isinstance(other, Tuple):"},{"location":"api/autogoal.kb._data/#return-false","text":"","title":"return False"},{"location":"api/autogoal.kb._data/#if-lenselfinner-lenotherinner","text":"","title":"if len(self.inner) != len(other.inner):"},{"location":"api/autogoal.kb._data/#return-false_1","text":"","title":"return False"},{"location":"api/autogoal.kb._data/#for-x-y-in-zipselfinner-otherinner","text":"","title":"for x, y in zip(self.inner, other.inner):"},{"location":"api/autogoal.kb._data/#if-not-conformsx-y","text":"","title":"if not conforms(x, y):"},{"location":"api/autogoal.kb._data/#return-false_2","text":"","title":"return False"},{"location":"api/autogoal.kb._data/#return-true","text":"","title":"return True"},{"location":"api/autogoal.kb._data/#data_types-frozenset","text":"","title":"DATA_TYPES = frozenset("},{"location":"api/autogoal.kb._data/#_17","text":"","title":"["},{"location":"api/autogoal.kb._data/#text","text":"","title":"Text,"},{"location":"api/autogoal.kb._data/#word_1","text":"","title":"Word,"},{"location":"api/autogoal.kb._data/#stem","text":"","title":"Stem,"},{"location":"api/autogoal.kb._data/#sentence_1","text":"","title":"Sentence,"},{"location":"api/autogoal.kb._data/#document_1","text":"","title":"Document,"},{"location":"api/autogoal.kb._data/#category","text":"","title":"Category,"},{"location":"api/autogoal.kb._data/#vector","text":"","title":"Vector,"},{"location":"api/autogoal.kb._data/#matrix","text":"","title":"Matrix,"},{"location":"api/autogoal.kb._data/#densematrix","text":"","title":"DenseMatrix,"},{"location":"api/autogoal.kb._data/#sparsematrix","text":"","title":"SparseMatrix,"},{"location":"api/autogoal.kb._data/#continuousvector_1","text":"","title":"ContinuousVector,"},{"location":"api/autogoal.kb._data/#discretevector_1","text":"","title":"DiscreteVector,"},{"location":"api/autogoal.kb._data/#categoricalvector_1","text":"","title":"CategoricalVector,"},{"location":"api/autogoal.kb._data/#matrixcontinuous","text":"","title":"MatrixContinuous,"},{"location":"api/autogoal.kb._data/#matrixcategorical","text":"","title":"MatrixCategorical,"},{"location":"api/autogoal.kb._data/#matrixcontinuousdense_1","text":"","title":"MatrixContinuousDense,"},{"location":"api/autogoal.kb._data/#matrixcontinuoussparse_1","text":"","title":"MatrixContinuousSparse,"},{"location":"api/autogoal.kb._data/#entity","text":"","title":"Entity,"},{"location":"api/autogoal.kb._data/#summary","text":"","title":"Summary,"},{"location":"api/autogoal.kb._data/#sentiment","text":"","title":"Sentiment,"},{"location":"api/autogoal.kb._data/#synset","text":"","title":"Synset,"},{"location":"api/autogoal.kb._data/#postag","text":"","title":"Postag,"},{"location":"api/autogoal.kb._data/#chunktag","text":"","title":"Chunktag,"},{"location":"api/autogoal.kb._data/#tensor3","text":"","title":"Tensor3,"},{"location":"api/autogoal.kb._data/#list","text":"","title":"List,"},{"location":"api/autogoal.kb._data/#tuple","text":"","title":"Tuple,"},{"location":"api/autogoal.kb._data/#flags","text":"","title":"Flags,"},{"location":"api/autogoal.kb._data/#_18","text":"","title":"]"},{"location":"api/autogoal.kb._data/#_19","text":"","title":")"},{"location":"api/autogoal.kb._data/#def-draw_data_hierarchyoutput_file","text":"","title":"def draw_data_hierarchy(output_file):"},{"location":"api/autogoal.kb._data/#_20","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#creates-an-svg-representation-of-the-datatype-hierarchy","text":"","title":"Creates an SVG representation of the DataType hierarchy,"},{"location":"api/autogoal.kb._data/#for-documentation-purposes","text":"","title":"for documentation purposes."},{"location":"api/autogoal.kb._data/#_21","text":"","title":"\"\"\""},{"location":"api/autogoal.kb._data/#import-pydot","text":"","title":"import pydot"},{"location":"api/autogoal.kb._data/#classes-listdata_types-datatype","text":"","title":"classes = list(DATA_TYPES) + [DataType]"},{"location":"api/autogoal.kb._data/#graph-pydotdotdirectiontb","text":"","title":"graph = pydot.Dot(direction=\"TB\")"},{"location":"api/autogoal.kb._data/#for-clss-in-classes","text":"","title":"for clss in classes:"},{"location":"api/autogoal.kb._data/#graphadd_nodepydotnodeclssname","text":"","title":"graph.add_node(pydot.Node(clss.name))"},{"location":"api/autogoal.kb._data/#for-clss-in-classes_1","text":"","title":"for clss in classes:"},{"location":"api/autogoal.kb._data/#for-base-in-clssbases","text":"","title":"for base in clss.bases:"},{"location":"api/autogoal.kb._data/#if-base-not-in-classes","text":"","title":"if base not in classes:"},{"location":"api/autogoal.kb._data/#continue","text":"","title":"continue"},{"location":"api/autogoal.kb._data/#graphadd_edgepydotedgebasename-clssname","text":"","title":"graph.add_edge(pydot.Edge(base.name, clss.name))"},{"location":"api/autogoal.kb._data/#graphwriteoutput_file-svg-formatsvg","text":"","title":"graph.write(output_file + \".svg\", format=\"svg\")"},{"location":"api/autogoal.kb._data/#graphwriteoutput_file-png-formatpng","text":"","title":"graph.write(output_file + \".png\", format=\"png\")"},{"location":"api/autogoal.kb._semantics/","text":"Note \"\"\"Defines the semantic types hierarchy and utilities to infer types and work with them. The AutoML features in AutoGOAL rest in the ability of automatically discovering pipelines of algorithms that are semantically annotated w.r.t. their inputs and outputs. This module defines the semantic types hierarchy. A semantic type is basically a class that provides a meaningful definition for a value in the problem domain. For example, a list of strings can either be a list of sentences, words, DNA sequences, or full-fledged documents. Depending on the interpretation of that value, we want AutoGOAL to select different algorithms, i.e., it makes no sense to tokenize words, while it is almost mandatory to do so when you have natural language text. We will never really instantiate these classes, just use them for annotations. from functools import reduce import inspect import copyreg from typing import Type We start by defining the base class of our hierarchy. A SemanticType is just a class that knows how to do one thing: determine if a given object matches its semantic definition. We will define a metaclass to allow for isinstance and issubclass to work based on our semantic match definition, and to leave space for implementing class-level __getitem__ to allow for a generics kind-of notation. class SemanticTypeMeta ( type ): def __instancecheck__ ( cls , instance ) -> bool : return cls . _match ( instance ) def __getitem__ ( cls , args ): if isinstance ( args , tuple ): return cls . _specialize ( * args ) else : return cls . _specialize ( args ) def __subclasscheck__ ( cls , subclass : type ) -> bool : if hasattr ( subclass , \"_conforms\" ) and subclass . _conforms ( cls ): return True return super () . __subclasscheck__ ( subclass ) def __call__ ( self , * args , ** kwds ): raise TypeError ( \"Cannot instantiate a semantic type\" ) def __repr__ ( cls ) -> str : return cls . _name () SemanticType defines a match method that implements our isinstance method. class SemanticType ( metaclass = SemanticTypeMeta ): @classmethod def _match ( cls , x ) -> bool : return False @classmethod def _conforms ( cls , other : type ) -> bool : return False @classmethod def _name ( cls ) -> str : return cls . __name__ @classmethod def _specialize ( cls , * args ): raise TypeError ( f \" { cls } cannot be specialized\" ) @classmethod def _reduce ( cls ): By default, we return the repr name of the class that allows to serialize globally defined classes. return cls . _name () @staticmethod def infer ( x ): Note \"\"\"Automatically determines the semantic type of a given value. SemanticType.infer(\"word\") Word SemanticType.infer(\"hello world\") Sentence SemanticType.infer(\"Hello world. This a two sentence Document.\") Document It works with some tensorial types as well. import numpy as np SemanticType.infer(np.ones(shape=(2,2))) Tensor[2, Continuous, Dense] types = inspect . getmembers ( inspect . getmodule ( SemanticType ), inspect . isclass ) best_type = SemanticType for _ , t in types : if isinstance ( x , t ) and issubclass ( t , best_type ): best_type = t if best_type == SemanticType : raise ValueError ( f \"Cannot infer semantic type for { x } \" ) return best_type To be able to serialize these types, we have to register a reduce function for SemanticTypeMeta . This reduce function will just dispatch to the proper instance method def _reduce_semantic_type ( t ): return t . _reduce () copyreg . pickle ( SemanticTypeMeta , _reduce_semantic_type ) Let's start with the natural language hierarchy. class Text ( SemanticType ): @classmethod def _match ( cls , x ): return isinstance ( x , str ) class Document ( Text ): pass class Sentence ( Document ): @classmethod def _match ( cls , x ): return super () . _match ( x ) and x . count ( \".\" ) <= 1 class Word ( Sentence ): @classmethod def _match ( cls , x ): return super () . _match ( x ) and \" \" not in x We also need some basic types for generic kinds of labels, used in NLP, for example. Keep in mind these do not implement _match as there is no sensible structural way to define them. They are used solely to annotate algorithms. class Label ( SemanticType ): pass class Postag ( Label ): pass class Chunktag ( Label ): pass class Stem ( Text ): pass class Synset ( SemanticType ): pass class Sentiment ( Label ): pass Another basic type is a feature set, which for us is basically a dictionary of strings vs anything: class FeatureSet ( SemanticType ): @classmethod def _match ( self , x ): TODO: This is a very naive implementation of match . return isinstance ( x , dict ) and isinstance ( list ( x . keys ()[ 0 ]), str ) A first complex type we can implement is Seq , to represent a list (or sequence) of another semantic type. We want this type to be able to specialize in this notation: Seq[Word] For this we have to implement _specialize and synthethize a new type with the corresponding semantic type that does the match internally. The challenge here is that we want to return the same Seq type every time we specialize on the same internal type, so we'll keep a class-level dictionary to store these classes as they are synthethized. Finally, we want Seq[Word]`` to be a subclass of Seq[Sentence] , or more generally, Seq[X] to be a subclass of Seq[Y] whenever X < Y`. class Seq ( SemanticType ): Note \"\"\"Represents a sequence that can be specialized in concrete internal semantic types. isinstance([\"hello\", \"world\"], Seq[Word]) True Specialized classes are exactly the same, by identity: id(Seq[Word]) == id(Seq[Word]) True Specialized classes are subclasses of the raw Seq class: issubclass(Seq[Word], Seq) True And they are subclasses of other specialized classes when the internal types hold the same relationship. issubclass(Seq[Word], Seq[Text]) True issubclass(Seq[Text], Seq[Word]) False They are pretty-printed as well Seq Seq Seq[Word] Seq[Word] Subclasses are also serializable (which requires some non-trivial dark magic on the implementation side): from pickle import dumps, loads loads(dumps(Seq[Word])) Seq[Word] __internal_types = {} @classmethod def _specialize ( cls , internal_type : Type [ SemanticType ]): try : return Seq . __internal_types [ internal_type ] except KeyError : pass class SeqImp ( Seq ): __internal_type__ = internal_type @classmethod def _name ( cls ): return f \"Seq[ { internal_type } ]\" @classmethod def _match ( cls , x ): return isinstance ( x , ( list , tuple )) and internal_type . _match ( x [ 0 ]) @classmethod def _conforms ( cls , other ): if not issubclass ( other , Seq ): return False if other == Seq : return True return issubclass ( cls . __internal_type__ , other . __internal_type__ ) @classmethod def _specialize ( cls , * args , ** kwargs ): raise TypeError ( \"Cannot specialize more a `Seq` type.\" ) @classmethod def _reduce ( cls ): return Seq . _specialize , ( internal_type ,) Seq . __internal_types [ internal_type ] = SeqImp return SeqImp Now let's move to the algebraic types, vectors, matrices, and tensors. These wrap numpy arrays of different dimensionalities We'll have three different semantic labels for each tensor: dimensionality (an integer), internal type, and a dense/sparse flag. from numpy import ndarray from scipy.sparse.base import spmatrix These instances represent the two types of tensorial structure. class TensorStructure : def __init__ ( self , base_class : type , name : str ) -> None : self . base_class = base_class self . name = name def __repr__ ( self ) -> str : return self . name def match ( self , x ): return isinstance ( x , self . base_class ) def __eq__ ( self , o : object ) -> bool : return isinstance ( o , TensorStructure ) and o . name == self . name def __hash__ ( self ) -> int : return hash ( self . name ) Dense = TensorStructure ( ndarray , \"Dense\" ) Sparse = TensorStructure ( spmatrix , \"Sparse\" ) class TensorData : def __init__ ( self , dtype_label : str , name : str ) -> None : self . dtype_label = dtype_label self . name = name def __repr__ ( self ) -> str : return self . name def match ( self , x ): return x . dtype . kind == self . dtype_label def __eq__ ( self , o : object ) -> bool : return isinstance ( o , TensorData ) and o . name == self . name def __hash__ ( self ) -> int : return hash ( self . name ) Categorical = TensorData ( \"U\" , \"Categorical\" ) Continuous = TensorData ( \"f\" , \"Continuous\" ) Discrete = TensorData ( \"i\" , \"Discrete\" ) We want the abstract Tensor type to be specializable using the notation: Tensor[3, Category, Dense] . This requires us to implement the _specialize just like we did with Seq . The internal class will in turn implement _match accordingly to how those values are defined. One special thing we want to do is to let some of these semantic flags undefined (using None ) such that Tensor[2, None, Dense] represents any structure that can have two dimensions no matter the internal type. And we want issubclass(...) to work in a way that Tensor[2, Categorical, Dense] is a subclass to Tensor[2, None, Dense] . For this purpose we will redefine _conforms to match according to how those semantic flags are defined. class Tensor ( SemanticType ): Note \"\"\"Represents an abstract tensor type. Can be specialized into more concrete types. They support type inference from numpy: import numpy as np a_matrix = np.ones(shape=(2,2)) isinstance(a_matrix, Tensor) True isinstance(a_matrix, Tensor[1, None, None]) False isinstance(a_matrix, Tensor[2, None, None]) True isinstance(a_matrix, Tensor[2, Continuous, Dense]) True isinstance(a_matrix, Tensor[2, Continuous, Sparse]) False Tensor types also respect a special definition of subclass in which more specialized classes are defined as subclasses of less specialized counterparts. issubclass(Tensor[2, Continuous, Sparse], Tensor[2, None, None]) True issubclass(Tensor[2, Continuous, Sparse], Tensor[2, Continuous, None]) True issubclass(Tensor[2, Continuous, Sparse], Tensor[2, None, Dense]) False issubclass(Tensor[2, Continuous, Sparse], Tensor[3, None, None]) False Each specific specialization is a singleton class: id(Tensor[2, Continuous, Dense]) == id(Tensor[2, Continuous, Dense]) True Tensor types are also serializable: from pickle import loads, dumps loads(dumps(Tensor[2, Continuous, Dense])) Tensor[2, Continuous, Dense] __internal_types = {} @classmethod def _match ( self , x ): return isinstance ( x , ( ndarray , spmatrix )) @classmethod def _specialize ( cls , dimension : int , internal_type : TensorData , structure : TensorStructure ): try : return Tensor . __internal_types [( dimension , internal_type , structure )] except KeyError : pass class TensorImp ( Tensor ): __flags = ( dimension , internal_type , structure ) @classmethod def _name ( cls ): return f \"Tensor[ { cls . __flags [ 0 ] } , { cls . __flags [ 1 ] } , { cls . __flags [ 2 ] } ]\" @classmethod def _match ( cls , x ): if not super () . _match ( x ): return False if dimension is not None and ( len ( x . shape ) != dimension ): return False if internal_type is not None and not internal_type . match ( x ): return False if structure is not None and not structure . match ( x ): return False return True @classmethod def _conforms ( cls , other : type ) -> bool : if other == Tensor : return True if not hasattr ( other , \"_TensorImp__flags\" ): return False for fmine , fother in zip ( cls . __flags , other . __flags ): if fother is None : continue if fmine is None : return False if fmine != fother : return False return True @classmethod def _reduce ( cls ): To reduce Tensor implementations for pickling return Tensor . _specialize , ( dimension , internal_type , structure ) Tensor . __internal_types [( dimension , internal_type , structure )] = TensorImp return TensorImp Now that we have the basic tensorial type implemented, we can add some aliases here. These aliases mostly serve for SemanticType.infer to work, and also to simplify imports, but keep in mind that anywhere we use Vector we just as well use Tensor[1, None, None] , as they are exactly the same class. Vector = Tensor [ 1 , None , None ] VectorContinuous = Tensor [ 1 , Continuous , None ] VectorCategorical = Tensor [ 1 , Categorical , Dense ] VectorDiscrete = Tensor [ 1 , Discrete , Dense ] Matrix = Tensor [ 2 , None , None ] MatrixContinuous = Tensor [ 2 , Continuous , None ] MatrixContinuousDense = Tensor [ 2 , Continuous , Dense ] MatrixContinuousSparse = Tensor [ 2 , Continuous , Sparse ] MatrixCategorical = Tensor [ 2 , Categorical , Dense ] MatrixDiscrete = Tensor [ 2 , Discrete , Dense ] \ud83d\udcdd Makes no sense to have sparse discrete or categorical tensors as you cannot have missing categories. Tensor3 = Tensor [ 3 , Continuous , Dense ] Tensor4 = Tensor [ 4 , Continuous , Dense ] Finally we define the publicly export classes __all__ = [ \"SemanticType\" , \"Seq\" , \"Text\" , \"Document\" , \"Sentence\" , \"Word\" , \"Label\" , \"Postag\" , \"Chunktag\" , \"Stem\" , \"Synset\" , \"Sentiment\" , \"FeatureSet\" , \"Tensor\" , \"Vector\" , \"VectorContinuous\" , \"VectorCategorical\" , \"VectorDiscrete\" , \"Matrix\" , \"MatrixContinuous\" , \"MatrixContinuousDense\" , \"MatrixContinuousSparse\" , \"MatrixCategorical\" , \"MatrixDiscrete\" , \"Tensor3\" , \"Tensor4\" , \"Dense\" , \"Sparse\" , \"Categorical\" , \"Continuous\" , \"Discrete\" , ] if __name__ == \"__main__\" : import doctest doctest . testmod ()","title":"Autogoal.kb. semantics"},{"location":"api/autogoal.logging.__init__/","text":"import logging import warnings from rich.logging import RichHandler from rich.console import Console _CONSOLE = Console () def setup ( level = \"INFO\" ): logging . basicConfig ( level = level , format = \" %(message)s \" , datefmt = \"[ %X ]\" , handlers = [ RichHandler ( rich_tracebacks = True , console = _CONSOLE )], ) logging . captureWarnings ( True ) def logger () -> logging . Logger : return logging . getLogger ( \"autogoal\" ) def console () -> Console : return _CONSOLE","title":"Autogoal.logging.  init  "},{"location":"api/autogoal.ml.__init__/","text":"from autogoal.ml._automl import AutoML","title":"Autogoal.ml.  init  "},{"location":"api/autogoal.ml._automl/","text":"import io import pickle import statistics import numpy as np from autogoal.contrib import find_classes from autogoal.kb import build_pipeline_graph , SemanticType from autogoal.ml.metrics import accuracy from autogoal.search import PESearch from autogoal.utils import nice_repr @nice_repr class AutoML : Note Predefined pipeline search with automatic type inference. An AutoML instance represents a general-purpose machine learning algorithm, that can be applied to any input and output. def __init__ ( self , input = None , output = None , random_state = None , search_algorithm = None , search_iterations = 100 , include_filter = \".*\" , exclude_filter = None , validation_split = 0.3 , errors = \"warn\" , cross_validation = \"median\" , cross_validation_steps = 3 , registry = None , score_metric = None , ** search_kwargs ): self . input = input self . output = output self . search_algorithm = search_algorithm or PESearch self . search_iterations = search_iterations self . include_filter = include_filter self . exclude_filter = exclude_filter self . validation_split = validation_split self . errors = errors self . cross_validation = cross_validation self . cross_validation_steps = cross_validation_steps self . registry = registry self . random_state = random_state self . score_metric = score_metric or accuracy self . search_kwargs = search_kwargs self . _unpickled = False if random_state : np . random . seed ( random_state ) def _check_fitted ( self ): if not hasattr ( self , \"best_pipeline_\" ): raise TypeError ( \"This operation cannot be performed on an unfitted AutoML instance. Call `fit` first.\" ) def make_pipeline_builder ( self ): registry = self . registry or find_classes ( include = self . include_filter , exclude = self . exclude_filter ) return build_pipeline_graph ( input_types = self . input , output_type = self . output , registry = registry , ) def fit ( self , X , y , ** kwargs ): self . input = self . _input_type ( X ) self . output = self . _output_type ( y ) search = self . search_algorithm ( self . make_pipeline_builder (), self . make_fitness_fn ( X , y ), random_state = self . random_state , errors = self . errors , ** self . search_kwargs , ) self . best_pipeline_ , self . best_score_ = search . run ( self . search_iterations , ** kwargs ) self . fit_pipeline ( X , y ) def fit_pipeline ( self , X , y ): self . _check_fitted () self . best_pipeline_ . send ( \"train\" ) self . best_pipeline_ . run ( X , y ) self . best_pipeline_ . send ( \"eval\" ) def save ( self , fp : io . BytesIO ): Note Serializes the AutoML instance. self . _check_fitted () pickle . Pickler ( fp ) . dump ( self ) @classmethod def load ( self , fp : io . FileIO ) -> \"AutoML\" : Note Deserializes an AutoML instance. After deserialization, the best pipeline found is ready to predict. automl = pickle . Unpickler ( fp ) . load () if not isinstance ( automl , AutoML ): raise ValueError ( \"The serialized file does not contain an AutoML instance.\" ) return automl def score ( self , X , y ): self . _check_fitted () y_pred = self . best_pipeline_ . run ( X , np . zeros_like ( y )) return self . score_metric ( y , y_pred ) def _input_type ( self , X ): return self . input or SemanticType . infer ( X ) def _output_type ( self , y ): return self . output or SemanticType . infer ( y ) def make_fitness_fn ( self , X , y ): y = np . asarray ( y ) def fitness_fn ( pipeline ): scores = [] for _ in range ( self . cross_validation_steps ): len_x = len ( X ) if isinstance ( X , list ) else X . shape [ 0 ] indices = np . arange ( 0 , len_x ) np . random . shuffle ( indices ) split_index = int ( self . validation_split * len ( indices )) train_indices = indices [: - split_index ] test_indices = indices [ - split_index :] if isinstance ( X , list ): X_train , y_train , X_test , y_test = ( [ X [ i ] for i in train_indices ], y [ train_indices ], [ X [ i ] for i in test_indices ], y [ test_indices ], ) else : X_train , y_train , X_test , y_test = ( X [ train_indices ], y [ train_indices ], X [ test_indices ], y [ test_indices ], ) pipeline . send ( \"train\" ) pipeline . run ( X_train , y_train ) pipeline . send ( \"eval\" ) y_pred = pipeline . run ( X_test , None ) scores . append ( self . score_metric ( y_test , y_pred )) return getattr ( statistics , self . cross_validation )( scores ) return fitness_fn def predict ( self , X ): self . _check_fitted () return self . best_pipeline_ . run ( X , None )","title":"Autogoal.ml. automl"},{"location":"api/autogoal.ml._metalearning/","text":"import uuid import abc import functools import warnings import json import collections import numpy as np from typing import List from autogoal.search import Logger from autogoal.utils import nice_repr from autogoal import sampling from sklearn.feature_extraction import DictVectorizer class DatasetFeatureLogger ( Logger ): def __init__ ( self , X , y = None , extractor = None , output_file = \"metalearning.json\" , problem_features = None , environment_features = None , ): self . extractor = extractor or DatasetFeatureExtractor () self . X = X self . y = y self . run_id = str ( uuid . uuid4 ()) self . output_file = output_file self . problem_features = problem_features or {} self . environment_features = environment_features or {} def begin ( self , generations , pop_size ): self . dataset_features_ = self . extractor . extract_features ( self . X , self . y ) def eval_solution ( self , solution , fitness ): if not hasattr ( solution , \"sampler_\" ): raise ( \"Cannot log if the underlying algorithm is not PESearch\" ) sampler = solution . sampler_ features = { k : v for k , v in sampler . _updates . items () if isinstance ( k , str )} feature_types = { k : repr ( v ) for k , v in sampler . _model . items () if k in features } info = SolutionInfo ( uuid = self . run_id , fitness = fitness , problem_features = dict ( self . dataset_features_ , ** self . problem_features ), environment_features = dict ( self . environment_features ), pipeline_features = features , feature_types = feature_types , ) . to_dict () with open ( self . output_file , \"a\" ) as fp : fp . write ( json . dumps ( info ) + \" \\n \" ) class DatasetFeatureExtractor : def __init__ ( self , features_extractors = None ): self . feature_extractors = list ( features_extractors or _EXTRACTORS ) def extract_features ( self , X , y = None ): features = {} for extractor in self . feature_extractors : features . update ( ** extractor ( X , y )) return features _EXTRACTORS = [] def feature_extractor ( func ): @functools . wraps ( func ) def wrapper ( X , y = None ): try : result = func ( X , y ) except : result = None raise return { func . __name__ : result } _EXTRACTORS . append ( wrapper ) return wrapper Feature extractor methods @feature_extractor def is_supervised ( X , y = None ): return y is not None @feature_extractor def dimensionality ( X , y = None ): d = 1 for di in X . shape [ 1 :]: d *= di return d @feature_extractor def training_examples ( X , y = None ): try : return X . shape [ 0 ] except : return len ( X ) @feature_extractor def has_numeric_features ( X , y = None ): return any ([ xi for xi in X [ 0 ] if isinstance ( xi , ( float , int ))]) @feature_extractor def numeric_variance ( X , y = None ): return X . std () @feature_extractor def average_number_of_words ( X , y = None ): return sum ( len ( sentence . split ( \" \" )) for sentence in X ) / len ( X ) @feature_extractor def has_text_features ( X , y = None ): return isinstance ( X [ 0 ], str ) @nice_repr class SolutionInfo : def __init__ ( self , uuid : str , problem_features : dict , pipeline_features : dict , environment_features : dict , feature_types : dict , fitness : float , ): self . problem_features = problem_features self . pipeline_features = pipeline_features self . environment_features = environment_features self . feature_types = feature_types self . fitness = fitness self . uuid = uuid def to_dict ( self ): return self . __dict__ @staticmethod def from_dict ( d ): return SolutionInfo ( ** d ) class LearnerMedia : def __init__ ( self , problem , solutions : List [ SolutionInfo ], beta = 1 ): self . solutions = solutions self . problem = problem self . beta = beta def initialize ( self ): raise NotImplementedError ( \"We need to refactor to not depend on DictVectorizer\" ) self . best_fitness = collections . defaultdict ( lambda : 0 ) self . all_features = {} for i in self . solutions : self . best_fitness [ i . uuid ] = max ( self . best_fitness [ i . uuid ], i . fitness ) for feature in i . pipeline_features : self . all_features [ feature ] = None self . vect = DictVectorizer ( sparse = False ) self . vect . fit ([ self . problem ]) self . weights_solution = self . calculate_weight_examples ( self . solutions ) def compute_all_features ( self ): self . initialize () for feature in list ( self . all_features ): self . all_features [ feature ] = self . compute_feature ( feature ) print ( feature , \"=\" , self . all_features [ feature ]) def compute_feature ( self , feature ): Note \"\"\"Select for training all solutions where is used the especific feature. Predict the media of the parameter value. find the relevant solutions, that contain the production to predict important_solutions = [] feature_prototype = None for i , w in zip ( self . solutions , self . weights_solution ): if feature in i . pipeline_features : for value in i . pipeline_features [ feature ]: important_solutions . append (( value , w )) if feature_prototype is None : feature_prototype = eval ( i . feature_types [ feature ], sampling . __dict__ , {} ) if feature_prototype is None : return None return feature_prototype . weighted ( important_solutions ) def calculate_weight_examples ( self , solutions : List [ SolutionInfo ]): Note \"\"\"Calcule a weight of each example considering the fitness and the similariti with the actual problem met = fitness * (similarity)^beta m\u00e9trica utilizada en active learning para combinar informativeness with representativeness weights = [] for info in solutions : normalize fitness info . fitness = self . normalize_fitness ( info ) if info . fitness == 0 : continue calculate similarity sim = self . similarity_cosine ( info . problem_features ) calculate metric for weight weights . append ( info . fitness * ( sim ) ** self . beta ) return weights def normalize_fitness ( self , info : SolutionInfo ): Note \"\"\"Normalize the fitness with respect to the best solution in the problem where that solution is evaluated return info . fitness / self . best_fitness [ info . uuid ] def similarity_cosine ( self , other_problem ): Note \"\"\"Caculate the cosine similarity for a particular solution problem(other problem) and the problem analizing x = self . vect . transform ( other_problem )[ 0 ] y = self . vect . transform ( self . problem )[ 0 ] return np . dot ( x , y ) / ( np . dot ( x , x ) ** 0.5 * np . dot ( y , y ) ** 0.5 ) def similarity_learning ( self , other_problem ): Note \"\"\" Implementar una espicie de encoding para los feature de los problemas raise NotImplementedError ()","title":"Autogoal.ml. metalearning"},{"location":"api/autogoal.ml.metrics/","text":"import inspect import numpy as np METRICS = [] def register_metric ( func ): METRICS . append ( func ) return func def find_metric ( * types ): for metric_func in METRICS : signature = inspect . signature ( metric_func ) if len ( types ) != len ( signature . parameters ): break for type_if , type_an in zip ( types , signature . parameters ): if not conforms ( type_an . annotation , type_if ): break return metric_func raise ValueError ( \"No metric found for types: %r \" % types ) def accuracy ( ytrue , ypred ) -> float : return np . mean ([ 1 if yt == yp else 0 for yt , yp in zip ( ytrue , ypred )])","title":"Autogoal.ml.metrics"},{"location":"api/autogoal.sampling.__init__/","text":"import math import random import statistics import pickle import numpy as np from typing import Dict , List , Sequence import abc from autogoal.utils import nice_repr class Sampler : Note Provides methods to obtain random samples with various distributions. Can receive a random_state to guarantee the same values are obtained in two different instantiations. def __init__ ( self , * , random_state : int = None ): self . rand = random . Random ( random_state ) def choice ( self , options , handle = None ): Note Returns one of the options. Examples \u00b6 ```python sampler = Sampler(random_state=0) [sampler.choice(['A', 'B', 'C']) for _ in range(5)] ['B', 'B', 'A', 'B', 'C'] ``` return self . categorical ( options , handle = handle ) def distribution ( self , name : str , handle = None , ** kwargs ): Note Shortcut function for generating from a distribution, either discrete , continuous , boolean or categorical . try : return getattr ( self , name )( handle = handle , ** kwargs ) except AttributeError : raise ValueError ( \"Unrecognized distribution name: %s \" % name ) def discrete ( self , min = 0 , max = 10 , handle = None ): Note Returns a discrete value between min and max . Examples \u00b6 ```python sampler = Sampler(random_state=0) [sampler.discrete(0, 10) for _ in range(10)] [6, 6, 0, 4, 8, 7, 6, 4, 7, 5] ``` return self . rand . randint ( min , max ) def continuous ( self , min = 0 , max = 1 , handle = None ): Note Returns a continuous value between min and max . Examples \u00b6 ```python sampler = Sampler(random_state=0) [round(sampler.continuous(0, 10), 2) for _ in range(10)] [8.44, 7.58, 4.21, 2.59, 5.11, 4.05, 7.84, 3.03, 4.77, 5.83] ``` return self . rand . uniform ( min , max ) def boolean ( self , handle = None ): Note Returns a boolean value. Examples \u00b6 ```python sampler = Sampler(random_state=0) [sampler.boolean() for _ in range(10)] [False, False, True, True, False, True, False, True, True, False] ``` return self . rand . uniform ( 0 , 1 ) < 0.5 def categorical ( self , options , handle = None ): Note Returns one of the options. The difference between choice and categorical is evident in more specialized classes of Sampler . In the default implementation, their behavior is exactly the same. Examples \u00b6 ```python sampler = Sampler(random_state=0) [sampler.categorical(['A', 'B', 'C']) for _ in range(5)] ['B', 'B', 'A', 'B', 'C'] ``` return self . rand . choice ( options ) class ModelSampler ( Sampler ): Note A sampler that builds and uses an internal probabilistic model to generate values with a non-uniform probability. For the model to work, the handler parameter in each sampling method must be suplied, otherwise it behaves exactly as the standard Sampler . def __init__ ( self , model : Dict = None , ** kwargs ): super () . __init__ ( ** kwargs ) self . _model : Dict = {} if model is None else model self . _updates : Dict = {} @property def model ( self ): return self . _model @property def updates ( self ): return self . _updates def _get_model_params ( self , handle , default ): if handle in self . _model : return self . _model [ handle ] else : self . _model [ handle ] = default return default def _register_update ( self , handle , result ): if handle not in self . _updates : self . _updates [ handle ] = [] self . _updates [ handle ] . append ( result ) return result def _clamp ( self , x , a , b ): if x < a : return a if x > b : return b return x def choice ( self , options , handle = None ): if handle is not None : return self . categorical ( options , handle ) weights = [ self . _get_model_params ( option , UnormalizedWeightParam ( value = 1 )) for option in options ] idx = self . rand . choices ( range ( len ( options )), weights = [ w . value for w in weights ], k = 1 )[ 0 ] option = options [ idx ] self . _register_update ( option , 1 ) return option def discrete ( self , min = 0 , max = 10 , handle = None ): if handle is None : return super () . discrete ( min , max , handle ) params = self . _get_model_params ( handle , MeanDevParam ( mean = ( min + max ) / 2 , dev = ( max - min )) ) value = self . _clamp ( int ( self . rand . gauss ( params . mean , params . dev )), min , max ) return self . _register_update ( handle , value ) def continuous ( self , min = 0 , max = 1 , handle = None ): if handle is None : return super () . continuous ( min , max , handle ) params = self . _get_model_params ( handle , MeanDevParam ( mean = ( min + max ) / 2 , dev = ( max - min )) ) value = self . _clamp ( self . rand . gauss ( params . mean , params . dev ), min , max ) return self . _register_update ( handle , value ) def boolean ( self , handle = None ): if handle is None : return super () . boolean ( handle ) params = self . _get_model_params ( handle , WeightParam ( value = 0.5 )) value = self . rand . uniform ( 0 , 1 ) < params . value return self . _register_update ( handle , value ) def categorical ( self , options , handle = None ): if handle is None : return super () . categorical ( options , handle ) params = self . _get_model_params ( handle , DistributionParam ( weights = [ 1 for _ in options ]) ) idx = self . rand . choices ( range ( len ( options )), weights = params . weights , k = 1 )[ 0 ] return options [ self . _register_update ( handle , idx )] class ReplaySampler : Note A sampler that records the generated values and then can replay the same outputs in the same order. One of the most interesting use cases for ReplaySampler is in conjunction with context free or graph grammars, for generating complex objects. You can pass a sampler wrapped in a ReplaySampler during generation, and then reuse it later for generating the same object or graph. Examples \u00b6 First, instantiate a ReplaySampler with an internal Sampler instance and use it normally. ```python sampler = ReplaySampler(Sampler(random_state=0)) [sampler.discrete(0,10) for _ in range(10)] [6, 6, 0, 4, 8, 7, 6, 4, 7, 5] ``` Then call the replay method and reuse the same values. replay() returns the same instance, to enable chaining method calls. ```python sampler.replay() [sampler.discrete(0,10) for _ in range(5)] [6, 6, 0, 4, 8] [sampler.discrete(0,10) for _ in range(5)] [7, 6, 4, 7, 5] ``` If you try to use it in a different way as originally, it will complain. ```python sampler.replay().discrete(0,5) Traceback (most recent call last): ... TypeError: Invalid invocation of discrete with args=(0, 5) , replay history says args='(0, 10)'. sampler.replay().boolean() Traceback (most recent call last): ... TypeError: Invalid invocation of boolean , replay history says discrete comes next. ``` RECORD = \"record\" REPLAY = \"replay\" def __init__ ( self , sampler ): self . sampler = sampler self . _mode = ReplaySampler . RECORD self . _history = [] self . _current_history = [] def _run ( self , method , * args , ** kwargs ): if self . _mode == ReplaySampler . RECORD : result = getattr ( self . sampler , method )( * args , ** kwargs ) self . _history . append ( dict ( method = method , args = repr ( args ), kwargs = repr ( kwargs ), result = result ) ) return result elif self . _mode == ReplaySampler . REPLAY : if not self . _current_history : raise TypeError ( f \"Invalid invocation of ` { method } `, replay history is empty. Maybe you forgot to call `replay`?\" ) top = self . _current_history [ 0 ] if top [ \"method\" ] != method : raise TypeError ( f \"Invalid invocation of ` { method } `, \" f \"replay history says { top [ 'method' ] } comes next.\" ) if top [ \"args\" ] != repr ( args ): raise TypeError ( f \"Invalid invocation of ` { method } ` with `args= { repr ( args ) } `, \" f \"replay history says args= { repr ( top [ 'args' ]) } .\" ) if top [ \"kwargs\" ] != repr ( kwargs ): raise TypeError ( f \"Invalid invocation of ` { method } ` with `kwargs= { repr ( kwargs ) } `, \" f \"replay history says kwargs= { repr ( top [ 'kwargs' ]) } .\" ) self . _current_history . pop ( 0 ) return top [ \"result\" ] def replay ( self ) -> \"ReplaySampler\" : self . _mode = ReplaySampler . REPLAY self . _current_history = list ( self . _history ) return self def save ( self , fp ): Note Saves the state of a ReplaySampler to a stream. It must be in replay mode. You are responsible for opening and closing the stream yourself. Examples \u00b6 In this example we create a sampler, and save its state into a StringIO stream to be able to see what's being saved. ```python sampler = ReplaySampler(Sampler(random_state=0)) [sampler.discrete(0, 10) for _ in range(3)] [6, 6, 0] import io fp = io.BytesIO() sampler.replay().save(fp) len(fp.getvalue()) 183 ``` if self . _mode != ReplaySampler . REPLAY : raise TypeError ( \"A sampler must be in replay mode, i.e., call the `replay()` method.\" ) pickle . Pickler ( fp ) . dump ( self . _history ) @staticmethod def load ( fp ) -> \"ReplaySampler\" : Note Creates a ReplaySampler from a stream and returns it already in replay mode. You are responsible for opening and closing the stream yourself. Examples \u00b6 ```python sampler = ReplaySampler(Sampler(random_state=1)) [sampler.discrete(0, 10) for _ in range(10)] [2, 9, 1, 4, 1, 7, 7, 7, 10, 6] import io fp = io.BytesIO() sampler.replay().save(fp) fp.seek(0) 0 other_sampler = ReplaySampler.load(fp) [other_sampler.discrete(0, 10) for _ in range(5)] [2, 9, 1, 4, 1] [other_sampler.discrete(0, 10) for _ in range(5)] [7, 7, 7, 10, 6] history = pickle . Unpickler ( fp ) . load () sampler = ReplaySampler ( None ) sampler . _history = history return sampler . replay () def choice ( self , * args , ** kwargs ): return self . _run ( \"choice\" , * args , ** kwargs ) def distribution ( self , * args , ** kwargs ): return self . _run ( \"distribution\" , * args , ** kwargs ) def discrete ( self , * args , ** kwargs ): return self . _run ( \"discrete\" , * args , ** kwargs ) def continuous ( self , * args , ** kwargs ): return self . _run ( \"continuous\" , * args , ** kwargs ) def boolean ( self , * args , ** kwargs ): return self . _run ( \"boolean\" , * args , ** kwargs ) def categorical ( self , * args , ** kwargs ): return self . _run ( \"categorical\" , * args , ** kwargs ) def __getattr__ ( self , attr ): if attr == \"sampler\" : return self . __dict__ . get ( \"sampler\" ) return getattr ( self . sampler , attr ) class ModelParam ( metaclass = abc . ABCMeta ): @abc . abstractmethod def update ( self , alpha : float , updates ) -> \"ModelParam\" : pass @nice_repr class UnormalizedWeightParam ( ModelParam ): def __init__ ( self , value ): self . value = value def update ( self , alpha : float , updates : list ) -> \"UnormalizedWeightParam\" : return UnormalizedWeightParam ( self . value + alpha * sum ( updates )) def weighted ( self , solutions ): result = 0 for s , w in solutions : result += s * w return UnormalizedWeightParam ( 1 + result ) @nice_repr class DistributionParam ( ModelParam ): def __init__ ( self , weights ): total = sum ( weights ) or 1 self . weights = [ w / total for w in weights ] def update ( self , alpha : float , updates : list ) -> \"DistributionParam\" : weights = list ( self . weights ) for i in updates : weights [ i ] += alpha return DistributionParam ( weights ) def weighted ( self , solutions ): weights = [ 1 ] * len ( self . weights ) for s , w in solutions : weights [ s ] += w return DistributionParam ( weights ) @nice_repr class MeanDevParam ( ModelParam ): def __init__ ( self , mean , dev , * , initial_params = None ): self . mean = mean self . dev = dev if initial_params is None : self . initial_params = ( mean , dev ) else : self . initial_params = initial_params def update ( self , alpha : float , updates ) -> \"MeanDevParam\" : new_mean = statistics . mean ( updates ) new_dev = statistics . stdev ( updates , new_mean ) if len ( updates ) > 1 else 0 return MeanDevParam ( mean = self . mean * ( 1 - alpha ) + new_mean * alpha , dev = self . dev * ( 1 - alpha ) + new_dev * alpha , initial_params = self . initial_params , ) def weighted ( self , solutions ): values = np . asarray ( [ s for s , w in solutions ] + [ self . initial_params [ 0 ] - 2 * self . initial_params [ 1 ], self . initial_params [ 0 ] + 2 * self . initial_params [ 1 ], ] ) weights = np . asarray ([ w for s , w in solutions ] + [ 1 , 1 ]) average = np . average ( values , weights = weights ) variance = np . average (( values - average ) ** 2 , weights = weights ) return MeanDevParam ( average , math . sqrt ( variance ), initial_params = self . initial_params ) @nice_repr class WeightParam ( ModelParam ): def __init__ ( self , value ): self . value = value def update ( self , alpha : float , updates ) -> \"WeightParam\" : new_value = statistics . mean ( updates ) return WeightParam ( value = self . value * ( 1 - alpha ) + new_value * alpha ) def weighted ( self , solutions ): values = np . asarray ([ s for s , w in solutions ] + [ 0 , 1 ]) weights = np . asarray ([ w for s , w in solutions ] + [ 1 , 1 ]) return WeightParam ( np . average ( values , weights = weights )) def update_model ( model , updates , alpha : float = 1 ): new_model = {} for handle , params in model . items (): upd = updates . get ( handle ) if upd is None : new_model [ handle ] = params else : new_model [ handle ] = params . update ( alpha , upd ) return new_model def _argsort ( l ): taken from https://stackoverflow.com/questions/6422700 return sorted ( range ( len ( l )), key = l . __getitem__ ) def best_indices ( values : List , k : int = 1 , maximize : bool = False ) -> List [ int ]: Note Computes the k best indices from values, i.e., the indices of the values that are the top minimum (or maximum). Parameters \u00b6 values: List : Values to compare, must be a sortable type (e.g., int , float , ...). k: int : Number of indices to calculate. Defaults to 1 . maximize: bool : Whether to compute the maximum or minimum values. Defaults to False , i.e., minimize by default. Returns: \u00b6 indices: List[int] : list of the indices that correspond to maximum (or minimum) values in values . Examples: \u00b6 ```python best_indices([.33, 0.12, 0.55, 0.09], k=2) [1, 3] best_indices([.33, 0.12, 0.55, 0.09], k=3, maximize=True) [0, 1, 2] best_indices([.33, 0.12, 0.55, 0.09]) [3] ``` Note Note that indices are returned in their original order, not in the order in which the values would be sorted themselves. indices = _argsort ( _argsort ( values )) if maximize : threshold = len ( values ) - k return [ i for i in range ( len ( values )) if indices [ i ] >= threshold ] else : threshold = k return [ i for i in range ( len ( values )) if indices [ i ] < threshold ] def merge_updates ( * updates : Sequence [ Dict ]) -> Dict : Note Merges a bunch of update dicts from ModelSampler into a single dictionary. Parameters: \u00b6 updates: Sequence[Dict] : Sequence of update dictionaries obtained from calling ModelSampler.updates . Returns: \u00b6 update: Dict : A single dictionary with the combined (appended) updates. Examples: \u00b6 ```python up1 = {'a': [1]} up2 = {'b': [2,3]} up3 = {'a': [4]} merge_updates(up1, up2, up3) {'a': [1, 4], 'b': [2, 3]} ``` result = {} for upd in updates : for key , value in upd . items (): if not key in result : result [ key ] = [] result [ key ] . extend ( value ) return result class ExhaustiveSampler : Note Performs an exhaustive sampling of the parameter space. The way this sampler works is by building an explicit representation of a parameter space in the form a search tree. As you sample values it will create nodes that represent each decision and the possible outcomes. We'll build a tree that represents all possible combinations in the parameter space. Each node in the tree contains set of values, and the children correspond to the nodes that represent the subsequent options for each value. For example, if our parameter space has two variables, X=[1,2,3] and Y=['a', 'b'] , we'll represent it in a tree that looks like this: [ 1 , 2 , 3 ] | | | [a,b] [a,b] [a,b] / \\ / \\ / \\ 1,a 1,b 2,a 2,b 3,a 3,b In each node we'll store a handle and a set of values for the distribution. The challenge here is build the paramter tree at the same time that we are sampling. Since we don't know beforehand what of the structure of the parameter space, we have to discover it as the different sampling methods are being invoked. However, since sampling is a never-ending process, we will have to explicitely tell this sampler when we finished with one instance. Otherwise we can't know when did we reach a leaf. def __init__ ( self ) -> None : self . _root = ExhaustiveSampler . Node () self . _current_node = self . _root def _sample_next ( self , distribution , params , handle ): At any moment when we call a sampling method, we are at some node in the parameter space. In the simplest case that node is not initialized, i.e., we have never sampled from it before. if not self . _current_node . is_initialized (): self . _current_node . initialize ( distribution , params , handle ) Then we'll sample from that node, which in this case will only return the next value. At the same time, we'll recurse down the corresponding children. self . _current_node , value = self . _current_node . sample () return value class Node : def is_initialized ( self ) -> bool : return hasattr ( self , \"handle\" ) def initialize ( self , distribution , params , handle ) -> None : self . handle = handle self . values = getattr ( self , f \"initialize_ { distribution } \" )( ** params ) In this dictionary we will store as values the nodes that represents the distributions that will be invoked after returning the corresponding result stored at each key. self . children = {} def sample ( self ): pass","title":"Autogoal.sampling.  init  "},{"location":"api/autogoal.sampling.__init__/#examples","text":"```python sampler = Sampler(random_state=0) [sampler.choice(['A', 'B', 'C']) for _ in range(5)] ['B', 'B', 'A', 'B', 'C'] ``` return self . categorical ( options , handle = handle ) def distribution ( self , name : str , handle = None , ** kwargs ): Note Shortcut function for generating from a distribution, either discrete , continuous , boolean or categorical . try : return getattr ( self , name )( handle = handle , ** kwargs ) except AttributeError : raise ValueError ( \"Unrecognized distribution name: %s \" % name ) def discrete ( self , min = 0 , max = 10 , handle = None ): Note Returns a discrete value between min and max .","title":"Examples"},{"location":"api/autogoal.sampling.__init__/#examples_1","text":"```python sampler = Sampler(random_state=0) [sampler.discrete(0, 10) for _ in range(10)] [6, 6, 0, 4, 8, 7, 6, 4, 7, 5] ``` return self . rand . randint ( min , max ) def continuous ( self , min = 0 , max = 1 , handle = None ): Note Returns a continuous value between min and max .","title":"Examples"},{"location":"api/autogoal.sampling.__init__/#examples_2","text":"```python sampler = Sampler(random_state=0) [round(sampler.continuous(0, 10), 2) for _ in range(10)] [8.44, 7.58, 4.21, 2.59, 5.11, 4.05, 7.84, 3.03, 4.77, 5.83] ``` return self . rand . uniform ( min , max ) def boolean ( self , handle = None ): Note Returns a boolean value.","title":"Examples"},{"location":"api/autogoal.sampling.__init__/#examples_3","text":"```python sampler = Sampler(random_state=0) [sampler.boolean() for _ in range(10)] [False, False, True, True, False, True, False, True, True, False] ``` return self . rand . uniform ( 0 , 1 ) < 0.5 def categorical ( self , options , handle = None ): Note Returns one of the options. The difference between choice and categorical is evident in more specialized classes of Sampler . In the default implementation, their behavior is exactly the same.","title":"Examples"},{"location":"api/autogoal.sampling.__init__/#examples_4","text":"```python sampler = Sampler(random_state=0) [sampler.categorical(['A', 'B', 'C']) for _ in range(5)] ['B', 'B', 'A', 'B', 'C'] ``` return self . rand . choice ( options ) class ModelSampler ( Sampler ): Note A sampler that builds and uses an internal probabilistic model to generate values with a non-uniform probability. For the model to work, the handler parameter in each sampling method must be suplied, otherwise it behaves exactly as the standard Sampler . def __init__ ( self , model : Dict = None , ** kwargs ): super () . __init__ ( ** kwargs ) self . _model : Dict = {} if model is None else model self . _updates : Dict = {} @property def model ( self ): return self . _model @property def updates ( self ): return self . _updates def _get_model_params ( self , handle , default ): if handle in self . _model : return self . _model [ handle ] else : self . _model [ handle ] = default return default def _register_update ( self , handle , result ): if handle not in self . _updates : self . _updates [ handle ] = [] self . _updates [ handle ] . append ( result ) return result def _clamp ( self , x , a , b ): if x < a : return a if x > b : return b return x def choice ( self , options , handle = None ): if handle is not None : return self . categorical ( options , handle ) weights = [ self . _get_model_params ( option , UnormalizedWeightParam ( value = 1 )) for option in options ] idx = self . rand . choices ( range ( len ( options )), weights = [ w . value for w in weights ], k = 1 )[ 0 ] option = options [ idx ] self . _register_update ( option , 1 ) return option def discrete ( self , min = 0 , max = 10 , handle = None ): if handle is None : return super () . discrete ( min , max , handle ) params = self . _get_model_params ( handle , MeanDevParam ( mean = ( min + max ) / 2 , dev = ( max - min )) ) value = self . _clamp ( int ( self . rand . gauss ( params . mean , params . dev )), min , max ) return self . _register_update ( handle , value ) def continuous ( self , min = 0 , max = 1 , handle = None ): if handle is None : return super () . continuous ( min , max , handle ) params = self . _get_model_params ( handle , MeanDevParam ( mean = ( min + max ) / 2 , dev = ( max - min )) ) value = self . _clamp ( self . rand . gauss ( params . mean , params . dev ), min , max ) return self . _register_update ( handle , value ) def boolean ( self , handle = None ): if handle is None : return super () . boolean ( handle ) params = self . _get_model_params ( handle , WeightParam ( value = 0.5 )) value = self . rand . uniform ( 0 , 1 ) < params . value return self . _register_update ( handle , value ) def categorical ( self , options , handle = None ): if handle is None : return super () . categorical ( options , handle ) params = self . _get_model_params ( handle , DistributionParam ( weights = [ 1 for _ in options ]) ) idx = self . rand . choices ( range ( len ( options )), weights = params . weights , k = 1 )[ 0 ] return options [ self . _register_update ( handle , idx )] class ReplaySampler : Note A sampler that records the generated values and then can replay the same outputs in the same order. One of the most interesting use cases for ReplaySampler is in conjunction with context free or graph grammars, for generating complex objects. You can pass a sampler wrapped in a ReplaySampler during generation, and then reuse it later for generating the same object or graph.","title":"Examples"},{"location":"api/autogoal.sampling.__init__/#examples_5","text":"First, instantiate a ReplaySampler with an internal Sampler instance and use it normally. ```python sampler = ReplaySampler(Sampler(random_state=0)) [sampler.discrete(0,10) for _ in range(10)] [6, 6, 0, 4, 8, 7, 6, 4, 7, 5] ``` Then call the replay method and reuse the same values. replay() returns the same instance, to enable chaining method calls. ```python sampler.replay() [sampler.discrete(0,10) for _ in range(5)] [6, 6, 0, 4, 8] [sampler.discrete(0,10) for _ in range(5)] [7, 6, 4, 7, 5] ``` If you try to use it in a different way as originally, it will complain. ```python sampler.replay().discrete(0,5) Traceback (most recent call last): ... TypeError: Invalid invocation of discrete with args=(0, 5) , replay history says args='(0, 10)'. sampler.replay().boolean() Traceback (most recent call last): ... TypeError: Invalid invocation of boolean , replay history says discrete comes next. ``` RECORD = \"record\" REPLAY = \"replay\" def __init__ ( self , sampler ): self . sampler = sampler self . _mode = ReplaySampler . RECORD self . _history = [] self . _current_history = [] def _run ( self , method , * args , ** kwargs ): if self . _mode == ReplaySampler . RECORD : result = getattr ( self . sampler , method )( * args , ** kwargs ) self . _history . append ( dict ( method = method , args = repr ( args ), kwargs = repr ( kwargs ), result = result ) ) return result elif self . _mode == ReplaySampler . REPLAY : if not self . _current_history : raise TypeError ( f \"Invalid invocation of ` { method } `, replay history is empty. Maybe you forgot to call `replay`?\" ) top = self . _current_history [ 0 ] if top [ \"method\" ] != method : raise TypeError ( f \"Invalid invocation of ` { method } `, \" f \"replay history says { top [ 'method' ] } comes next.\" ) if top [ \"args\" ] != repr ( args ): raise TypeError ( f \"Invalid invocation of ` { method } ` with `args= { repr ( args ) } `, \" f \"replay history says args= { repr ( top [ 'args' ]) } .\" ) if top [ \"kwargs\" ] != repr ( kwargs ): raise TypeError ( f \"Invalid invocation of ` { method } ` with `kwargs= { repr ( kwargs ) } `, \" f \"replay history says kwargs= { repr ( top [ 'kwargs' ]) } .\" ) self . _current_history . pop ( 0 ) return top [ \"result\" ] def replay ( self ) -> \"ReplaySampler\" : self . _mode = ReplaySampler . REPLAY self . _current_history = list ( self . _history ) return self def save ( self , fp ): Note Saves the state of a ReplaySampler to a stream. It must be in replay mode. You are responsible for opening and closing the stream yourself.","title":"Examples"},{"location":"api/autogoal.sampling.__init__/#examples_6","text":"In this example we create a sampler, and save its state into a StringIO stream to be able to see what's being saved. ```python sampler = ReplaySampler(Sampler(random_state=0)) [sampler.discrete(0, 10) for _ in range(3)] [6, 6, 0] import io fp = io.BytesIO() sampler.replay().save(fp) len(fp.getvalue()) 183 ``` if self . _mode != ReplaySampler . REPLAY : raise TypeError ( \"A sampler must be in replay mode, i.e., call the `replay()` method.\" ) pickle . Pickler ( fp ) . dump ( self . _history ) @staticmethod def load ( fp ) -> \"ReplaySampler\" : Note Creates a ReplaySampler from a stream and returns it already in replay mode. You are responsible for opening and closing the stream yourself.","title":"Examples"},{"location":"api/autogoal.sampling.__init__/#examples_7","text":"```python sampler = ReplaySampler(Sampler(random_state=1)) [sampler.discrete(0, 10) for _ in range(10)] [2, 9, 1, 4, 1, 7, 7, 7, 10, 6] import io fp = io.BytesIO() sampler.replay().save(fp) fp.seek(0) 0 other_sampler = ReplaySampler.load(fp) [other_sampler.discrete(0, 10) for _ in range(5)] [2, 9, 1, 4, 1] [other_sampler.discrete(0, 10) for _ in range(5)] [7, 7, 7, 10, 6] history = pickle . Unpickler ( fp ) . load () sampler = ReplaySampler ( None ) sampler . _history = history return sampler . replay () def choice ( self , * args , ** kwargs ): return self . _run ( \"choice\" , * args , ** kwargs ) def distribution ( self , * args , ** kwargs ): return self . _run ( \"distribution\" , * args , ** kwargs ) def discrete ( self , * args , ** kwargs ): return self . _run ( \"discrete\" , * args , ** kwargs ) def continuous ( self , * args , ** kwargs ): return self . _run ( \"continuous\" , * args , ** kwargs ) def boolean ( self , * args , ** kwargs ): return self . _run ( \"boolean\" , * args , ** kwargs ) def categorical ( self , * args , ** kwargs ): return self . _run ( \"categorical\" , * args , ** kwargs ) def __getattr__ ( self , attr ): if attr == \"sampler\" : return self . __dict__ . get ( \"sampler\" ) return getattr ( self . sampler , attr ) class ModelParam ( metaclass = abc . ABCMeta ): @abc . abstractmethod def update ( self , alpha : float , updates ) -> \"ModelParam\" : pass @nice_repr class UnormalizedWeightParam ( ModelParam ): def __init__ ( self , value ): self . value = value def update ( self , alpha : float , updates : list ) -> \"UnormalizedWeightParam\" : return UnormalizedWeightParam ( self . value + alpha * sum ( updates )) def weighted ( self , solutions ): result = 0 for s , w in solutions : result += s * w return UnormalizedWeightParam ( 1 + result ) @nice_repr class DistributionParam ( ModelParam ): def __init__ ( self , weights ): total = sum ( weights ) or 1 self . weights = [ w / total for w in weights ] def update ( self , alpha : float , updates : list ) -> \"DistributionParam\" : weights = list ( self . weights ) for i in updates : weights [ i ] += alpha return DistributionParam ( weights ) def weighted ( self , solutions ): weights = [ 1 ] * len ( self . weights ) for s , w in solutions : weights [ s ] += w return DistributionParam ( weights ) @nice_repr class MeanDevParam ( ModelParam ): def __init__ ( self , mean , dev , * , initial_params = None ): self . mean = mean self . dev = dev if initial_params is None : self . initial_params = ( mean , dev ) else : self . initial_params = initial_params def update ( self , alpha : float , updates ) -> \"MeanDevParam\" : new_mean = statistics . mean ( updates ) new_dev = statistics . stdev ( updates , new_mean ) if len ( updates ) > 1 else 0 return MeanDevParam ( mean = self . mean * ( 1 - alpha ) + new_mean * alpha , dev = self . dev * ( 1 - alpha ) + new_dev * alpha , initial_params = self . initial_params , ) def weighted ( self , solutions ): values = np . asarray ( [ s for s , w in solutions ] + [ self . initial_params [ 0 ] - 2 * self . initial_params [ 1 ], self . initial_params [ 0 ] + 2 * self . initial_params [ 1 ], ] ) weights = np . asarray ([ w for s , w in solutions ] + [ 1 , 1 ]) average = np . average ( values , weights = weights ) variance = np . average (( values - average ) ** 2 , weights = weights ) return MeanDevParam ( average , math . sqrt ( variance ), initial_params = self . initial_params ) @nice_repr class WeightParam ( ModelParam ): def __init__ ( self , value ): self . value = value def update ( self , alpha : float , updates ) -> \"WeightParam\" : new_value = statistics . mean ( updates ) return WeightParam ( value = self . value * ( 1 - alpha ) + new_value * alpha ) def weighted ( self , solutions ): values = np . asarray ([ s for s , w in solutions ] + [ 0 , 1 ]) weights = np . asarray ([ w for s , w in solutions ] + [ 1 , 1 ]) return WeightParam ( np . average ( values , weights = weights )) def update_model ( model , updates , alpha : float = 1 ): new_model = {} for handle , params in model . items (): upd = updates . get ( handle ) if upd is None : new_model [ handle ] = params else : new_model [ handle ] = params . update ( alpha , upd ) return new_model def _argsort ( l ): taken from https://stackoverflow.com/questions/6422700 return sorted ( range ( len ( l )), key = l . __getitem__ ) def best_indices ( values : List , k : int = 1 , maximize : bool = False ) -> List [ int ]: Note Computes the k best indices from values, i.e., the indices of the values that are the top minimum (or maximum).","title":"Examples"},{"location":"api/autogoal.sampling.__init__/#parameters","text":"values: List : Values to compare, must be a sortable type (e.g., int , float , ...). k: int : Number of indices to calculate. Defaults to 1 . maximize: bool : Whether to compute the maximum or minimum values. Defaults to False , i.e., minimize by default.","title":"Parameters"},{"location":"api/autogoal.sampling.__init__/#returns","text":"indices: List[int] : list of the indices that correspond to maximum (or minimum) values in values .","title":"Returns:"},{"location":"api/autogoal.sampling.__init__/#examples_8","text":"```python best_indices([.33, 0.12, 0.55, 0.09], k=2) [1, 3] best_indices([.33, 0.12, 0.55, 0.09], k=3, maximize=True) [0, 1, 2] best_indices([.33, 0.12, 0.55, 0.09]) [3] ``` Note Note that indices are returned in their original order, not in the order in which the values would be sorted themselves. indices = _argsort ( _argsort ( values )) if maximize : threshold = len ( values ) - k return [ i for i in range ( len ( values )) if indices [ i ] >= threshold ] else : threshold = k return [ i for i in range ( len ( values )) if indices [ i ] < threshold ] def merge_updates ( * updates : Sequence [ Dict ]) -> Dict : Note Merges a bunch of update dicts from ModelSampler into a single dictionary.","title":"Examples:"},{"location":"api/autogoal.sampling.__init__/#parameters_1","text":"updates: Sequence[Dict] : Sequence of update dictionaries obtained from calling ModelSampler.updates .","title":"Parameters:"},{"location":"api/autogoal.sampling.__init__/#returns_1","text":"update: Dict : A single dictionary with the combined (appended) updates.","title":"Returns:"},{"location":"api/autogoal.sampling.__init__/#examples_9","text":"```python up1 = {'a': [1]} up2 = {'b': [2,3]} up3 = {'a': [4]} merge_updates(up1, up2, up3) {'a': [1, 4], 'b': [2, 3]} ``` result = {} for upd in updates : for key , value in upd . items (): if not key in result : result [ key ] = [] result [ key ] . extend ( value ) return result class ExhaustiveSampler : Note Performs an exhaustive sampling of the parameter space. The way this sampler works is by building an explicit representation of a parameter space in the form a search tree. As you sample values it will create nodes that represent each decision and the possible outcomes. We'll build a tree that represents all possible combinations in the parameter space. Each node in the tree contains set of values, and the children correspond to the nodes that represent the subsequent options for each value. For example, if our parameter space has two variables, X=[1,2,3] and Y=['a', 'b'] , we'll represent it in a tree that looks like this: [ 1 , 2 , 3 ] | | | [a,b] [a,b] [a,b] / \\ / \\ / \\ 1,a 1,b 2,a 2,b 3,a 3,b In each node we'll store a handle and a set of values for the distribution. The challenge here is build the paramter tree at the same time that we are sampling. Since we don't know beforehand what of the structure of the parameter space, we have to discover it as the different sampling methods are being invoked. However, since sampling is a never-ending process, we will have to explicitely tell this sampler when we finished with one instance. Otherwise we can't know when did we reach a leaf. def __init__ ( self ) -> None : self . _root = ExhaustiveSampler . Node () self . _current_node = self . _root def _sample_next ( self , distribution , params , handle ): At any moment when we call a sampling method, we are at some node in the parameter space. In the simplest case that node is not initialized, i.e., we have never sampled from it before. if not self . _current_node . is_initialized (): self . _current_node . initialize ( distribution , params , handle ) Then we'll sample from that node, which in this case will only return the next value. At the same time, we'll recurse down the corresponding children. self . _current_node , value = self . _current_node . sample () return value class Node : def is_initialized ( self ) -> bool : return hasattr ( self , \"handle\" ) def initialize ( self , distribution , params , handle ) -> None : self . handle = handle self . values = getattr ( self , f \"initialize_ { distribution } \" )( ** params ) In this dictionary we will store as values the nodes that represents the distributions that will be invoked after returning the corresponding result stored at each key. self . children = {} def sample ( self ): pass","title":"Examples:"},{"location":"api/autogoal.search.__init__/","text":"from ._base import ( SearchAlgorithm , ProgressLogger , ConsoleLogger , Logger , MemoryLogger , RichLogger , ) from ._random import RandomSearch from ._pge import ModelSampler , PESearch from ._learning import SurrogateSearch","title":"Autogoal.search.  init  "},{"location":"api/autogoal.search._base/","text":"import logging import enlighten import time import datetime import statistics import math import termcolor import autogoal.logging from autogoal.utils import RestrictedWorkerByJoin , Min , Gb , Sec from autogoal.sampling import ReplaySampler from rich.progress import Progress from rich.panel import Panel class SearchAlgorithm : def __init__ ( self , generator_fn = None , fitness_fn = None , pop_size = 20 , maximize = True , errors = \"raise\" , early_stop = 0.5 , evaluation_timeout : int = 10 * Sec , memory_limit : int = 4 * Gb , search_timeout : int = 5 * Min , target_fn = None , allow_duplicates = True , ): if generator_fn is None and fitness_fn is None : raise ValueError ( \"You must provide either `generator_fn` or `fitness_fn`\" ) self . _generator_fn = generator_fn self . _fitness_fn = fitness_fn or ( lambda x : x ) self . _pop_size = pop_size self . _maximize = maximize self . _errors = errors self . _evaluation_timeout = evaluation_timeout self . _memory_limit = memory_limit self . _early_stop = early_stop self . _search_timeout = search_timeout self . _target_fn = target_fn self . _allow_duplicates = allow_duplicates if self . _evaluation_timeout > 0 or self . _memory_limit > 0 : self . _fitness_fn = RestrictedWorkerByJoin ( self . _fitness_fn , self . _evaluation_timeout , self . _memory_limit ) def run ( self , generations = None , logger = None ): Note \"\"\"Runs the search performing at most generations of fitness_fn . Returns: Tuple (best, fn) of the best found solution and its corresponding fitness. if logger is None : logger = Logger () if generations is None : generations = math . inf if isinstance ( logger , list ): logger = MultiLogger ( * logger ) if isinstance ( self . _early_stop , float ): early_stop = int ( self . _early_stop * generations ) else : early_stop = self . _early_stop best_solution = None best_fn = None no_improvement = 0 start_time = time . time () seen = set () logger . begin ( generations , self . _pop_size ) try : while generations > 0 : stop = False logger . start_generation ( generations , best_fn ) self . _start_generation () fns = [] improvement = False for _ in range ( self . _pop_size ): solution = None try : solution = self . _generate () except Exception as e : logger . error ( \"Error while generating solution: %s \" % e , solution ) continue if not self . _allow_duplicates and repr ( solution ) in seen : continue try : logger . sample_solution ( solution ) fn = self . _fitness_fn ( solution ) except Exception as e : fn = 0.0 logger . error ( e , solution ) if self . _errors == \"raise\" : logger . end ( best_solution , best_fn ) raise e from None if not self . _allow_duplicates : seen . add ( repr ( solution )) logger . eval_solution ( solution , fn ) fns . append ( fn ) if ( best_fn is None or ( fn > best_fn and self . _maximize ) or ( fn < best_fn and not self . _maximize ) ): logger . update_best ( solution , fn , best_solution , best_fn ) best_solution = solution best_fn = fn improvement = True if self . _target_fn and best_fn >= self . _target_fn : stop = True break spent_time = time . time () - start_time if self . _search_timeout and spent_time > self . _search_timeout : autogoal . logging . logger () . info ( \"(!) Stopping since time spent is %.2f .\" % ( spent_time ) ) stop = True break if not improvement : no_improvement += 1 else : no_improvement = 0 generations -= 1 if generations <= 0 : autogoal . logging . logger () . info ( \"(!) Stopping since all generations are done.\" ) stop = True break if early_stop and no_improvement >= early_stop : autogoal . logging . logger () . info ( \"(!) Stopping since no improvement for %i generations.\" % no_improvement ) stop = True break logger . finish_generation ( fns ) self . _finish_generation ( fns ) if stop : break except KeyboardInterrupt : pass logger . end ( best_solution , best_fn ) return best_solution , best_fn def _generate ( self ): BUG: When multiprocessing is used for evaluation and no generation function is defined, the actual sampling occurs during fitness evaluation, and since that process has a copy of the solution we don't get the history in the ReplaySampler . sampler = ReplaySampler ( self . _build_sampler ()) if self . _generator_fn is not None : solution = self . _generator_fn ( sampler ) else : solution = sampler solution . sampler_ = sampler return solution def _build_sampler ( self ): raise NotImplementedError () def _start_generation ( self ): pass def _finish_generation ( self , fns ): pass class Logger : def begin ( self , generations , pop_size ): pass def end ( self , best , best_fn ): pass def start_generation ( self , generations , best_fn ): pass def finish_generation ( self , fns ): pass def sample_solution ( self , solution ): pass def eval_solution ( self , solution , fitness ): pass def error ( self , e : Exception , solution ): pass def update_best ( self , new_best , new_fn , previous_best , previous_fn ): pass class ConsoleLogger ( Logger ): def begin ( self , generations , pop_size ): print ( \"Starting search: generations= %i \" % generations ) self . start_time = time . time () self . start_generations = generations @staticmethod def normal ( text ): return termcolor . colored ( text , color = \"gray\" ) @staticmethod def emph ( text ): return termcolor . colored ( text , color = \"white\" , attrs = [ \"bold\" ]) @staticmethod def success ( text ): return termcolor . colored ( text , color = \"green\" ) @staticmethod def primary ( text ): return termcolor . colored ( text , color = \"blue\" ) @staticmethod def warn ( text ): return termcolor . colored ( text , color = \"orange\" ) @staticmethod def err ( text ): return termcolor . colored ( text , color = \"red\" ) def start_generation ( self , generations , best_fn ): current_time = time . time () elapsed = int ( current_time - self . start_time ) avg_time = elapsed / ( self . start_generations - generations + 1 ) remaining = int ( avg_time * generations ) elapsed = datetime . timedelta ( seconds = elapsed ) remaining = datetime . timedelta ( seconds = remaining ) print ( self . emph ( \"New generation started\" ), self . success ( f \"best_fn= { float ( best_fn or 0.0 ) : 0.3 } \" ), self . primary ( f \"generations= { generations } \" ), self . primary ( f \"elapsed= { elapsed } \" ), self . primary ( f \"remaining= { remaining } \" ), ) def error ( self , e : Exception , solution ): print ( self . err ( \"(!) Error evaluating pipeline: %s \" % e )) def end ( self , best , best_fn ): print ( self . emph ( \"Search completed: best_fn= %.3f , best= \\n %r \" % ( best_fn , best ))) def sample_solution ( self , solution ): print ( self . emph ( \"Evaluating pipeline:\" )) print ( solution ) def eval_solution ( self , solution , fitness ): print ( self . primary ( \"Fitness= %.3f \" % fitness )) def update_best ( self , new_best , new_fn , previous_best , previous_fn ): print ( self . success ( \"Best solution: improved= %.3f , previous= %.3f \" % ( new_fn , previous_fn or 0 ) ) ) class ProgressLogger ( Logger ): def begin ( self , generations , pop_size ): self . manager = enlighten . get_manager () self . pop_counter = self . manager . counter ( total = pop_size , unit = \"evals\" , leave = True , desc = \"Current Gen\" ) self . total_counter = self . manager . counter ( total = generations * pop_size , unit = \"evals\" , leave = True , desc = \"Best: 0.000\" ) def sample_solution ( self , solution ): self . pop_counter . update () self . total_counter . update () def start_generation ( self , generations , best_fn ): self . pop_counter . count = 0 self . total_counter . update ( force = True ) def update_best ( self , new_best , new_fn , * args ): self . total_counter . desc = \"Best: %.3f \" % new_fn def end ( self , * args ): self . pop_counter . close () self . total_counter . close () self . manager . stop () class RichLogger ( Logger ): def __init__ ( self ) -> None : self . console = autogoal . logging . console () self . logger = autogoal . logging . logger () def begin ( self , generations , pop_size ): self . progress = Progress ( console = self . console ) self . pop_counter = self . progress . add_task ( \"Generation\" , total = pop_size ) self . total_counter = self . progress . add_task ( \"Overall\" , total = pop_size * generations ) self . progress . start () self . console . rule ( \"Search starting\" , style = \"blue\" ) def sample_solution ( self , solution ): self . progress . advance ( self . pop_counter ) self . progress . advance ( self . total_counter ) self . console . rule ( \"Evaluating pipeline\" ) self . console . print ( repr ( solution )) def eval_solution ( self , solution , fitness ): self . console . print ( Panel ( f \"\ud83d\udcc8 Fitness=[blue] { fitness : .3f } \" )) def error ( self , e : Exception , solution ): self . console . print ( f \"\u26a0\ufe0f[red bold]Error:[/] { e } \" ) def start_generation ( self , generations , best_fn ): self . console . rule ( f \"New generation - Remaining= { generations } - Best= { best_fn or 0 : .3f } \" ) def start_generation ( self , generations , best_fn ): self . progress . update ( self . pop_counter , completed = 0 ) def update_best ( self , new_best , new_fn , previous_best , previous_fn ): self . console . print ( Panel ( f \"\ud83d\udd25 Best improved from [red bold] { previous_fn or 0 : .3f } [/] to [green bold] { new_fn : .3f } [/]\" ) ) def end ( self , best , best_fn ): self . console . rule ( f \"Search finished\" ) self . console . print ( repr ( best )) self . console . print ( Panel ( f \"\ud83c\udf1f Best=[green bold] { best_fn or 0 : .3f } \" )) self . progress . stop () self . console . rule ( \"Search finished\" , style = \"red\" ) class MemoryLogger ( Logger ): def __init__ ( self ): self . generation_best_fn = [ 0 ] self . generation_mean_fn = [] def update_best ( self , new_best , new_fn , previous_best , previous_fn ): self . generation_best_fn [ - 1 ] = new_fn def finish_generation ( self , fns ): try : mean = statistics . mean ( fns ) except : mean = 0 self . generation_mean_fn . append ( mean ) self . generation_best_fn . append ( self . generation_best_fn [ - 1 ]) class MultiLogger ( Logger ): def __init__ ( self , * loggers ): self . loggers = loggers def run ( self , name , * args , ** kwargs ): for logger in self . loggers : getattr ( logger , name )( * args , ** kwargs ) def begin ( self , * args , ** kwargs ): self . run ( \"begin\" , * args , ** kwargs ) def end ( self , * args , ** kwargs ): self . run ( \"end\" , * args , ** kwargs ) def start_generation ( self , * args , ** kwargs ): self . run ( \"start_generation\" , * args , ** kwargs ) def finish_generation ( self , * args , ** kwargs ): self . run ( \"finish_generation\" , * args , ** kwargs ) def sample_solution ( self , * args , ** kwargs ): self . run ( \"sample_solution\" , * args , ** kwargs ) def eval_solution ( self , * args , ** kwargs ): self . run ( \"eval_solution\" , * args , ** kwargs ) def error ( self , * args , ** kwargs ): self . run ( \"error\" , * args , ** kwargs ) def update_best ( self , * args , ** kwargs ): self . run ( \"update_best\" , * args , ** kwargs )","title":"Autogoal.search. base"},{"location":"api/autogoal.search._learning/","text":"from autogoal.search import SearchAlgorithm , ModelSampler from typing import Callable class SurrogateFunction : def __init__ ( self , func , learning_algorithm ): self . func = func self . learning_algorithm = learning_algorithm class SurrogateSearch ( SearchAlgorithm ): def __init__ ( self , base_search : Callable [[], SearchAlgorithm ], estimator , generation_size : int = 10 , initial_pop_size : int = 10 , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . base_search = base_search self . generation_size = generation_size self . initial_pop_size = initial_pop_size self . training_X = [] self . training_y = [] def _start_generation ( self ): pass def _finish_generation ( self , fns ): pass def _build_sampler ( self ): return ModelSampler () def _generate ( self ): if len ( self . training_X ) < self . initial_pop_size : return super () . _generate ()","title":"Autogoal.search. learning"},{"location":"api/autogoal.search._pge/","text":"import statistics import abc from typing import Mapping , Optional , Dict , List , Sequence from autogoal.sampling import ModelSampler , best_indices , merge_updates , update_model from ._base import SearchAlgorithm import random import pickle import time class PESearch ( SearchAlgorithm ): def __init__ ( self , * args , learning_factor : float = 0.05 , selection : float = 0.2 , epsilon_greed : float = 0.1 , random_state : Optional [ int ] = None , name : str = None , save : bool = False , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . _learning_factor = learning_factor self . _selection = selection self . _epsilon_greed = epsilon_greed self . _model : Dict = {} self . _random_states = random . Random ( random_state ) self . _name = name or str ( time . time ()) self . _save = save def _start_generation ( self ): self . _samplers = [] def _build_sampler ( self ): if len ( self . _samplers ) < self . _epsilon_greed * self . _pop_size : sampler = ModelSampler ( random_state = self . _random_states . getrandbits ( 32 )) else : sampler = ModelSampler ( self . _model , random_state = self . _random_states . getrandbits ( 32 ) ) self . _samplers . append ( sampler ) return sampler def _finish_generation ( self , fns ): Compute the marginal model of the best pipelines indices = best_indices ( fns , k = int ( self . _selection * len ( fns )), maximize = self . _maximize ) samplers : List [ ModelSampler ] = [ self . _samplers [ i ] for i in indices ] updates : Dict = merge_updates ( * [ sampler . updates for sampler in samplers ]) Update the probabilistic model with the marginal model from the best pipelines self . _model = update_model ( self . _model , updates , self . _learning_factor ) save an internal state of metaheuristic for other executions if self . _save == True : with open ( \"model-\" + self . _name + \".pickle\" , \"wb\" ) as f : pickle . dump ( self . _model , f ) def load ( self , name_pickle_file ): Note \"\"\"Rewrites the probabilistic distribution of metaheuristic with the value of the name model. with open ( name_pickle_file ) as f : loaded_obj = pickle . load ( f ) self . _model = loaded_obj","title":"Autogoal.search. pge"},{"location":"api/autogoal.search._random/","text":"import random from ._base import SearchAlgorithm from autogoal.sampling import Sampler class RandomSearch ( SearchAlgorithm ): def __init__ ( self , * args , random_state : int = None , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . _sampler = Sampler ( random_state = random_state ) def _build_sampler ( self ): return self . _sampler","title":"Autogoal.search. random"},{"location":"api/autogoal.utils.__init__/","text":"import enum import inspect import collections MAX_REPR_DEPTH = 10 _repr_depth = [ 0 ] def nice_repr ( cls ): Note A decorator that adds a nice repr(.) to any decorated class. Decorate a class with @nice_repr to automatically generate a __repr__() method that prints the class name along with any parameters defined in the constructor which can be found in dir(self) . Examples \u00b6 All of the parameters that you want to be printed in repr(.) should be either stored in the instance or accesible by name (e.g., as a property). ```python @nice_repr ... class MyType: ... def init (self, a, b, c): ... self.a = a ... self._b = b ... self._c = c ... ... @property ... def b(self): ... return self._b ... x = MyType(42, b='hello', c='world') x MyType(a=42, b=\"hello\") ``` It works nicely with nested objects, if all of them are @nice_repr decorated. ```python @nice_repr ... class A: ... def init (self, inner): ... self.inner = inner @nice_repr ... class B: ... def init (self, value): ... self.value = value A([B(i) for i in range(10)]) A( inner=[ B(value=0), B(value=1), B(value=2), B(value=3), B(value=4), B(value=5), B(value=6), B(value=7), B(value=8), B(value=9), ] ) ``` It works with cyclic object graphs as well: ```python @nice_repr ... class A: ... def init (self, a:A=None): ... self.a = self A() A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(...)))))))))))) ``` Note Change autogoal.utils.MAX_REPR_DEPTH to increase the depth level of recursive repr . def repr_method ( self ): init_signature = inspect . signature ( self . __init__ ) exclude_param_names = set ([ \"self\" ]) if _repr_depth [ 0 ] > MAX_REPR_DEPTH : return f \" { self . __class__ . __name__ } (...)\" _repr_depth [ 0 ] += 1 parameter_names = [ name for name in init_signature . parameters if name not in exclude_param_names ] parameter_values = [ getattr ( self , param , None ) for param in parameter_names ] if hasattr ( self , \"__nice_repr_hook__\" ): self . __nice_repr_hook__ ( parameter_names , parameter_values ) args = \", \" . join ( f \" { name } = { repr ( value ) } \" for name , value in zip ( parameter_names , parameter_values ) if value is not None ) fr = f \" { self . __class__ . __name__ } ( { args } )\" _repr_depth [ 0 ] -= 1 try : import black return black . format_str ( fr , mode = black . FileMode ()) . strip () except : return fr cls . __repr__ = repr_method return cls Kb = 1024 Mb = 1024 * Kb Gb = 1024 * Mb Sec = 1 Min = 60 * Sec Hour = 60 * Min def flatten ( y ): Note Recursively flattens a list. Examples \u00b6 ```python flatten([[1],[2,[3]],4]) [1, 2, 3, 4] ``` if isinstance ( y , list ): return [ z for x in y for z in flatten ( x )] else : return [ y ] def compute_class_weights ( y ): Note Computes relative class weights for imbalanced datasets. Works with nested input. Examples \u00b6 ```python compute_class_weights([['A', 'B', 'A'], ['C'], ['C', 'C']]) {'A': 1.5, 'B': 3.0, 'C': 1.0} ``` y = flatten ( y ) class_counts = collections . Counter ( y ) _ , max_class = class_counts . most_common ( 1 )[ 0 ] return { k : max_class / v for k , v in class_counts . items ()} def factory ( func_or_type , * args , ** kwargs ): def call (): return func_or_type ( * args , ** kwargs ) return call from ._resource import ResourceManager from ._process import RestrictedWorker , RestrictedWorkerByJoin from ._cache import CacheManager","title":"Autogoal.utils.  init  "},{"location":"api/autogoal.utils.__init__/#examples","text":"All of the parameters that you want to be printed in repr(.) should be either stored in the instance or accesible by name (e.g., as a property). ```python @nice_repr ... class MyType: ... def init (self, a, b, c): ... self.a = a ... self._b = b ... self._c = c ... ... @property ... def b(self): ... return self._b ... x = MyType(42, b='hello', c='world') x MyType(a=42, b=\"hello\") ``` It works nicely with nested objects, if all of them are @nice_repr decorated. ```python @nice_repr ... class A: ... def init (self, inner): ... self.inner = inner @nice_repr ... class B: ... def init (self, value): ... self.value = value A([B(i) for i in range(10)]) A( inner=[ B(value=0), B(value=1), B(value=2), B(value=3), B(value=4), B(value=5), B(value=6), B(value=7), B(value=8), B(value=9), ] ) ``` It works with cyclic object graphs as well: ```python @nice_repr ... class A: ... def init (self, a:A=None): ... self.a = self A() A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(a=A(...)))))))))))) ``` Note Change autogoal.utils.MAX_REPR_DEPTH to increase the depth level of recursive repr . def repr_method ( self ): init_signature = inspect . signature ( self . __init__ ) exclude_param_names = set ([ \"self\" ]) if _repr_depth [ 0 ] > MAX_REPR_DEPTH : return f \" { self . __class__ . __name__ } (...)\" _repr_depth [ 0 ] += 1 parameter_names = [ name for name in init_signature . parameters if name not in exclude_param_names ] parameter_values = [ getattr ( self , param , None ) for param in parameter_names ] if hasattr ( self , \"__nice_repr_hook__\" ): self . __nice_repr_hook__ ( parameter_names , parameter_values ) args = \", \" . join ( f \" { name } = { repr ( value ) } \" for name , value in zip ( parameter_names , parameter_values ) if value is not None ) fr = f \" { self . __class__ . __name__ } ( { args } )\" _repr_depth [ 0 ] -= 1 try : import black return black . format_str ( fr , mode = black . FileMode ()) . strip () except : return fr cls . __repr__ = repr_method return cls Kb = 1024 Mb = 1024 * Kb Gb = 1024 * Mb Sec = 1 Min = 60 * Sec Hour = 60 * Min def flatten ( y ): Note Recursively flattens a list.","title":"Examples"},{"location":"api/autogoal.utils.__init__/#examples_1","text":"```python flatten([[1],[2,[3]],4]) [1, 2, 3, 4] ``` if isinstance ( y , list ): return [ z for x in y for z in flatten ( x )] else : return [ y ] def compute_class_weights ( y ): Note Computes relative class weights for imbalanced datasets. Works with nested input.","title":"Examples"},{"location":"api/autogoal.utils.__init__/#examples_2","text":"```python compute_class_weights([['A', 'B', 'A'], ['C'], ['C', 'C']]) {'A': 1.5, 'B': 3.0, 'C': 1.0} ``` y = flatten ( y ) class_counts = collections . Counter ( y ) _ , max_class = class_counts . most_common ( 1 )[ 0 ] return { k : max_class / v for k , v in class_counts . items ()} def factory ( func_or_type , * args , ** kwargs ): def call (): return func_or_type ( * args , ** kwargs ) return call from ._resource import ResourceManager from ._process import RestrictedWorker , RestrictedWorkerByJoin from ._cache import CacheManager","title":"Examples"},{"location":"api/autogoal.utils._cache/","text":"import mmap import functools import pickle , json , csv , os , shutil class PersistentDict ( dict ): Note Persistent dictionary with an API compatible with shelve and anydbm. Taken from https://code.activestate.com/recipes/576642/ The dict is kept in memory, so the dictionary operations run as fast as a regular dictionary. Write to disk is delayed until close or sync (similar to gdbm's fast mode). Input file format is automatically discovered. Output file format is selectable between pickle, json, and csv. All three serialization formats are backed by fast C implementations. def __init__ ( self , filename , flag = \"c\" , mode = None , format = \"pickle\" , * args , ** kwds ): self . flag = flag # r=readonly, c=create, or n=new self . mode = mode # None or an octal triple like 0644 self . format = format # 'csv', 'json', or 'pickle' self . filename = filename if flag != \"n\" and os . access ( filename , os . R_OK ): fileobj = open ( filename , \"rb\" if format == \"pickle\" else \"r\" ) with fileobj : self . load ( fileobj ) dict . __init__ ( self , * args , ** kwds ) def sync ( self ): \"Write dict to disk\" if self . flag == \"r\" : return filename = self . filename tempname = filename + \".tmp\" fileobj = open ( tempname , \"wb\" if self . format == \"pickle\" else \"w\" ) try : self . dump ( fileobj ) except Exception : os . remove ( tempname ) raise finally : fileobj . close () shutil . move ( tempname , self . filename ) # atomic commit if self . mode is not None : os . chmod ( self . filename , self . mode ) def close ( self ): self . sync () def __enter__ ( self ): return self def __exit__ ( self , * exc_info ): self . close () def dump ( self , fileobj ): if self . format == \"csv\" : csv . writer ( fileobj ) . writerows ( self . items ()) elif self . format == \"json\" : json . dump ( self , fileobj , separators = ( \",\" , \":\" )) elif self . format == \"pickle\" : pickle . dump ( dict ( self ), fileobj , 2 ) else : raise NotImplementedError ( \"Unknown format: \" + repr ( self . format )) def load ( self , fileobj ): try formats from most restrictive to least restrictive for loader in ( pickle . load , json . load , csv . reader ): fileobj . seek ( 0 ) try : return self . update ( loader ( fileobj )) except Exception : pass raise ValueError ( \"File not in a supported format\" ) from pathlib import Path class CacheManager : _instance = None def __init__ ( self ): self . cache = PersistentDict ( str ( Path ( __file__ ) . parent / \"cache.pickle\" )) @staticmethod def get ( name : str , func ): instance = CacheManager . instance () if name not in instance . cache : print ( \"Creating cached object ' %s '\" % name ) instance . cache [ name ] = func () instance . cache . sync () return instance . cache [ name ] @staticmethod def instance () -> \"CacheManager\" : if CacheManager . _instance is None : CacheManager . _instance = CacheManager () return CacheManager . _instance def cached_run ( func ): @functools . wraps ( func ) def run ( self , input ): if not hasattr ( input , \"__cached_id__\" ): return func ( input ) cached_id = input . __cached_id__ return run","title":"Autogoal.utils. cache"},{"location":"api/autogoal.utils._helpers/","text":"autogoal.utils._helpers \u00b6 Note This module contains the optimize function that allows to apply black-box hyper-parameter search to an arbitrary Python code. import inspect import textwrap import functools from typing import Callable from autogoal.search import PESearch from autogoal.grammar import generate_cfg Black-box optimization \u00b6 The following function defines a black-box optimization that can be applied to any function. def optimize ( fn , search_strategy = PESearch , generations = 100 , pop_size = 10 , allow_duplicates = False , logger = None , ** kwargs , ): Note A general-purpose optimization function. Simply define any function fn with suitable parameter annotations and apply optimize . Parameters : search_strategy : customize the search strategy. By default a PESearch will be performed. generations : max number of generations to run. logger : instance of Logger (or list) to pass to the search strategy. **kwargs : additional keyword arguments passed to the search strategy constructor. params_func = _make_params_func ( fn ) @functools . wraps ( fn ) def eval_func ( kwargs ): return fn ( ** kwargs ) grammar = generate_cfg ( params_func ) search = search_strategy ( grammar , eval_func , pop_size = pop_size , allow_duplicates = allow_duplicates , ** kwargs , ) best , best_fn = search . run ( generations , logger = logger ) return best , best_fn Implementation details \u00b6 To make optimize work we need to define both a grammar and a callable function to pass to the search algorithm class _ParamsDict ( dict ): pass def _make_params_func ( fn : Callable ): signature = inspect . signature ( fn ) func_name = f \" { fn . __name__ } _params\" args_names = signature . parameters . keys () def annotation_repr ( ann ): if inspect . isclass ( ann ) or inspect . isfunction ( ann ): return ann . __name__ return repr ( ann ) args_line = \", \\n \" . join ( f \" { k } = { k } \" for k in args_names ) params_line = \", \" . join ( f \" { arg . name } : { annotation_repr ( arg . annotation ) } \" for arg in signature . parameters . values () ) func_code = textwrap . dedent ( f \"\"\" def { func_name } ( { params_line } ): return _ParamsDict( { args_line } )\"\"\" ) globals_dict = dict ( fn . __globals__ ) globals_dict [ \"_ParamsDict\" ] = _ParamsDict locals_dict = {} exec ( func_code , globals_dict , locals_dict ) return locals_dict [ func_name ]","title":"`autogoal.utils._helpers`"},{"location":"api/autogoal.utils._helpers/#autogoalutils_helpers","text":"Note This module contains the optimize function that allows to apply black-box hyper-parameter search to an arbitrary Python code. import inspect import textwrap import functools from typing import Callable from autogoal.search import PESearch from autogoal.grammar import generate_cfg","title":"autogoal.utils._helpers"},{"location":"api/autogoal.utils._helpers/#black-box-optimization","text":"The following function defines a black-box optimization that can be applied to any function. def optimize ( fn , search_strategy = PESearch , generations = 100 , pop_size = 10 , allow_duplicates = False , logger = None , ** kwargs , ): Note A general-purpose optimization function. Simply define any function fn with suitable parameter annotations and apply optimize . Parameters : search_strategy : customize the search strategy. By default a PESearch will be performed. generations : max number of generations to run. logger : instance of Logger (or list) to pass to the search strategy. **kwargs : additional keyword arguments passed to the search strategy constructor. params_func = _make_params_func ( fn ) @functools . wraps ( fn ) def eval_func ( kwargs ): return fn ( ** kwargs ) grammar = generate_cfg ( params_func ) search = search_strategy ( grammar , eval_func , pop_size = pop_size , allow_duplicates = allow_duplicates , ** kwargs , ) best , best_fn = search . run ( generations , logger = logger ) return best , best_fn","title":"Black-box optimization"},{"location":"api/autogoal.utils._helpers/#implementation-details","text":"To make optimize work we need to define both a grammar and a callable function to pass to the search algorithm class _ParamsDict ( dict ): pass def _make_params_func ( fn : Callable ): signature = inspect . signature ( fn ) func_name = f \" { fn . __name__ } _params\" args_names = signature . parameters . keys () def annotation_repr ( ann ): if inspect . isclass ( ann ) or inspect . isfunction ( ann ): return ann . __name__ return repr ( ann ) args_line = \", \\n \" . join ( f \" { k } = { k } \" for k in args_names ) params_line = \", \" . join ( f \" { arg . name } : { annotation_repr ( arg . annotation ) } \" for arg in signature . parameters . values () ) func_code = textwrap . dedent ( f \"\"\" def { func_name } ( { params_line } ): return _ParamsDict( { args_line } )\"\"\" ) globals_dict = dict ( fn . __globals__ ) globals_dict [ \"_ParamsDict\" ] = _ParamsDict locals_dict = {} exec ( func_code , globals_dict , locals_dict ) return locals_dict [ func_name ]","title":"Implementation details"},{"location":"api/autogoal.utils._process/","text":"from logging import log import multiprocessing import warnings import psutil import signal import os import traceback import logging import platform if platform . system () == \"Linux\" : import resource from autogoal.utils import Mb logger = logging . getLogger ( \"autogoal\" ) class RestrictedWorker : def __init__ ( self , function , timeout : int , memory : int ): self . function = function self . timeout = timeout self . memory = memory signal . signal ( signal . SIGXCPU , alarm_handler ) def _restrict ( self ): if platform . system () == \"Linux\" : msoft , mhard = resource . getrlimit ( resource . RLIMIT_DATA ) csoft , chard = resource . getrlimit ( resource . RLIMIT_CPU ) used_memory = self . get_used_memory () if self . memory and self . memory > ( used_memory + 500 * Mb ): memory may be restricted resource . setrlimit ( resource . RLIMIT_DATA , ( self . memory , mhard )) else : warnings . warn ( \"Cannot restrict memory\" ) if self . timeout : time may be restricted resource . setrlimit ( resource . RLIMIT_CPU , ( self . timeout , chard )) else : warnings . warn ( \"Cannot restrict cpu time\" ) def _restricted_function ( self , result_bucket , args , kwargs ): try : self . _restrict () result = self . function ( * args , ** kwargs ) result_bucket [ \"result\" ] = result except Exception as e : msg = \" {} \\n\\n Original {} \" . format ( e , traceback . format_exc ()) result_bucket [ \"result\" ] = e . __class__ ( msg ) def run_restricted ( self , * args , ** kwargs ): Note Executes a given function with restricted amount of CPU time and RAM memory usage manager = multiprocessing . Manager () result_bucket = manager . dict () rprocess = multiprocessing . Process ( target = self . _restricted_function , args = [ result_bucket , args , kwargs ] ) rprocess . start () rprocess . join () result = result_bucket [ \"result\" ] if isinstance ( result , Exception ): # Exception ocurred raise result return result def get_used_memory ( self ): Note returns the amount of memory being used by the current process process = psutil . Process ( os . getpid ()) return process . memory_info () . rss def __call__ ( self , * args , ** kwargs ): return self . run_restricted ( * args , ** kwargs ) def alarm_handler ( * args ): raise TimeoutError ( \"process %d got to time limit\" % os . getpid ()) class RestrictedWorkerByJoin ( RestrictedWorker ): def __init__ ( self , function , timeout : int , memory : int ): self . function = function self . timeout = timeout self . memory = memory def _restrict ( self ): if platform . system () == \"Linux\" : _ , mhard = resource . getrlimit ( resource . RLIMIT_AS ) used_memory = self . get_used_memory () if self . memory is None : return if self . memory > ( used_memory + 50 * Mb ): memory may be restricted logger . info ( \"\ud83d\udcbb Restricting memory to %s \" % self . memory ) resource . setrlimit ( resource . RLIMIT_DATA , ( self . memory , mhard )) else : raise ValueError ( \"Cannot restrict memory to %s < %i \" % ( self . memory , used_memory + 50 * Mb ) ) def run_restricted ( self , * args , ** kwargs ): Note Executes a given function with restricted amount of CPU time and RAM memory usage manager = multiprocessing . Manager () result_bucket = manager . dict () rprocess = multiprocessing . Process ( target = self . _restricted_function , args = [ result_bucket , args , kwargs ] ) rprocess . start () rprocess . join ( self . timeout ) if rprocess . exitcode == 0 : result = result_bucket [ \"result\" ] else : rprocess . terminate () raise TimeoutError () if isinstance ( result , Exception ): # Exception ocurred raise result return result","title":"Autogoal.utils. process"},{"location":"api/autogoal.utils._resource/","text":"import signal import psutil import warnings import os import multiprocessing import platform if platform . system () == \"Linux\" : import resource class ResourceManager : Note Resource manager class. Parameters \u00b6 time_limit: int : maximum amount of seconds a function can run for (default = 5 minutes ). ram_limit: int : maximum amount of memory bytes a function can use (default = 4 GB ). Notes \u00b6 Only one function may be restricted at the same time. Upcoming updates will fix this matter. Memory restriction is issued upon the process's heap memory size and not over total address space in order to get a better estimation of the used memory. def __init__ ( self , time_limit : int = 300 , memory_limit : int = 4294967296 ): self . set_time_limit ( time_limit ) self . set_memory_limit ( memory_limit ) signal . signal ( signal . SIGXCPU , alarm_handler ) def set_memory_limit ( self , limit ): Note Set the memory limit for future restricted functions. If memory limit is higher or equal than the current OS limit then it won't be changed. soft , hard = resource . getrlimit ( resource . RLIMIT_DATA ) self . original_limit = ( soft , hard ) self . memory_limit = None used_memory = self . get_used_memory () if limit <= ( used_memory + 500 * 1024 ** 2 ): warnings . warn ( \"Especified memory limit is too close to the used amount. Will not be taken into account.\" ) return if soft == - 1 or limit < soft : self . memory_limit = ( limit , hard ) else : warnings . warn ( \"Especified memory limit is higher than OS limit. Will not be taken into account.\" ) def set_time_limit ( self , limit ): self . time_limit = limit def _restrict ( self , memory_amount ): if memory_amount : _ , hard = self . original_limit limit , _ = memory_amount resource . setrlimit ( resource . RLIMIT_DATA , ( limit , hard )) resource . setrlimit ( resource . RLIMIT_CPU , ( self . time_limit , hard )) def _unrestrict_memory ( self ): self . _restrict ( self . original_limit ) def _run_for ( self , function , * args , ** kwargs ): def signal_handler ( * args ): raise TimeoutError () try : signal . signal ( signal . SIGALRM , signal_handler ) signal . alarm ( self . time_limit ) result = function ( * args , ** kwargs ) signal . alarm ( 0 ) # cancel the alarm return result except Exception as e : signal.alarm(0) #cancel the alarm raise e def get_used_memory ( self ): Note Returns the amount of memory being used by the current process. process = psutil . Process ( os . getpid ()) return process . memory_info () . rss def _restricted_function ( self , result_bucket , function , args , kwargs ): try : self . _restrict ( self . memory_limit ) result = function ( * args , ** kwargs ) result_bucket [ \"result\" ] = result except Exception as e : result_bucket [ \"result\" ] = e def run_restricted ( self , function , * args , ** kwargs ): Note Executes a given function with restricted amount of CPU time and RAM memory usage. manager = multiprocessing . Manager () result_bucket = manager . dict () rprocess = multiprocessing . Process ( target = self . _restricted_function , args = [ result_bucket , function , args , kwargs ], ) rprocess . start () print(\"started process:\", rprocess.pid) rprocess . join () print(\"ended process:\", rprocess.pid) result = result_bucket [ \"result\" ] if isinstance ( result , Exception ): # Exception ocurred raise result return result def alarm_handler ( * args ): raise TimeoutError ( \"process %d got to time limit\" % os . getpid ())","title":"Autogoal.utils. resource"},{"location":"api/autogoal.utils._resource/#parameters","text":"time_limit: int : maximum amount of seconds a function can run for (default = 5 minutes ). ram_limit: int : maximum amount of memory bytes a function can use (default = 4 GB ).","title":"Parameters"},{"location":"api/autogoal.utils._resource/#notes","text":"Only one function may be restricted at the same time. Upcoming updates will fix this matter. Memory restriction is issued upon the process's heap memory size and not over total address space in order to get a better estimation of the used memory. def __init__ ( self , time_limit : int = 300 , memory_limit : int = 4294967296 ): self . set_time_limit ( time_limit ) self . set_memory_limit ( memory_limit ) signal . signal ( signal . SIGXCPU , alarm_handler ) def set_memory_limit ( self , limit ): Note Set the memory limit for future restricted functions. If memory limit is higher or equal than the current OS limit then it won't be changed. soft , hard = resource . getrlimit ( resource . RLIMIT_DATA ) self . original_limit = ( soft , hard ) self . memory_limit = None used_memory = self . get_used_memory () if limit <= ( used_memory + 500 * 1024 ** 2 ): warnings . warn ( \"Especified memory limit is too close to the used amount. Will not be taken into account.\" ) return if soft == - 1 or limit < soft : self . memory_limit = ( limit , hard ) else : warnings . warn ( \"Especified memory limit is higher than OS limit. Will not be taken into account.\" ) def set_time_limit ( self , limit ): self . time_limit = limit def _restrict ( self , memory_amount ): if memory_amount : _ , hard = self . original_limit limit , _ = memory_amount resource . setrlimit ( resource . RLIMIT_DATA , ( limit , hard )) resource . setrlimit ( resource . RLIMIT_CPU , ( self . time_limit , hard )) def _unrestrict_memory ( self ): self . _restrict ( self . original_limit ) def _run_for ( self , function , * args , ** kwargs ): def signal_handler ( * args ): raise TimeoutError () try : signal . signal ( signal . SIGALRM , signal_handler ) signal . alarm ( self . time_limit ) result = function ( * args , ** kwargs ) signal . alarm ( 0 ) # cancel the alarm return result except Exception as e : signal.alarm(0) #cancel the alarm raise e def get_used_memory ( self ): Note Returns the amount of memory being used by the current process. process = psutil . Process ( os . getpid ()) return process . memory_info () . rss def _restricted_function ( self , result_bucket , function , args , kwargs ): try : self . _restrict ( self . memory_limit ) result = function ( * args , ** kwargs ) result_bucket [ \"result\" ] = result except Exception as e : result_bucket [ \"result\" ] = e def run_restricted ( self , function , * args , ** kwargs ): Note Executes a given function with restricted amount of CPU time and RAM memory usage. manager = multiprocessing . Manager () result_bucket = manager . dict () rprocess = multiprocessing . Process ( target = self . _restricted_function , args = [ result_bucket , function , args , kwargs ], ) rprocess . start () print(\"started process:\", rprocess.pid) rprocess . join () print(\"ended process:\", rprocess.pid) result = result_bucket [ \"result\" ] if isinstance ( result , Exception ): # Exception ocurred raise result return result def alarm_handler ( * args ): raise TimeoutError ( \"process %d got to time limit\" % os . getpid ())","title":"Notes"},{"location":"examples/","text":"AutoGOAL examples \u00b6 The following examples illustrate the use of AutoGOAL is more practical scenarios. These three examples conform the experimentation for the paper Solving Heterogenous AutoML Problems with AutoGOAL presented at the 7th Workshop on AutoML in ICML 2020. Optimizing seven UCI repository datasets Optimizing the HAHA 2019 challenge Optimizing the MEDDOCAN 2019 challenge The following examples test AutoGOAL's neural architecture search (NAS) capabilities on common datasets: Solving CARS with Keras Solving MEDDOCAN with Keras","title":"Index"},{"location":"examples/#autogoal-examples","text":"The following examples illustrate the use of AutoGOAL is more practical scenarios. These three examples conform the experimentation for the paper Solving Heterogenous AutoML Problems with AutoGOAL presented at the 7th Workshop on AutoML in ICML 2020. Optimizing seven UCI repository datasets Optimizing the HAHA 2019 challenge Optimizing the MEDDOCAN 2019 challenge The following examples test AutoGOAL's neural architecture search (NAS) capabilities on common datasets: Solving CARS with Keras Solving MEDDOCAN with Keras","title":"AutoGOAL examples"},{"location":"examples/tests.examples.automl_basic/","text":"AutoGOAL Example: basic usage of the AutoML class from autogoal.datasets import cars from autogoal.kb import MatrixContinuousDense , Supervised , VectorCategorical from autogoal.ml import AutoML Load dataset X , y = cars . load () Instantiate AutoML and define input/output types automl = AutoML ( input = ( MatrixContinuousDense , Supervised [ VectorCategorical ]), output = VectorCategorical , ) Run the pipeline search process automl . fit ( X , y ) Report the best pipeline print ( automl . best_pipeline_ ) print ( automl . best_score_ )","title":"Tests.examples.automl basic"},{"location":"examples/tests.examples.comparing_search_strategies/","text":"Comparing Search Strategies \u00b6 This example compares the performance of RandomSearch and PESearch on a toy problem. The full source code can be found here . from autogoal.search import RandomSearch , PESearch , RichLogger The problem to solve consists of a simple puzzle: Combining the digits 1 through 9 in different operations to obtain the largest possible value. We can apply addition, substraction, multiplication, division and exponentiation. To model all the possible operations and operators we will design a simple grammar using the class API from autogoal.grammar import generate_cfg , Union from autogoal.utils import nice_repr @nice_repr class Number : def __call__ ( self , stream ): return next ( stream ) class Operator : def __init__ ( self , left : \"Expr\" , right : \"Expr\" ): self . left = left self . right = right def __call__ ( self , stream ): return self . operate ( self . left ( stream ), self . right ( stream )) @nice_repr class Add ( Operator ): def operate ( self , left , right ): return left + right @nice_repr class Mult ( Operator ): def operate ( self , left , right ): return left * right @nice_repr class Concat ( Operator ): def operate ( self , left , right ): return int ( str ( left ) + str ( right )) Expr = Union ( \"Expr\" , Number , Add , Mult , Concat ) grammar = generate_cfg ( Expr ) print ( grammar ) Our grammar is composed of addition, multiplication and concatenation operators. Here are some possible examples: for i in range ( 100 ): try : solution = grammar . sample () print ( solution ) except ValueError : continue To evaluate how good a formula is, we simply feed the expression instance with a sequence of numbers from 1 to 9. If the expression requires more than 9 digits, it results in an error. The actual value of performing corresponding operations is done in the __call__ method of the expression classes. def evaluate ( expr ): def stream (): for i in range ( 1 , 10 ): yield i raise ValueError ( \"Too many values asked\" ) return expr ( stream ()) We will run 1000 iterations of each search strategy to compare their long-term performance. search_rand = RandomSearch ( grammar , evaluate , errors = \"ignore\" ) best_rand , best_fn_rand = search_rand . run ( 1000 , logger = RichLogger ()) search_pe = PESearch ( grammar , evaluate , pop_size = 10 , errors = \"ignore\" ) best_pe , best_fn_pe = search_pe . run ( 1000 , logger = RichLogger ()) And here are the results. print ( best_rand , best_fn_rand ) print ( best_pe , best_fn_pe )","title":"Comparing Search Strategies"},{"location":"examples/tests.examples.comparing_search_strategies/#comparing-search-strategies","text":"This example compares the performance of RandomSearch and PESearch on a toy problem. The full source code can be found here . from autogoal.search import RandomSearch , PESearch , RichLogger The problem to solve consists of a simple puzzle: Combining the digits 1 through 9 in different operations to obtain the largest possible value. We can apply addition, substraction, multiplication, division and exponentiation. To model all the possible operations and operators we will design a simple grammar using the class API from autogoal.grammar import generate_cfg , Union from autogoal.utils import nice_repr @nice_repr class Number : def __call__ ( self , stream ): return next ( stream ) class Operator : def __init__ ( self , left : \"Expr\" , right : \"Expr\" ): self . left = left self . right = right def __call__ ( self , stream ): return self . operate ( self . left ( stream ), self . right ( stream )) @nice_repr class Add ( Operator ): def operate ( self , left , right ): return left + right @nice_repr class Mult ( Operator ): def operate ( self , left , right ): return left * right @nice_repr class Concat ( Operator ): def operate ( self , left , right ): return int ( str ( left ) + str ( right )) Expr = Union ( \"Expr\" , Number , Add , Mult , Concat ) grammar = generate_cfg ( Expr ) print ( grammar ) Our grammar is composed of addition, multiplication and concatenation operators. Here are some possible examples: for i in range ( 100 ): try : solution = grammar . sample () print ( solution ) except ValueError : continue To evaluate how good a formula is, we simply feed the expression instance with a sequence of numbers from 1 to 9. If the expression requires more than 9 digits, it results in an error. The actual value of performing corresponding operations is done in the __call__ method of the expression classes. def evaluate ( expr ): def stream (): for i in range ( 1 , 10 ): yield i raise ValueError ( \"Too many values asked\" ) return expr ( stream ()) We will run 1000 iterations of each search strategy to compare their long-term performance. search_rand = RandomSearch ( grammar , evaluate , errors = \"ignore\" ) best_rand , best_fn_rand = search_rand . run ( 1000 , logger = RichLogger ()) search_pe = PESearch ( grammar , evaluate , pop_size = 10 , errors = \"ignore\" ) best_pe , best_fn_pe = search_pe . run ( 1000 , logger = RichLogger ()) And here are the results. print ( best_rand , best_fn_rand ) print ( best_pe , best_fn_pe )","title":"Comparing Search Strategies"},{"location":"examples/tests.examples.nn_cars/","text":"Solving the CARS dataset with Keras \u00b6 This script runs an instance of AutoML on the CARS dataset. In this example we only want to test the Neural Architecture Search (NAS) capabilities of AutoGOAL. CARS is a classic numeric feature dataset. Dataset URL Cars https://archive.ics.uci.edu/ml/datasets/Car+Evaluation As usual, we will need the AutoML class, a suitable logger, and semantic types from autogoal.kb . from autogoal.ml import AutoML from autogoal.search import RichLogger from autogoal.kb import * To restrict which types of algorithms can AutoML use, we will manually invoke find_classes . from autogoal.contrib import find_classes Experimentation \u00b6 Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., supervised classification from matrix-like features. classifier = AutoML ( input = ( MatrixContinuousDense , Supervised [ VectorCategorical ]), output = VectorCategorical , We will set cross_validation_steps=1 to reduce the time that we spend on each pipeline. Keep in mind this will increase the generalization error of the AutoML process. cross_validation_steps = 1 , Since we only want to try neural networks, we restrict the contrib registry to algorithms matching with Keras . registry = find_classes ( \"Keras\" ), ) Basic logging configuration. loggers = [ RichLogger ()] Finally, loading the CARS dataset, running the AutoML instance, and printing the results. from autogoal.datasets import cars from sklearn.model_selection import train_test_split X , y = cars . load () X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) By default, this will run for 5 minutes. classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) Let's see what we got! print ( score )","title":"Solving CARS with Keras"},{"location":"examples/tests.examples.nn_cars/#solving-the-cars-dataset-with-keras","text":"This script runs an instance of AutoML on the CARS dataset. In this example we only want to test the Neural Architecture Search (NAS) capabilities of AutoGOAL. CARS is a classic numeric feature dataset. Dataset URL Cars https://archive.ics.uci.edu/ml/datasets/Car+Evaluation As usual, we will need the AutoML class, a suitable logger, and semantic types from autogoal.kb . from autogoal.ml import AutoML from autogoal.search import RichLogger from autogoal.kb import * To restrict which types of algorithms can AutoML use, we will manually invoke find_classes . from autogoal.contrib import find_classes","title":"Solving the CARS dataset with Keras"},{"location":"examples/tests.examples.nn_cars/#experimentation","text":"Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., supervised classification from matrix-like features. classifier = AutoML ( input = ( MatrixContinuousDense , Supervised [ VectorCategorical ]), output = VectorCategorical , We will set cross_validation_steps=1 to reduce the time that we spend on each pipeline. Keep in mind this will increase the generalization error of the AutoML process. cross_validation_steps = 1 , Since we only want to try neural networks, we restrict the contrib registry to algorithms matching with Keras . registry = find_classes ( \"Keras\" ), ) Basic logging configuration. loggers = [ RichLogger ()] Finally, loading the CARS dataset, running the AutoML instance, and printing the results. from autogoal.datasets import cars from sklearn.model_selection import train_test_split X , y = cars . load () X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) By default, this will run for 5 minutes. classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) Let's see what we got! print ( score )","title":"Experimentation"},{"location":"examples/tests.examples.nn_cifar10/","text":"Solving the MEDDOCAN challenge with Keras \u00b6 This script runs an instance of AutoML In this example we only want to test the Neural Architecture Search (NAS) capabilities of AutoGOAL. Dataset URL CIFAR https:// from autogoal.utils import Hour , Min from autogoal.ml import AutoML from autogoal.search import ( RichLogger , PESearch , ) from autogoal.kb import * from autogoal.contrib import find_classes Experimentation \u00b6 Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., entity recognition. classifier = AutoML ( search_algorithm = PESearch , input = ( Tensor4 , Supervised [ VectorCategorical ]), output = VectorCategorical , cross_validation_steps = 1 , Since we only want to try neural networks, we restrict the contrib registry to algorithms matching with Keras . registry = find_classes ( \"Keras\" ), errors = \"raise\" , Since image classifiers are heavy to train, let's give them a longer timeout... evaluation_timeout = 5 * Min , search_timeout = 1 * Hour , ) Basic logging configuration. loggers = [ RichLogger ()] Finally, loading the CIFAR dataset, running the AutoML instance, and printing the results. from autogoal.datasets import cifar10 X_train , y_train , X_test , y_test = cifar10 . load () classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score )","title":"Solving the MEDDOCAN challenge with Keras"},{"location":"examples/tests.examples.nn_cifar10/#solving-the-meddocan-challenge-with-keras","text":"This script runs an instance of AutoML In this example we only want to test the Neural Architecture Search (NAS) capabilities of AutoGOAL. Dataset URL CIFAR https:// from autogoal.utils import Hour , Min from autogoal.ml import AutoML from autogoal.search import ( RichLogger , PESearch , ) from autogoal.kb import * from autogoal.contrib import find_classes","title":"Solving the MEDDOCAN challenge with Keras"},{"location":"examples/tests.examples.nn_cifar10/#experimentation","text":"Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., entity recognition. classifier = AutoML ( search_algorithm = PESearch , input = ( Tensor4 , Supervised [ VectorCategorical ]), output = VectorCategorical , cross_validation_steps = 1 , Since we only want to try neural networks, we restrict the contrib registry to algorithms matching with Keras . registry = find_classes ( \"Keras\" ), errors = \"raise\" , Since image classifiers are heavy to train, let's give them a longer timeout... evaluation_timeout = 5 * Min , search_timeout = 1 * Hour , ) Basic logging configuration. loggers = [ RichLogger ()] Finally, loading the CIFAR dataset, running the AutoML instance, and printing the results. from autogoal.datasets import cifar10 X_train , y_train , X_test , y_test = cifar10 . load () classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score )","title":"Experimentation"},{"location":"examples/tests.examples.nn_meddocan/","text":"Solving the MEDDOCAN challenge with Keras \u00b6 This script runs an instance of AutoML in the MEDDOCAN 2019 challenge . The full source code can be found here . In this example we only want to test the Neural Architecture Search (NAS) capabilities of AutoGOAL. Dataset URL MEDDOCAN 2019 https://github.com/PlanTL-SANIDAD/SPACCC_MEDDOCAN from autogoal.ml import AutoML from autogoal.datasets import meddocan from autogoal.search import ( RichLogger , PESearch , ) from autogoal.kb import * from autogoal.contrib import find_classes Experimentation \u00b6 Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., entity recognition. classifier = AutoML ( search_algorithm = PESearch , input = ( Seq [ Seq [ Word ]], Supervised [ Seq [ Seq [ Label ]]]), output = Seq [ Seq [ Label ]], score_metric = meddocan . F1_beta , cross_validation_steps = 1 , Since we only want to try neural networks, we restrict the contrib registry to algorithms matching with Keras . registry = find_classes ( \"Keras|Bert\" ), We need to give some extra time because neural networks are slow evaluation_timeout = 300 , search_timeout = 1800 , ) Basic logging configuration. loggers = [ RichLogger ()] Finally, loading the MEDDOCAN dataset, running the AutoML instance, and printing the results. X_train , y_train , X_test , y_test = meddocan . load () classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score )","title":"Solving MEDDOCAN with Keras"},{"location":"examples/tests.examples.nn_meddocan/#solving-the-meddocan-challenge-with-keras","text":"This script runs an instance of AutoML in the MEDDOCAN 2019 challenge . The full source code can be found here . In this example we only want to test the Neural Architecture Search (NAS) capabilities of AutoGOAL. Dataset URL MEDDOCAN 2019 https://github.com/PlanTL-SANIDAD/SPACCC_MEDDOCAN from autogoal.ml import AutoML from autogoal.datasets import meddocan from autogoal.search import ( RichLogger , PESearch , ) from autogoal.kb import * from autogoal.contrib import find_classes","title":"Solving the MEDDOCAN challenge with Keras"},{"location":"examples/tests.examples.nn_meddocan/#experimentation","text":"Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., entity recognition. classifier = AutoML ( search_algorithm = PESearch , input = ( Seq [ Seq [ Word ]], Supervised [ Seq [ Seq [ Label ]]]), output = Seq [ Seq [ Label ]], score_metric = meddocan . F1_beta , cross_validation_steps = 1 , Since we only want to try neural networks, we restrict the contrib registry to algorithms matching with Keras . registry = find_classes ( \"Keras|Bert\" ), We need to give some extra time because neural networks are slow evaluation_timeout = 300 , search_timeout = 1800 , ) Basic logging configuration. loggers = [ RichLogger ()] Finally, loading the MEDDOCAN dataset, running the AutoML instance, and printing the results. X_train , y_train , X_test , y_test = meddocan . load () classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score )","title":"Experimentation"},{"location":"examples/tests.examples.solving_haha_2019/","text":"Solving the HAHA challenge \u00b6 This script runs an instance of AutoML in the HAHA 2019 challenge . The full source code can be found here . The dataset used is: Dataset URL HAHA 2019 https://www.fing.edu.uy/inco/grupos/pln/haha/index.html#data Experimentation parameters \u00b6 This experiment was run with the following parameters: Parameter Value Total epochs 1 Maximum iterations 10000 Timeout per pipeline 30 min Global timeout - Max RAM per pipeline 20 GB Population size 50 Selection (k-best) 10 Early stop - The experiments were run in the following hardware configurations (allocated indistinctively according to available resources): Config CPU Cache Memory HDD A 12 core Intel Xeon Gold 6126 19712 KB 191927.2MB 999.7GB B 6 core Intel Xeon E5-1650 v3 15360 KB 32045.5MB 2500.5GB C Quad core Intel Core i7-2600 8192 KB 15917.1MB 1480.3GB Note The hardware configuration details were extracted with inxi -CmD and summarized. Relevant imports \u00b6 Most of this example follows the same logic as the UCI example . First the necessary imports from autogoal.ml import AutoML from autogoal.datasets import haha from autogoal.search import ( PESearch , RichLogger , ) from autogoal.kb import Seq , Sentence , VectorCategorical , Supervised from autogoal.contrib import find_classes from sklearn.metrics import f1_score Next, we parse the command line arguments to configure the experiment. Parsing arguments \u00b6 The default values are the ones used for the experimentation reported in the paper. import argparse parser = argparse . ArgumentParser () parser . add_argument ( \"--iterations\" , type = int , default = 10000 ) parser . add_argument ( \"--timeout\" , type = int , default = 60 ) parser . add_argument ( \"--memory\" , type = int , default = 2 ) parser . add_argument ( \"--popsize\" , type = int , default = 50 ) parser . add_argument ( \"--selection\" , type = int , default = 10 ) parser . add_argument ( \"--global-timeout\" , type = int , default = None ) parser . add_argument ( \"--examples\" , type = int , default = None ) parser . add_argument ( \"--token\" , default = None ) parser . add_argument ( \"--channel\" , default = None ) args = parser . parse_args () print ( args ) The next line will print all the algorithms that AutoGOAL found in the contrib library, i.e., anything that could be potentially used to solve an AutoML problem. for cls in find_classes (): print ( \"Using: %s \" % cls . __name__ ) Experimentation \u00b6 Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., text classification. classifier = AutoML ( search_algorithm = PESearch , input = ( Seq [ Sentence ], Supervised [ VectorCategorical ]), output = VectorCategorical , search_iterations = args . iterations , score_metric = f1_score , errors = \"warn\" , pop_size = args . popsize , search_timeout = args . global_timeout , evaluation_timeout = args . timeout , memory_limit = args . memory * 1024 ** 3 , ) loggers = [ RichLogger ()] if args . token : from autogoal.contrib.telegram import TelegramLogger telegram = TelegramLogger ( token = args . token , name = f \"HAHA\" , channel = args . channel ,) loggers . append ( telegram ) Finally, loading the HAHA dataset, running the AutoML instance, and printing the results. X_train , y_train , X_test , y_test = haha . load ( max_examples = args . examples ) classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score )","title":"Solving HAHA 2019"},{"location":"examples/tests.examples.solving_haha_2019/#solving-the-haha-challenge","text":"This script runs an instance of AutoML in the HAHA 2019 challenge . The full source code can be found here . The dataset used is: Dataset URL HAHA 2019 https://www.fing.edu.uy/inco/grupos/pln/haha/index.html#data","title":"Solving the HAHA challenge"},{"location":"examples/tests.examples.solving_haha_2019/#experimentation-parameters","text":"This experiment was run with the following parameters: Parameter Value Total epochs 1 Maximum iterations 10000 Timeout per pipeline 30 min Global timeout - Max RAM per pipeline 20 GB Population size 50 Selection (k-best) 10 Early stop - The experiments were run in the following hardware configurations (allocated indistinctively according to available resources): Config CPU Cache Memory HDD A 12 core Intel Xeon Gold 6126 19712 KB 191927.2MB 999.7GB B 6 core Intel Xeon E5-1650 v3 15360 KB 32045.5MB 2500.5GB C Quad core Intel Core i7-2600 8192 KB 15917.1MB 1480.3GB Note The hardware configuration details were extracted with inxi -CmD and summarized.","title":"Experimentation parameters"},{"location":"examples/tests.examples.solving_haha_2019/#relevant-imports","text":"Most of this example follows the same logic as the UCI example . First the necessary imports from autogoal.ml import AutoML from autogoal.datasets import haha from autogoal.search import ( PESearch , RichLogger , ) from autogoal.kb import Seq , Sentence , VectorCategorical , Supervised from autogoal.contrib import find_classes from sklearn.metrics import f1_score Next, we parse the command line arguments to configure the experiment.","title":"Relevant imports"},{"location":"examples/tests.examples.solving_haha_2019/#parsing-arguments","text":"The default values are the ones used for the experimentation reported in the paper. import argparse parser = argparse . ArgumentParser () parser . add_argument ( \"--iterations\" , type = int , default = 10000 ) parser . add_argument ( \"--timeout\" , type = int , default = 60 ) parser . add_argument ( \"--memory\" , type = int , default = 2 ) parser . add_argument ( \"--popsize\" , type = int , default = 50 ) parser . add_argument ( \"--selection\" , type = int , default = 10 ) parser . add_argument ( \"--global-timeout\" , type = int , default = None ) parser . add_argument ( \"--examples\" , type = int , default = None ) parser . add_argument ( \"--token\" , default = None ) parser . add_argument ( \"--channel\" , default = None ) args = parser . parse_args () print ( args ) The next line will print all the algorithms that AutoGOAL found in the contrib library, i.e., anything that could be potentially used to solve an AutoML problem. for cls in find_classes (): print ( \"Using: %s \" % cls . __name__ )","title":"Parsing arguments"},{"location":"examples/tests.examples.solving_haha_2019/#experimentation","text":"Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., text classification. classifier = AutoML ( search_algorithm = PESearch , input = ( Seq [ Sentence ], Supervised [ VectorCategorical ]), output = VectorCategorical , search_iterations = args . iterations , score_metric = f1_score , errors = \"warn\" , pop_size = args . popsize , search_timeout = args . global_timeout , evaluation_timeout = args . timeout , memory_limit = args . memory * 1024 ** 3 , ) loggers = [ RichLogger ()] if args . token : from autogoal.contrib.telegram import TelegramLogger telegram = TelegramLogger ( token = args . token , name = f \"HAHA\" , channel = args . channel ,) loggers . append ( telegram ) Finally, loading the HAHA dataset, running the AutoML instance, and printing the results. X_train , y_train , X_test , y_test = haha . load ( max_examples = args . examples ) classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score )","title":"Experimentation"},{"location":"examples/tests.examples.solving_meddocan_2019/","text":"Solving the MEDDOCAN challenge \u00b6 This script runs an instance of AutoML in the MEDDOCAN 2019 challenge . The full source code can be found here . Dataset URL MEDDOCAN 2019 https://github.com/PlanTL-SANIDAD/SPACCC_MEDDOCAN Experimentation parameters \u00b6 This experiment was run with the following parameters: Parameter Value Total epochs 1 Maximum iterations 10000 Timeout per pipeline 30 min Global timeout - Max RAM per pipeline 20 GB Population size 50 Selection (k-best) 10 Early stop - The experiments were run in the following hardware configurations (allocated indistinctively according to available resources): Config CPU Cache Memory HDD A 12 core Intel Xeon Gold 6126 19712 KB 191927.2MB 999.7GB B 6 core Intel Xeon E5-1650 v3 15360 KB 32045.5MB 2500.5GB C Quad core Intel Core i7-2600 8192 KB 15917.1MB 1480.3GB Note The hardware configuration details were extracted with inxi -CmD and summarized. Relevant imports \u00b6 Most of this example follows the same logic as the UCI example . First the necessary imports from autogoal.ml import AutoML from autogoal.datasets import meddocan from autogoal.search import ( RichLogger , PESearch , ) from autogoal.kb import * Parsing arguments \u00b6 Next, we parse the command line arguments to configure the experiment. The default values are the ones used for the experimentation reported in the paper. import argparse parser = argparse . ArgumentParser () parser . add_argument ( \"--iterations\" , type = int , default = 10000 ) parser . add_argument ( \"--timeout\" , type = int , default = 1800 ) parser . add_argument ( \"--memory\" , type = int , default = 20 ) parser . add_argument ( \"--popsize\" , type = int , default = 50 ) parser . add_argument ( \"--selection\" , type = int , default = 10 ) parser . add_argument ( \"--global-timeout\" , type = int , default = None ) parser . add_argument ( \"--examples\" , type = int , default = None ) parser . add_argument ( \"--token\" , default = None ) parser . add_argument ( \"--channel\" , default = None ) args = parser . parse_args () print ( args ) Experimentation \u00b6 Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., entity recognition. from autogoal.contrib import find_classes classifier = AutoML ( search_algorithm = PESearch , input = ( Seq [ Seq [ Word ]], Supervised [ Seq [ Seq [ Label ]]]), output = Seq [ Seq [ Label ]], registry = find_classes ( exclude = \"Keras|Bert\" ), search_iterations = args . iterations , score_metric = meddocan . F1_beta , cross_validation_steps = 1 , pop_size = args . popsize , search_timeout = args . global_timeout , evaluation_timeout = args . timeout , memory_limit = args . memory * 1024 ** 3 , ) Basic logging configuration. loggers = [ RichLogger ()] if args . token : from autogoal.contrib.telegram import TelegramLogger telegram = TelegramLogger ( token = args . token , name = f \"MEDDOCAN\" , channel = args . channel ,) loggers . append ( telegram ) Finally, loading the MEDDOCAN dataset, running the AutoML instance, and printing the results. X_train , y_train , X_test , y_test = meddocan . load ( max_examples = args . examples ) classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score )","title":"Solving MEDDOCAN 2019"},{"location":"examples/tests.examples.solving_meddocan_2019/#solving-the-meddocan-challenge","text":"This script runs an instance of AutoML in the MEDDOCAN 2019 challenge . The full source code can be found here . Dataset URL MEDDOCAN 2019 https://github.com/PlanTL-SANIDAD/SPACCC_MEDDOCAN","title":"Solving the MEDDOCAN challenge"},{"location":"examples/tests.examples.solving_meddocan_2019/#experimentation-parameters","text":"This experiment was run with the following parameters: Parameter Value Total epochs 1 Maximum iterations 10000 Timeout per pipeline 30 min Global timeout - Max RAM per pipeline 20 GB Population size 50 Selection (k-best) 10 Early stop - The experiments were run in the following hardware configurations (allocated indistinctively according to available resources): Config CPU Cache Memory HDD A 12 core Intel Xeon Gold 6126 19712 KB 191927.2MB 999.7GB B 6 core Intel Xeon E5-1650 v3 15360 KB 32045.5MB 2500.5GB C Quad core Intel Core i7-2600 8192 KB 15917.1MB 1480.3GB Note The hardware configuration details were extracted with inxi -CmD and summarized.","title":"Experimentation parameters"},{"location":"examples/tests.examples.solving_meddocan_2019/#relevant-imports","text":"Most of this example follows the same logic as the UCI example . First the necessary imports from autogoal.ml import AutoML from autogoal.datasets import meddocan from autogoal.search import ( RichLogger , PESearch , ) from autogoal.kb import *","title":"Relevant imports"},{"location":"examples/tests.examples.solving_meddocan_2019/#parsing-arguments","text":"Next, we parse the command line arguments to configure the experiment. The default values are the ones used for the experimentation reported in the paper. import argparse parser = argparse . ArgumentParser () parser . add_argument ( \"--iterations\" , type = int , default = 10000 ) parser . add_argument ( \"--timeout\" , type = int , default = 1800 ) parser . add_argument ( \"--memory\" , type = int , default = 20 ) parser . add_argument ( \"--popsize\" , type = int , default = 50 ) parser . add_argument ( \"--selection\" , type = int , default = 10 ) parser . add_argument ( \"--global-timeout\" , type = int , default = None ) parser . add_argument ( \"--examples\" , type = int , default = None ) parser . add_argument ( \"--token\" , default = None ) parser . add_argument ( \"--channel\" , default = None ) args = parser . parse_args () print ( args )","title":"Parsing arguments"},{"location":"examples/tests.examples.solving_meddocan_2019/#experimentation","text":"Instantiate the classifier. Note that the input and output types here are defined to match the problem statement, i.e., entity recognition. from autogoal.contrib import find_classes classifier = AutoML ( search_algorithm = PESearch , input = ( Seq [ Seq [ Word ]], Supervised [ Seq [ Seq [ Label ]]]), output = Seq [ Seq [ Label ]], registry = find_classes ( exclude = \"Keras|Bert\" ), search_iterations = args . iterations , score_metric = meddocan . F1_beta , cross_validation_steps = 1 , pop_size = args . popsize , search_timeout = args . global_timeout , evaluation_timeout = args . timeout , memory_limit = args . memory * 1024 ** 3 , ) Basic logging configuration. loggers = [ RichLogger ()] if args . token : from autogoal.contrib.telegram import TelegramLogger telegram = TelegramLogger ( token = args . token , name = f \"MEDDOCAN\" , channel = args . channel ,) loggers . append ( telegram ) Finally, loading the MEDDOCAN dataset, running the AutoML instance, and printing the results. X_train , y_train , X_test , y_test = meddocan . load ( max_examples = args . examples ) classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score )","title":"Experimentation"},{"location":"examples/tests.examples.solving_uci_datasets/","text":"Solving UCI datasets \u00b6 This script runs an instance of AutoML in anyone of the UCI datasets available. The full source code can be found here . The datasets used in this experimentation are taken from the UCI repository . Concretely, the following datasets are used: Dataset URL Abalone https://archive.ics.uci.edu/ml/datasets/Abalone Cars https://archive.ics.uci.edu/ml/datasets/Car+Evaluation Dorothea https://archive.ics.uci.edu/ml/datasets/dorothea Gisette https://archive.ics.uci.edu/ml/datasets/Gisette Shuttle https://archive.ics.uci.edu/ml/datasets/Statlog+(Shuttle) Yeast https://archive.ics.uci.edu/ml/datasets/Yeast German Credit https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data) Experimentation parameters \u00b6 This experiment was run with the following parameters: Parameter Value Total epochs 20 Maximum iterations 10000 Timeout per pipeline 5 min Global timeout 1 hour Max RAM per pipeline 10 GB Population size 100 Selection (k-best) 20 Early stop 200 iterations The experiments were run in the following hardware configurations (allocated indistinctively according to available resources): Config CPU Cache Memory HDD A 12 core Intel Xeon Gold 6126 19712 KB 191927.2MB 999.7GB B 6 core Intel Xeon E5-1650 v3 15360 KB 32045.5MB 2500.5GB C Quad core Intel Core i7-2600 8192 KB 15917.1MB 1480.3GB Note The hardware configuration details were extracted with inxi -CmD and summarized. Relevant imports \u00b6 We will need argparse for passing arguments to the script and json for serialization of results. import argparse from autogoal.kb import Supervised from autogoal.logging import logger import json From sklearn we will use train_test_split to build train and validation sets. from sklearn.model_selection import train_test_split From autogoal we need a bunch of datasets. from autogoal import datasets from autogoal.datasets import ( abalone , cars , dorothea , gisette , shuttle , yeast , german_credit , ) We will also import this annotation type. from autogoal.kb import MatrixContinuousDense , VectorCategorical This is the real deal, the class AutoML does all the work. from autogoal.ml import AutoML And from the autogoal.search module we will need a couple logging utilities and the PESearch class. from autogoal.search import ( RichLogger , MemoryLogger , PESearch , ) Parsing arguments \u00b6 Here we simply receive a bunch of arguments from the command line to decide which dataset to run and hyperparameters. They should be pretty self-explanatory. The default values are the ones used for the experimentation reported in the paper. parser = argparse . ArgumentParser () parser . add_argument ( \"--iterations\" , type = int , default = 10000 ) parser . add_argument ( \"--timeout\" , type = int , default = 300 ) parser . add_argument ( \"--memory\" , type = int , default = 10 ) parser . add_argument ( \"--popsize\" , type = int , default = 100 ) parser . add_argument ( \"--selection\" , type = int , default = 20 ) parser . add_argument ( \"--epochs\" , type = int , default = 20 ) parser . add_argument ( \"--global-timeout\" , type = int , default = 60 * 60 ) parser . add_argument ( \"--early-stop\" , type = int , default = 200 ) parser . add_argument ( \"--token\" , default = None ) parser . add_argument ( \"--channel\" , default = None ) parser . add_argument ( \"--target\" , default = 1.0 , type = float ) The most important argument is this one, which selects the dataset. valid_datasets = [ \"abalone\" , \"cars\" , \"dorothea\" , \"gisette\" , \"shuttle\" , \"yeast\" , \"german_credit\" , ] parser . add_argument ( \"--dataset\" , choices = valid_datasets + [ \"all\" ], default = \"all\" ) args = parser . parse_args () Experimentation \u00b6 Now we run all the experiments, in each of the datasets selected, for as many epochs as specified in the command line. if args . dataset != \"all\" : valid_datasets = [ args . dataset ] for epoch in range ( args . epochs ): for dataset in valid_datasets : print ( \"=============================================\" ) print ( \" Running dataset: %s - Epoch: %s \" % ( dataset , epoch )) print ( \"=============================================\" ) data = getattr ( datasets , dataset ) . load () Here we dynamically load the corresponding dataset and, if necesary, split it into training and testing sets. if len ( data ) == 4 : X_train , X_test , y_train , y_test = data else : X , y = data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 ) Finally we can instantiate out AutoML with all the custom parameters we received from the command line. classifier = AutoML ( input = ( MatrixContinuousDense , Supervised [ VectorCategorical ]), output = VectorCategorical , search_algorithm = PESearch , search_iterations = args . iterations , pop_size = args . popsize , selection = args . selection , evaluation_timeout = args . timeout , memory_limit = args . memory * 1024 ** 3 , early_stop = args . early_stop , search_timeout = args . global_timeout , target_fn = args . target , ) logger = MemoryLogger () loggers = [ RichLogger ()] TelegramLogger outputs debug information to a custom Telegram channel, if configured. if args . token : from autogoal.contrib.telegram import TelegramLogger telegram = TelegramLogger ( token = args . token , name = f \"UCI dataset=` { dataset } ` run=` { epoch } `\" , channel = args . channel , ) loggers . append ( telegram ) Finally, we run the AutoML classifier once and compute the score on an independent test-set. classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score ) print ( logger . generation_best_fn ) print ( logger . generation_mean_fn ) And store the results on a log file. with open ( \"uci_datasets.log\" , \"a\" ) as fp : fp . write ( json . dumps ( dict ( dataset = dataset , score = score , generation_best = logger . generation_best_fn , generation_mean = logger . generation_mean_fn , best_pipeline = repr ( classifier . best_pipeline_ ), search_iterations = args . iterations , pop_size = args . popsize , selection = args . selection , evaluation_timeout = args . timeout , memory_limit = args . memory * 1024 ** 3 , early_stop = args . early_stop , search_timeout = args . global_timeout , ) ) ) fp . write ( \" \\n \" )","title":"Solving UCI datasets"},{"location":"examples/tests.examples.solving_uci_datasets/#solving-uci-datasets","text":"This script runs an instance of AutoML in anyone of the UCI datasets available. The full source code can be found here . The datasets used in this experimentation are taken from the UCI repository . Concretely, the following datasets are used: Dataset URL Abalone https://archive.ics.uci.edu/ml/datasets/Abalone Cars https://archive.ics.uci.edu/ml/datasets/Car+Evaluation Dorothea https://archive.ics.uci.edu/ml/datasets/dorothea Gisette https://archive.ics.uci.edu/ml/datasets/Gisette Shuttle https://archive.ics.uci.edu/ml/datasets/Statlog+(Shuttle) Yeast https://archive.ics.uci.edu/ml/datasets/Yeast German Credit https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)","title":"Solving UCI datasets"},{"location":"examples/tests.examples.solving_uci_datasets/#experimentation-parameters","text":"This experiment was run with the following parameters: Parameter Value Total epochs 20 Maximum iterations 10000 Timeout per pipeline 5 min Global timeout 1 hour Max RAM per pipeline 10 GB Population size 100 Selection (k-best) 20 Early stop 200 iterations The experiments were run in the following hardware configurations (allocated indistinctively according to available resources): Config CPU Cache Memory HDD A 12 core Intel Xeon Gold 6126 19712 KB 191927.2MB 999.7GB B 6 core Intel Xeon E5-1650 v3 15360 KB 32045.5MB 2500.5GB C Quad core Intel Core i7-2600 8192 KB 15917.1MB 1480.3GB Note The hardware configuration details were extracted with inxi -CmD and summarized.","title":"Experimentation parameters"},{"location":"examples/tests.examples.solving_uci_datasets/#relevant-imports","text":"We will need argparse for passing arguments to the script and json for serialization of results. import argparse from autogoal.kb import Supervised from autogoal.logging import logger import json From sklearn we will use train_test_split to build train and validation sets. from sklearn.model_selection import train_test_split From autogoal we need a bunch of datasets. from autogoal import datasets from autogoal.datasets import ( abalone , cars , dorothea , gisette , shuttle , yeast , german_credit , ) We will also import this annotation type. from autogoal.kb import MatrixContinuousDense , VectorCategorical This is the real deal, the class AutoML does all the work. from autogoal.ml import AutoML And from the autogoal.search module we will need a couple logging utilities and the PESearch class. from autogoal.search import ( RichLogger , MemoryLogger , PESearch , )","title":"Relevant imports"},{"location":"examples/tests.examples.solving_uci_datasets/#parsing-arguments","text":"Here we simply receive a bunch of arguments from the command line to decide which dataset to run and hyperparameters. They should be pretty self-explanatory. The default values are the ones used for the experimentation reported in the paper. parser = argparse . ArgumentParser () parser . add_argument ( \"--iterations\" , type = int , default = 10000 ) parser . add_argument ( \"--timeout\" , type = int , default = 300 ) parser . add_argument ( \"--memory\" , type = int , default = 10 ) parser . add_argument ( \"--popsize\" , type = int , default = 100 ) parser . add_argument ( \"--selection\" , type = int , default = 20 ) parser . add_argument ( \"--epochs\" , type = int , default = 20 ) parser . add_argument ( \"--global-timeout\" , type = int , default = 60 * 60 ) parser . add_argument ( \"--early-stop\" , type = int , default = 200 ) parser . add_argument ( \"--token\" , default = None ) parser . add_argument ( \"--channel\" , default = None ) parser . add_argument ( \"--target\" , default = 1.0 , type = float ) The most important argument is this one, which selects the dataset. valid_datasets = [ \"abalone\" , \"cars\" , \"dorothea\" , \"gisette\" , \"shuttle\" , \"yeast\" , \"german_credit\" , ] parser . add_argument ( \"--dataset\" , choices = valid_datasets + [ \"all\" ], default = \"all\" ) args = parser . parse_args ()","title":"Parsing arguments"},{"location":"examples/tests.examples.solving_uci_datasets/#experimentation","text":"Now we run all the experiments, in each of the datasets selected, for as many epochs as specified in the command line. if args . dataset != \"all\" : valid_datasets = [ args . dataset ] for epoch in range ( args . epochs ): for dataset in valid_datasets : print ( \"=============================================\" ) print ( \" Running dataset: %s - Epoch: %s \" % ( dataset , epoch )) print ( \"=============================================\" ) data = getattr ( datasets , dataset ) . load () Here we dynamically load the corresponding dataset and, if necesary, split it into training and testing sets. if len ( data ) == 4 : X_train , X_test , y_train , y_test = data else : X , y = data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 ) Finally we can instantiate out AutoML with all the custom parameters we received from the command line. classifier = AutoML ( input = ( MatrixContinuousDense , Supervised [ VectorCategorical ]), output = VectorCategorical , search_algorithm = PESearch , search_iterations = args . iterations , pop_size = args . popsize , selection = args . selection , evaluation_timeout = args . timeout , memory_limit = args . memory * 1024 ** 3 , early_stop = args . early_stop , search_timeout = args . global_timeout , target_fn = args . target , ) logger = MemoryLogger () loggers = [ RichLogger ()] TelegramLogger outputs debug information to a custom Telegram channel, if configured. if args . token : from autogoal.contrib.telegram import TelegramLogger telegram = TelegramLogger ( token = args . token , name = f \"UCI dataset=` { dataset } ` run=` { epoch } `\" , channel = args . channel , ) loggers . append ( telegram ) Finally, we run the AutoML classifier once and compute the score on an independent test-set. classifier . fit ( X_train , y_train , logger = loggers ) score = classifier . score ( X_test , y_test ) print ( score ) print ( logger . generation_best_fn ) print ( logger . generation_mean_fn ) And store the results on a log file. with open ( \"uci_datasets.log\" , \"a\" ) as fp : fp . write ( json . dumps ( dict ( dataset = dataset , score = score , generation_best = logger . generation_best_fn , generation_mean = logger . generation_mean_fn , best_pipeline = repr ( classifier . best_pipeline_ ), search_iterations = args . iterations , pop_size = args . popsize , selection = args . selection , evaluation_timeout = args . timeout , memory_limit = args . memory * 1024 ** 3 , early_stop = args . early_stop , search_timeout = args . global_timeout , ) ) ) fp . write ( \" \\n \" )","title":"Experimentation"},{"location":"guide/","text":"User Guide \u00b6 AutoGOAL is a framework for the automatic generation and optimization of software pipelines. A pipeline is defined as a series of steps, which together form a program that performs some desired task. AutoGOAL was designed specifically for optimizing machine learning pipelines, a problem often called AutoML , but it can be used to optimize anything that can be defined as a set of steps with parameters. AutoGOAL has been designed to suit a broad range of users with different skill levels, from beginners to experts. Likewise, the API suits different needs, from practical use cases requiring fast iteration and out-of-the-box solutions to more involved, research-oriented use cases that require customizing and tweaking many things. Whatever your case, the following guides should help you get started. Black-Box Optimization : A black-box optimizer that can be applied to any function. Predefined Pipelines : Pre-packaged with pipelines based on popular machine learning frameworks, that you can use in few lines of code to build highly optimized machine learning pipelines for a broad range of problems. Using custom algorithms : Additionally, you can add your own implementations to the algorithm library. Class-based Pipelines : The class-based API allows you to turn any class hierarchy into an optimizable space. You define classes and annotate the constructor's parameters with attributes, and AutoGOAL automatically builds a grammar that generates all possible instances of your hierarchy. Graph-based Pipelines : The graph-based API allows you to explore spaces defined as graphs. You define a graph grammar as a set of graph rewriting rules, that take existing nodes and replace them for more complex patterns. AutoGOAL then transforms into an evaluatable object, e.g., a neural network. Functional Pipelines : If none of the previous suits you, the functional API allows you to magically turn any Python code that solves some task into an optimizable pipeline. You write a regular method and introduce AutoGOAL parameters in the code flow, which will be later automatically optimized to produce the optimal output. Don't forget to also look at the examples for more down-to-earth specific use cases.","title":"First steps"},{"location":"guide/#user-guide","text":"AutoGOAL is a framework for the automatic generation and optimization of software pipelines. A pipeline is defined as a series of steps, which together form a program that performs some desired task. AutoGOAL was designed specifically for optimizing machine learning pipelines, a problem often called AutoML , but it can be used to optimize anything that can be defined as a set of steps with parameters. AutoGOAL has been designed to suit a broad range of users with different skill levels, from beginners to experts. Likewise, the API suits different needs, from practical use cases requiring fast iteration and out-of-the-box solutions to more involved, research-oriented use cases that require customizing and tweaking many things. Whatever your case, the following guides should help you get started. Black-Box Optimization : A black-box optimizer that can be applied to any function. Predefined Pipelines : Pre-packaged with pipelines based on popular machine learning frameworks, that you can use in few lines of code to build highly optimized machine learning pipelines for a broad range of problems. Using custom algorithms : Additionally, you can add your own implementations to the algorithm library. Class-based Pipelines : The class-based API allows you to turn any class hierarchy into an optimizable space. You define classes and annotate the constructor's parameters with attributes, and AutoGOAL automatically builds a grammar that generates all possible instances of your hierarchy. Graph-based Pipelines : The graph-based API allows you to explore spaces defined as graphs. You define a graph grammar as a set of graph rewriting rules, that take existing nodes and replace them for more complex patterns. AutoGOAL then transforms into an evaluatable object, e.g., a neural network. Functional Pipelines : If none of the previous suits you, the functional API allows you to magically turn any Python code that solves some task into an optimizable pipeline. You write a regular method and introduce AutoGOAL parameters in the code flow, which will be later automatically optimized to produce the optimal output. Don't forget to also look at the examples for more down-to-earth specific use cases.","title":"User Guide"},{"location":"guide/tests.guide.automl_parameters/","text":"Ya sabemos como se utiliza la clase AutoML de la forma m\u00e1s b\u00e1sica pero esta clase cuenta con varios par\u00e1metros que nos permiten personalizar la ejecuci\u00f3n a nuestras condiciones. from autogoal.ml import AutoML Utilizando el corpus de HAHA from autogoal.datasets import haha Cargando los datos X_train , y_train , X_test , y_test = haha . load () Cargando los tipos de datos para representar el dataset from autogoal.kb import Seq , Sentence , VectorCategorical , Supervised Vemos ahora que p\u00e1rametros nuevos podemos definirle a la clase AutoML automl = AutoML ( input = ( Seq [ Sentence ], Supervised [ VectorCategorical ]), # **tipos de entrada** output = VectorCategorical , # **tipo de salida** el score_metric define la funci\u00f3n objetivo a optimizar y puede ser definida por nosotros en un m\u00e9todo propio score_metric = balanced_accuracy_score , el par\u00e1metro registry nos permite seleccionar un conjunto espec\u00edfico de algoritmo a utilizar en nuestra implementaci\u00f3n. Si no se define o se pone None se utilizan todos los algorismos disponibles en AutoGOAL. registry = None , search_algorithm permite cambiar el algoritmo de optimization que utiliza AutoGOAL, en estos moemntos tambi\u00e9n est\u00e1 implementada una b\u00fasqueda aleatoria o puedes implementar una nueva clase. search_algorithm = PESearch , search_iterations se utiliza para definir la cantidad de iteraciones que queremos que haga nuestro algoritmo de b\u00fasqueda osea cantidad de generaciones en la b\u00fasqued aevolutiva o en el random search_iterations = args . iterations , search_kwargs este par\u00e1metro se utiliza para pasar opciones adicionales al algoritmo de b\u00fasqueda search_kwargs = dict ( pop_size es el tama\u00f1o de la poblaci\u00f3n pop_size = args . popsize , search_timeout es el tiempo m\u00e1ximo total que queremos dedicarle a la b\u00fasqueda en segundos search_timeout = args . global_timeout , evaluation_timeout es el tiempo m\u00e1ximo para un pipeline, si la ejecuci\u00f3n del pipeline se pasa de este texto se detenine y se le asigna fitness cero. evaluation_timeout = args . timeout , cantidad m\u00e1xima de RAM por pipeline. Este n\u00famero debe ser inferior a la RAM del dispositivo donde se ejecute la experimentaci\u00f3n para evitar que el despositivo de bloquee. memory_limit = args . memory * 1024 ** 3 , ), cross_validation_steps cantidad de veces que se eval\u00faa cada pipeline cross_validation_steps = 3 , validation_split por ciento del tama\u00f1o del training set que se utiliza para cross validation. validation_split = 0.3 , cross_validation es la m\u00e9trica que se utiliza para mezclar los score de los cross_validation_steps. Tambi\u00e9n est\u00e1 \"mean\" cross_validation = \"median\" , random_state es un n\u00famero para fijar la semilla random de la b\u00fasqueda. Esto nos puede ayudar a que aparezcan pipelines similares a los de otra ejecuci\u00f3n. random_state = None , errors determina que se hce cuando un pipeline lanza una excepci\u00f3n. \"warn\" lanza un wargnig, \"\u00efgnore\" los ignora y \"raise\" que lanza la excepci\u00f3n y detiene la ejecuci\u00f3n. errors = \"warn\" , ) Entrenando..... automl . fit ( X_train , y_train ) Conociemdo que tan bueno es nuestro algoritmo result = automl . score ( X_test , y_test ) print ( result )","title":"Tests.guide.automl parameters"},{"location":"guide/tests.guide.cfg/","text":"Class-based API \u00b6 AutoGOAL's class-based API allows to automatically find optimal instances of complex objects in user-defined class hierarchies that solve a given task. A task is simply some method that evaluates an object's performance. The solution space is defined by a class hierarchy and all possible ways of combining instances of different types, and creating them with different parameters. Note The following code requires sklearn dependencies. Read the dependencies section for more information. For example, suppose we want to build the best possible classifier in scikit-learn for a given dataset. Let's begin with a simple classification problem. from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split We use make_classification to create a toy classification problem. X , y = make_classification ( random_state = 0 ) # Fixed seed for reproducibility One first idea is to use a specific algorithm, such as Logistic Regression, to solve this problem. Since the nature of these problems is stochastic, we need to train in one subset, test on another, and perform a sensible number of evaluations to actually know if this is any good. from sklearn.linear_model import LogisticRegression def evaluate ( estimator , iters = 30 ): scores = [] for i in range ( iters ): X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 ) estimator . fit ( X_train , y_train ) scores . append ( estimator . score ( X_test , y_test )) return sum ( scores ) / len ( scores ) lr = LogisticRegression () score = evaluate ( lr ) # around 0.83 print ( score ) # :hide: So far so good, but maybe we could do better with a different set of parameters. Logistic regression has at least two parameters that influence heavily its performance: the penalty function and the regularization strength. Instead of writing a loop through a bunch of different parameters, we can use AutoGOAL to automatically explore the space of possible combinations. We can do this with the class-based API by providing annotations for the parameters we want to explore. Trying multiple logistic regressions \u00b6 First we import some annotation types from AutoGOAL from autogoal.grammar import ContinuousValue , CategoricalValue Next we annotate the parameters we want to explore. Since we cannot modify the class LogisticRegression we will inherit from it. class LR ( LogisticRegression ): def __init__ ( self , penalty : CategoricalValue ( \"l1\" , \"l2\" ), C : ContinuousValue ( 0.1 , 10 ) ): super () . __init__ ( penalty = penalty , C = C , solver = \"liblinear\" ) The penalty: Categorical(\"l1\", \"l2\") annotation tells AutoGOAL that for this class the parameter penalty can take values from a list of predefined values. Likewise the C: Continuous(0.1, 10) annotation indicates that the parameter C can take a float value in a specified range. Now we will use AutoGOAL to automatically generate different instances of our LR class. With the class-based API we achieve this by building a context-free grammar that describes all possible instances. from autogoal.grammar import generate_cfg grammar = generate_cfg ( LR ) print ( grammar ) <LR> : = LR ( penalty = <LR_penalty>, C = <LR_C> ) <LR_penalty> : = categorical ( options =[ 'l1' , 'l2' ]) <LR_C> : = continuous ( min = 0 .1, max = 10 ) As you can see, this grammar describes the set of all possible instances of LR by describing how to call the constructor, and how to generate random values for its parameters. Note Formally, this a called a Context-free grammar . They are used in Computer Science to describe formal languages, such as programming languages, mathematical expresions, etc. Context-free grammars work by describing a set of replacement rules that you can apply recursively to construct a string of a specific language. In this case we are using grammars to describe the language of all possible Python codes that instantiates an LR . You can read more in Wikipedia . You can use this grammar to generate a bunch of random instances. for _ in range ( 5 ): print ( grammar . sample ()) LR ( C = 4 .015231900472649, penalty = 'l2' ) LR ( C = 9 .556786605505499, penalty = 'l2' ) LR ( C = 4 .05716261883461, penalty = 'l1' ) LR ( C = 3 .2786487445120858, penalty = 'l1' ) LR ( C = 4 .655510386502897, penalty = 'l2' ) Now we can search for the best combination of constructor parameters by trying a bunch of different instances and see which one obtains the best score. AutoGOAL also has tools for automating this process. from autogoal.search import RandomSearch search = RandomSearch ( grammar , evaluate , random_state = 0 ) # Fixed seed best , score = search . run ( 100 ) print ( \"Best:\" , best , \" \\n Score:\" , score ) The RandomSearch will try 100 different random instances, and for each one run the evaluate method we defined earlier. It returns the best one and the corresponding score. Best : LR ( C = 0.7043201482743121 , penalty = 'l1' ) Score : 0.8853333333333337 So we can do a little bit better by carefully selecting the right parameters. However, maybe we can do even better. Trying different algorithms \u00b6 To continue this line of thought, maybe we could do better with a different classifier. We could try decision trees, support vector machines, naive bayes, and many more. Here is the first time AutoGOAL can come to our aid. Instead of writing ourselves a loop through all the possible classes, we can do the following. First, we import everything we need. from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.naive_bayes import GaussianNB Now that we have all the classes we want to try, we have to tell AutoGOAL that there is something to optimize. We start by defining a space of possible parameters that we want to tune for each of these classes. Like with LR , we will wrap these classes in our own to provide the corresponding annotations. class SVM ( SVC ): def __init__ ( self , kernel : CategoricalValue ( \"rbf\" , \"linear\" , \"poly\" ), C : ContinuousValue ( 0.1 , 10 ), ): super () . __init__ ( C = C , kernel = kernel ) class DT ( DecisionTreeClassifier ): def __init__ ( self , criterion : CategoricalValue ( \"gini\" , \"entropy\" )): super () . __init__ ( criterion = criterion ) class NB ( GaussianNB ): def __init__ ( self , var_smoothing : ContinuousValue ( 1e-10 , 0.1 )): super () . __init__ ( var_smoothing = var_smoothing ) Next, we use AutoGOAL to construct a grammar for the union of the possible instances of each of these clases. from autogoal.grammar import Union from autogoal.grammar import generate_cfg grammar = generate_cfg ( Union ( \"Classifier\" , LR , SVM , NB , DT )) Note The method generate_cfg works not only with annotated classes but also with plain methods, or anything that has a __call__ and suitable annotations. This grammar defines all possible ways to obtain a Classifier , which is basically by instantiating one of the classes we gave it with a suitable value for each parameter. We can test it by generating a few of them. print ( grammar ) <Classifier> : = <LR> | <SVM> | <NB> | <DT> <LR> : = LR ( penalty = <LR_penalty>, C = <LR_C> ) <LR_penalty> : = categorical ( options =[ 'l1' , 'l2' ]) <LR_C> : = continuous ( min = 0 .1, max = 10 ) <SVM> : = SVM ( kernel = <SVM_kernel>, C = <SVM_C> ) <SVM_kernel> : = categorical ( options =[ 'rbf' , 'linear' , 'poly' ]) <SVM_C> : = continuous ( min = 0 .1, max = 10 ) <NB> : = NB ( var_smoothing = <NB_var_smoothing> ) <NB_var_smoothing> : = continuous ( min = 1e-10, max = 0 .1 ) <DT> : = DT ( criterion = <DT_criterion> ) <DT_criterion> : = categorical ( options =[ 'gini' , 'entropy' ]) Note The constructor for Union requires as first parameter a name so that in the grammar a suitable production can be defined. Think of it as the name of an abstract class that groups all your classes, just there is no actual type ever created, it's just for organizational purposes. for _ in range ( 5 ): print ( grammar . sample ()) NB ( var_smoothing = 0 .04620465447733762 ) DT ( criterion = 'gini' ) SVM ( C = 3 .2914771222720116, kernel = 'rbf' ) LR ( C = 7 .809744923904822, penalty = 'l1' ) DT ( criterion = 'gini' ) Now that we have a bunch of possible algorithms, let's see which one is best. search = RandomSearch ( grammar , evaluate , random_state = 0 ) best , score = search . run ( 100 ) print ( \"Best:\" , best , \" \\n Score:\" , score ) Best: NB ( var_smoothing = 0 .08450775758264377 ) Score: 0 .8840000000000003 So it doesn't really seem that we can do much better, which is unsurprising given that we are only doing a random search (there are better search methods in AutoGOAL), and this is a toy problem which basically any algorithm can solve fairly well. However, to continue with the example, now that we know how to optimize any given grammar, what is interesting is can we increase the complexity of our pipeline by adding more and more layers and steps to it, to solve more challenging problems. Adding more steps \u00b6 To illustrate how to build more complex pipelines, let's change our focus to a bit more challenging problem: sentiment analysis . We will use the ultra-know movie reviews corpus as a testbed in the next few examples. from autogoal.datasets import movie_reviews To solve sentiment analysis we need to add a step before the actual classification in order to get feature matrices from text. The simplest solution is to use a vectorizer from scikit-learn . There are two options to choose from. from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer The CountVectorizer class has many parameters that we might want to tune, but in this example we are interested only in trying different n-gram combinations. Hence, we will wrap CountVectorizer in our own Count class, and redefine its constructor to receive an ngram parameter. We annotate this parameter with :Discrete(1,3) to indicate that the possible values are integers in the interval [1,3] . from autogoal.grammar import DiscreteValue class Count ( CountVectorizer ): def __init__ ( self , ngram : DiscreteValue ( 1 , 3 )): super () . __init__ ( ngram_range = ( 1 , ngram )) self . ngram = ngram Note The reason why we store ngram in the __init__() method is for documentation purposes, so that when we call print() we get to see the actual parameters that where selected. This works automatically for parameters that are named exactly as sklearn parameters, because their __repr__ takes care, but for parameters which we introduce we need to store them in the instance so that __repr__ works. Now we will do the same with the TfIdfVectorizer class, but this time we also want to explore automatically whether enabling or disabling use_idf is better. We will use the Boolean annotation in this case. from autogoal.grammar import BooleanValue class TfIdf ( TfidfVectorizer ): def __init__ ( self , ngram : DiscreteValue ( 1 , 3 ), use_idf : BooleanValue ()): super () . __init__ ( ngram_range = ( 1 , ngram ), use_idf = use_idf ) self . ngram = ngram Besides vectorization, another common step in NLP pipelines is dimensionality reduction. For dimensionality reduction, we want to either use singular value decomposition, or nothing at all. The implementation of TruncatedSVD is suitable here because it provides a fast and scalable approximation to SVDs when dealing with spare matrices. As before, we want to parameterize the end dimension, so we will use :Discrete(50,200) , i.e., if we reduce at all, reduce between 50 and 200 dimensions. We will use the Discrete annotation in this case. from sklearn.decomposition import TruncatedSVD class SVD ( TruncatedSVD ): def __init__ ( self , n : DiscreteValue ( 50 , 200 )): super () . __init__ ( n_components = n ) self . n = n To disable dimensionality reduction in some pipelines, it's not correct to simply pass a None object. That would raise an exception. Instead, we make use of the Null Object design pattern and provide a \"no-op\" implementation that simply passes through the values. class Noop : def fit_transform ( self , X , y = None ): return X def transform ( self , X , y = None ): return X def __repr__ ( self ): return \"Noop()\" Note Technically, we could use \"passtrough\" as an argument to the Pipeline class that we will use below and achieve the same result. However, this approach is more general and clean, since it doesn't rely on the underlying API providing us with an implementation of the Null Object pattern. Now that we have all of the necessary classes with their corresponding parameters correctly annotated, it's time to put it all together into a pipeline. We will inherit from sklearn 's own implementation of Pipeline , because we want to fix the actual steps that are gonna be used. Just as before, out initializer declares the parameters. In this case, we want a vectorizer, a decomposer and a classifier. To tell autogoal to try different classes for the same parameter we use the Union annotation. Likewise, just as before, we have to call the base initializer, this time passing the corresponding configuration for an sklearn pipeline . from sklearn.pipeline import Pipeline as _Pipeline class Pipeline ( _Pipeline ): def __init__ ( self , vectorizer : Union ( \"Vectorizer\" , Count , TfIdf ), decomposer : Union ( \"Decomposer\" , Noop , SVD ), classifier : Union ( \"Classifier\" , LR , SVM , DT , NB ), ): self . vectorizer = vectorizer self . decomposer = decomposer self . classifier = classifier super () . __init__ ( [( \"vec\" , vectorizer ), ( \"dec\" , decomposer ), ( \"cls\" , classifier ),] ) Once everything is in place, we can tell autogoal to automatically infer a grammar for all the possible combinations of parameters and clases that we can use. The root of our grammar is the Pipeline class we just defined. The method generate_cfg does exactly that, taking a class and building a context free grammar to construct that class, based on the parameters' annotations and recursively building the corresponding rules for all classes down to basic parameter types. grammar = generate_cfg ( Pipeline ) Notice how the grammar specifies all the possible ways to build a Pipeline , both considering the different implementations we have for vectorizers, decomposers and classifiers; as well as their corresponding parameters. Our grammar is fairly simple because we only have two levels of recursion, Pipeline and its parameters; but this same process can be applied to any hierarchy of any complexity, including circular references. print ( grammar ) <Pipeline> : = Pipeline ( vectorizer = <Vectorizer>, decomposer = <Decomposer>, classifier = <Classifier> ) <Vectorizer> : = <Count> | <TfIdf> <Count> : = Count ( ngram = <Count_ngram> ) <Count_ngram> : = discrete ( min = 1 , max = 3 ) <TfIdf> : = TfIdf ( ngram = <TfIdf_ngram>, use_idf = <TfIdf_use_idf> ) <TfIdf_ngram> : = discrete ( min = 1 , max = 3 ) <TfIdf_use_idf> : = boolean () <Decomposer> : = <Noop> | <SVD> <Noop> : = Noop () <SVD> : = SVD ( n = <SVD_n> ) <SVD_n> : = discrete ( min = 50 , max = 200 ) <Classifier> : = <LR> | <SVM> | <DT> | <NB> <LR> : = LR ( penalty = <LR_penalty>, C = <LR_C> ) <LR_penalty> : = categorical ( options =[ 'l1' , 'l2' ]) <LR_C> : = continuous ( min = 0 .1, max = 10 ) <SVM> : = SVM ( kernel = <SVM_kernel>, C = <SVM_C> ) <SVM_kernel> : = categorical ( options =[ 'rbf' , 'linear' , 'poly' ]) <SVM_C> : = continuous ( min = 0 .1, max = 10 ) <DT> : = DT ( criterion = <DT_criterion> ) <DT_criterion> : = categorical ( options =[ 'gini' , 'entropy' ]) <NB> : = NB ( var_smoothing = <NB_var_smoothing> ) <NB_var_smoothing> : = continuous ( min = 1e-10, max = 0 .1 ) Now we can start to see the power of the class-based API. Just with a few annotations in the same classes that we anyway have to write, we automatically obtain a computational representation (a grammar) that knows how to build infinitely many of these instances. Futhermore, this works with any level of complexity, whether our classes receive simple arguments (such as integers, floats, strings) or instances of other classes, and so on. Let's take a look at how different pipelines can be generated with this grammar by sampling 10 random pipelines. for _ in range ( 10 ): print ( grammar . sample ()) You should see something like this, but your exact pipelines will be different due to random sampling. Pipeline ( classifier = SVM ( C = 4 .09762837283166, kernel = 'rbf' ) , decomposer = Noop () , vectorizer = Count ( ngram = 1 )) Pipeline ( classifier = DT ( criterion = 'entropy' ) , decomposer = Noop () , vectorizer = Count ( ngram = 2 )) Pipeline ( classifier = LR ( C = 5 .309978916527087, penalty = 'l2' ) , decomposer = Noop () , vectorizer = Count ( ngram = 3 )) Pipeline ( classifier = LR ( C = 9 .776994352626533, penalty = 'l1' ) , decomposer = Noop () , vectorizer = TfIdf ( ngram = 2 , use_idf = True )) Pipeline ( classifier = SVM ( C = 5 .973033047496386, kernel = 'rbf' ) , decomposer = SVD ( n = 197 ) , vectorizer = Count ( ngram = 3 )) Pipeline ( classifier = NB ( var_smoothing = 0 .07941925220053651 ) , decomposer = SVD ( n = 183 ) , vectorizer = TfIdf ( ngram = 3 , use_idf = False )) Pipeline ( classifier = DT ( criterion = 'entropy' ) , decomposer = SVD ( n = 144 ) , vectorizer = TfIdf ( ngram = 1 , use_idf = True )) Pipeline ( classifier = SVM ( C = 6 .052775609636756, kernel = 'poly' ) , decomposer = SVD ( n = 160 ) , vectorizer = TfIdf ( ngram = 1 , use_idf = True )) Pipeline ( classifier = DT ( criterion = 'entropy' ) , decomposer = Noop () , vectorizer = Count ( ngram = 1 )) Pipeline ( classifier = DT ( criterion = 'entropy' ) , decomposer = Noop () , vectorizer = TfIdf ( ngram = 3 , use_idf = True )) Finding the best pipeline \u00b6 To continue with the example, we will now search for the best pipeline. We will evaluate our pipelines on the movie_reviews corpus. For that purpose we need a fitness function, which is a simple callable that takes a pipeline and outputs a score. Fortunately, the movie_reviews.make_fn function does this for us, taking care of train/test splitting, fitting a pipeline in the training set and computing the accuracy on the test set. fitness_fn = movie_reviews . make_fn ( examples = 100 ) The RandomSearch strategy simply calls grammar.sample() a bunch of times and stores the best performing pipeline. It has no intelligence whatsoever, but it serves as a good baseline implementation. We will run it for a total of 1000 fitness evaluations, or equivalently, a total of 1000 different random pipelines. random_search = RandomSearch ( grammar , fitness_fn , random_state = 0 ) best , score = random_search . run ( 1000 ) Note For reproducibility purposes we can pass a fixed random seed in random_state . Final remarks \u00b6 We only used scikit-learn here for illustrative purposes, but you can apply this strategy to any problem whose solution consists of exploring a large space of complex class instances interrelated with each other. Also, in this example we have manually written wrappers for scikit-learn classes to provide the necessary annotations. However, specifically for scikit-learn , we already provide a bunch of wrappers with suitable annotations in autogoal.contrib.sklearn . We also only use RandomSearch in this example because the focus is on defining the pipelines. However, the autogoal.search namespace contains other search strategies that perform much better than plain random sampling.","title":"Class-based API"},{"location":"guide/tests.guide.cfg/#class-based-api","text":"AutoGOAL's class-based API allows to automatically find optimal instances of complex objects in user-defined class hierarchies that solve a given task. A task is simply some method that evaluates an object's performance. The solution space is defined by a class hierarchy and all possible ways of combining instances of different types, and creating them with different parameters. Note The following code requires sklearn dependencies. Read the dependencies section for more information. For example, suppose we want to build the best possible classifier in scikit-learn for a given dataset. Let's begin with a simple classification problem. from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split We use make_classification to create a toy classification problem. X , y = make_classification ( random_state = 0 ) # Fixed seed for reproducibility One first idea is to use a specific algorithm, such as Logistic Regression, to solve this problem. Since the nature of these problems is stochastic, we need to train in one subset, test on another, and perform a sensible number of evaluations to actually know if this is any good. from sklearn.linear_model import LogisticRegression def evaluate ( estimator , iters = 30 ): scores = [] for i in range ( iters ): X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 ) estimator . fit ( X_train , y_train ) scores . append ( estimator . score ( X_test , y_test )) return sum ( scores ) / len ( scores ) lr = LogisticRegression () score = evaluate ( lr ) # around 0.83 print ( score ) # :hide: So far so good, but maybe we could do better with a different set of parameters. Logistic regression has at least two parameters that influence heavily its performance: the penalty function and the regularization strength. Instead of writing a loop through a bunch of different parameters, we can use AutoGOAL to automatically explore the space of possible combinations. We can do this with the class-based API by providing annotations for the parameters we want to explore.","title":"Class-based API"},{"location":"guide/tests.guide.cfg/#trying-multiple-logistic-regressions","text":"First we import some annotation types from AutoGOAL from autogoal.grammar import ContinuousValue , CategoricalValue Next we annotate the parameters we want to explore. Since we cannot modify the class LogisticRegression we will inherit from it. class LR ( LogisticRegression ): def __init__ ( self , penalty : CategoricalValue ( \"l1\" , \"l2\" ), C : ContinuousValue ( 0.1 , 10 ) ): super () . __init__ ( penalty = penalty , C = C , solver = \"liblinear\" ) The penalty: Categorical(\"l1\", \"l2\") annotation tells AutoGOAL that for this class the parameter penalty can take values from a list of predefined values. Likewise the C: Continuous(0.1, 10) annotation indicates that the parameter C can take a float value in a specified range. Now we will use AutoGOAL to automatically generate different instances of our LR class. With the class-based API we achieve this by building a context-free grammar that describes all possible instances. from autogoal.grammar import generate_cfg grammar = generate_cfg ( LR ) print ( grammar ) <LR> : = LR ( penalty = <LR_penalty>, C = <LR_C> ) <LR_penalty> : = categorical ( options =[ 'l1' , 'l2' ]) <LR_C> : = continuous ( min = 0 .1, max = 10 ) As you can see, this grammar describes the set of all possible instances of LR by describing how to call the constructor, and how to generate random values for its parameters. Note Formally, this a called a Context-free grammar . They are used in Computer Science to describe formal languages, such as programming languages, mathematical expresions, etc. Context-free grammars work by describing a set of replacement rules that you can apply recursively to construct a string of a specific language. In this case we are using grammars to describe the language of all possible Python codes that instantiates an LR . You can read more in Wikipedia . You can use this grammar to generate a bunch of random instances. for _ in range ( 5 ): print ( grammar . sample ()) LR ( C = 4 .015231900472649, penalty = 'l2' ) LR ( C = 9 .556786605505499, penalty = 'l2' ) LR ( C = 4 .05716261883461, penalty = 'l1' ) LR ( C = 3 .2786487445120858, penalty = 'l1' ) LR ( C = 4 .655510386502897, penalty = 'l2' ) Now we can search for the best combination of constructor parameters by trying a bunch of different instances and see which one obtains the best score. AutoGOAL also has tools for automating this process. from autogoal.search import RandomSearch search = RandomSearch ( grammar , evaluate , random_state = 0 ) # Fixed seed best , score = search . run ( 100 ) print ( \"Best:\" , best , \" \\n Score:\" , score ) The RandomSearch will try 100 different random instances, and for each one run the evaluate method we defined earlier. It returns the best one and the corresponding score. Best : LR ( C = 0.7043201482743121 , penalty = 'l1' ) Score : 0.8853333333333337 So we can do a little bit better by carefully selecting the right parameters. However, maybe we can do even better.","title":"Trying multiple logistic regressions"},{"location":"guide/tests.guide.cfg/#trying-different-algorithms","text":"To continue this line of thought, maybe we could do better with a different classifier. We could try decision trees, support vector machines, naive bayes, and many more. Here is the first time AutoGOAL can come to our aid. Instead of writing ourselves a loop through all the possible classes, we can do the following. First, we import everything we need. from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.naive_bayes import GaussianNB Now that we have all the classes we want to try, we have to tell AutoGOAL that there is something to optimize. We start by defining a space of possible parameters that we want to tune for each of these classes. Like with LR , we will wrap these classes in our own to provide the corresponding annotations. class SVM ( SVC ): def __init__ ( self , kernel : CategoricalValue ( \"rbf\" , \"linear\" , \"poly\" ), C : ContinuousValue ( 0.1 , 10 ), ): super () . __init__ ( C = C , kernel = kernel ) class DT ( DecisionTreeClassifier ): def __init__ ( self , criterion : CategoricalValue ( \"gini\" , \"entropy\" )): super () . __init__ ( criterion = criterion ) class NB ( GaussianNB ): def __init__ ( self , var_smoothing : ContinuousValue ( 1e-10 , 0.1 )): super () . __init__ ( var_smoothing = var_smoothing ) Next, we use AutoGOAL to construct a grammar for the union of the possible instances of each of these clases. from autogoal.grammar import Union from autogoal.grammar import generate_cfg grammar = generate_cfg ( Union ( \"Classifier\" , LR , SVM , NB , DT )) Note The method generate_cfg works not only with annotated classes but also with plain methods, or anything that has a __call__ and suitable annotations. This grammar defines all possible ways to obtain a Classifier , which is basically by instantiating one of the classes we gave it with a suitable value for each parameter. We can test it by generating a few of them. print ( grammar ) <Classifier> : = <LR> | <SVM> | <NB> | <DT> <LR> : = LR ( penalty = <LR_penalty>, C = <LR_C> ) <LR_penalty> : = categorical ( options =[ 'l1' , 'l2' ]) <LR_C> : = continuous ( min = 0 .1, max = 10 ) <SVM> : = SVM ( kernel = <SVM_kernel>, C = <SVM_C> ) <SVM_kernel> : = categorical ( options =[ 'rbf' , 'linear' , 'poly' ]) <SVM_C> : = continuous ( min = 0 .1, max = 10 ) <NB> : = NB ( var_smoothing = <NB_var_smoothing> ) <NB_var_smoothing> : = continuous ( min = 1e-10, max = 0 .1 ) <DT> : = DT ( criterion = <DT_criterion> ) <DT_criterion> : = categorical ( options =[ 'gini' , 'entropy' ]) Note The constructor for Union requires as first parameter a name so that in the grammar a suitable production can be defined. Think of it as the name of an abstract class that groups all your classes, just there is no actual type ever created, it's just for organizational purposes. for _ in range ( 5 ): print ( grammar . sample ()) NB ( var_smoothing = 0 .04620465447733762 ) DT ( criterion = 'gini' ) SVM ( C = 3 .2914771222720116, kernel = 'rbf' ) LR ( C = 7 .809744923904822, penalty = 'l1' ) DT ( criterion = 'gini' ) Now that we have a bunch of possible algorithms, let's see which one is best. search = RandomSearch ( grammar , evaluate , random_state = 0 ) best , score = search . run ( 100 ) print ( \"Best:\" , best , \" \\n Score:\" , score ) Best: NB ( var_smoothing = 0 .08450775758264377 ) Score: 0 .8840000000000003 So it doesn't really seem that we can do much better, which is unsurprising given that we are only doing a random search (there are better search methods in AutoGOAL), and this is a toy problem which basically any algorithm can solve fairly well. However, to continue with the example, now that we know how to optimize any given grammar, what is interesting is can we increase the complexity of our pipeline by adding more and more layers and steps to it, to solve more challenging problems.","title":"Trying different algorithms"},{"location":"guide/tests.guide.cfg/#adding-more-steps","text":"To illustrate how to build more complex pipelines, let's change our focus to a bit more challenging problem: sentiment analysis . We will use the ultra-know movie reviews corpus as a testbed in the next few examples. from autogoal.datasets import movie_reviews To solve sentiment analysis we need to add a step before the actual classification in order to get feature matrices from text. The simplest solution is to use a vectorizer from scikit-learn . There are two options to choose from. from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer The CountVectorizer class has many parameters that we might want to tune, but in this example we are interested only in trying different n-gram combinations. Hence, we will wrap CountVectorizer in our own Count class, and redefine its constructor to receive an ngram parameter. We annotate this parameter with :Discrete(1,3) to indicate that the possible values are integers in the interval [1,3] . from autogoal.grammar import DiscreteValue class Count ( CountVectorizer ): def __init__ ( self , ngram : DiscreteValue ( 1 , 3 )): super () . __init__ ( ngram_range = ( 1 , ngram )) self . ngram = ngram Note The reason why we store ngram in the __init__() method is for documentation purposes, so that when we call print() we get to see the actual parameters that where selected. This works automatically for parameters that are named exactly as sklearn parameters, because their __repr__ takes care, but for parameters which we introduce we need to store them in the instance so that __repr__ works. Now we will do the same with the TfIdfVectorizer class, but this time we also want to explore automatically whether enabling or disabling use_idf is better. We will use the Boolean annotation in this case. from autogoal.grammar import BooleanValue class TfIdf ( TfidfVectorizer ): def __init__ ( self , ngram : DiscreteValue ( 1 , 3 ), use_idf : BooleanValue ()): super () . __init__ ( ngram_range = ( 1 , ngram ), use_idf = use_idf ) self . ngram = ngram Besides vectorization, another common step in NLP pipelines is dimensionality reduction. For dimensionality reduction, we want to either use singular value decomposition, or nothing at all. The implementation of TruncatedSVD is suitable here because it provides a fast and scalable approximation to SVDs when dealing with spare matrices. As before, we want to parameterize the end dimension, so we will use :Discrete(50,200) , i.e., if we reduce at all, reduce between 50 and 200 dimensions. We will use the Discrete annotation in this case. from sklearn.decomposition import TruncatedSVD class SVD ( TruncatedSVD ): def __init__ ( self , n : DiscreteValue ( 50 , 200 )): super () . __init__ ( n_components = n ) self . n = n To disable dimensionality reduction in some pipelines, it's not correct to simply pass a None object. That would raise an exception. Instead, we make use of the Null Object design pattern and provide a \"no-op\" implementation that simply passes through the values. class Noop : def fit_transform ( self , X , y = None ): return X def transform ( self , X , y = None ): return X def __repr__ ( self ): return \"Noop()\" Note Technically, we could use \"passtrough\" as an argument to the Pipeline class that we will use below and achieve the same result. However, this approach is more general and clean, since it doesn't rely on the underlying API providing us with an implementation of the Null Object pattern. Now that we have all of the necessary classes with their corresponding parameters correctly annotated, it's time to put it all together into a pipeline. We will inherit from sklearn 's own implementation of Pipeline , because we want to fix the actual steps that are gonna be used. Just as before, out initializer declares the parameters. In this case, we want a vectorizer, a decomposer and a classifier. To tell autogoal to try different classes for the same parameter we use the Union annotation. Likewise, just as before, we have to call the base initializer, this time passing the corresponding configuration for an sklearn pipeline . from sklearn.pipeline import Pipeline as _Pipeline class Pipeline ( _Pipeline ): def __init__ ( self , vectorizer : Union ( \"Vectorizer\" , Count , TfIdf ), decomposer : Union ( \"Decomposer\" , Noop , SVD ), classifier : Union ( \"Classifier\" , LR , SVM , DT , NB ), ): self . vectorizer = vectorizer self . decomposer = decomposer self . classifier = classifier super () . __init__ ( [( \"vec\" , vectorizer ), ( \"dec\" , decomposer ), ( \"cls\" , classifier ),] ) Once everything is in place, we can tell autogoal to automatically infer a grammar for all the possible combinations of parameters and clases that we can use. The root of our grammar is the Pipeline class we just defined. The method generate_cfg does exactly that, taking a class and building a context free grammar to construct that class, based on the parameters' annotations and recursively building the corresponding rules for all classes down to basic parameter types. grammar = generate_cfg ( Pipeline ) Notice how the grammar specifies all the possible ways to build a Pipeline , both considering the different implementations we have for vectorizers, decomposers and classifiers; as well as their corresponding parameters. Our grammar is fairly simple because we only have two levels of recursion, Pipeline and its parameters; but this same process can be applied to any hierarchy of any complexity, including circular references. print ( grammar ) <Pipeline> : = Pipeline ( vectorizer = <Vectorizer>, decomposer = <Decomposer>, classifier = <Classifier> ) <Vectorizer> : = <Count> | <TfIdf> <Count> : = Count ( ngram = <Count_ngram> ) <Count_ngram> : = discrete ( min = 1 , max = 3 ) <TfIdf> : = TfIdf ( ngram = <TfIdf_ngram>, use_idf = <TfIdf_use_idf> ) <TfIdf_ngram> : = discrete ( min = 1 , max = 3 ) <TfIdf_use_idf> : = boolean () <Decomposer> : = <Noop> | <SVD> <Noop> : = Noop () <SVD> : = SVD ( n = <SVD_n> ) <SVD_n> : = discrete ( min = 50 , max = 200 ) <Classifier> : = <LR> | <SVM> | <DT> | <NB> <LR> : = LR ( penalty = <LR_penalty>, C = <LR_C> ) <LR_penalty> : = categorical ( options =[ 'l1' , 'l2' ]) <LR_C> : = continuous ( min = 0 .1, max = 10 ) <SVM> : = SVM ( kernel = <SVM_kernel>, C = <SVM_C> ) <SVM_kernel> : = categorical ( options =[ 'rbf' , 'linear' , 'poly' ]) <SVM_C> : = continuous ( min = 0 .1, max = 10 ) <DT> : = DT ( criterion = <DT_criterion> ) <DT_criterion> : = categorical ( options =[ 'gini' , 'entropy' ]) <NB> : = NB ( var_smoothing = <NB_var_smoothing> ) <NB_var_smoothing> : = continuous ( min = 1e-10, max = 0 .1 ) Now we can start to see the power of the class-based API. Just with a few annotations in the same classes that we anyway have to write, we automatically obtain a computational representation (a grammar) that knows how to build infinitely many of these instances. Futhermore, this works with any level of complexity, whether our classes receive simple arguments (such as integers, floats, strings) or instances of other classes, and so on. Let's take a look at how different pipelines can be generated with this grammar by sampling 10 random pipelines. for _ in range ( 10 ): print ( grammar . sample ()) You should see something like this, but your exact pipelines will be different due to random sampling. Pipeline ( classifier = SVM ( C = 4 .09762837283166, kernel = 'rbf' ) , decomposer = Noop () , vectorizer = Count ( ngram = 1 )) Pipeline ( classifier = DT ( criterion = 'entropy' ) , decomposer = Noop () , vectorizer = Count ( ngram = 2 )) Pipeline ( classifier = LR ( C = 5 .309978916527087, penalty = 'l2' ) , decomposer = Noop () , vectorizer = Count ( ngram = 3 )) Pipeline ( classifier = LR ( C = 9 .776994352626533, penalty = 'l1' ) , decomposer = Noop () , vectorizer = TfIdf ( ngram = 2 , use_idf = True )) Pipeline ( classifier = SVM ( C = 5 .973033047496386, kernel = 'rbf' ) , decomposer = SVD ( n = 197 ) , vectorizer = Count ( ngram = 3 )) Pipeline ( classifier = NB ( var_smoothing = 0 .07941925220053651 ) , decomposer = SVD ( n = 183 ) , vectorizer = TfIdf ( ngram = 3 , use_idf = False )) Pipeline ( classifier = DT ( criterion = 'entropy' ) , decomposer = SVD ( n = 144 ) , vectorizer = TfIdf ( ngram = 1 , use_idf = True )) Pipeline ( classifier = SVM ( C = 6 .052775609636756, kernel = 'poly' ) , decomposer = SVD ( n = 160 ) , vectorizer = TfIdf ( ngram = 1 , use_idf = True )) Pipeline ( classifier = DT ( criterion = 'entropy' ) , decomposer = Noop () , vectorizer = Count ( ngram = 1 )) Pipeline ( classifier = DT ( criterion = 'entropy' ) , decomposer = Noop () , vectorizer = TfIdf ( ngram = 3 , use_idf = True ))","title":"Adding more steps"},{"location":"guide/tests.guide.cfg/#finding-the-best-pipeline","text":"To continue with the example, we will now search for the best pipeline. We will evaluate our pipelines on the movie_reviews corpus. For that purpose we need a fitness function, which is a simple callable that takes a pipeline and outputs a score. Fortunately, the movie_reviews.make_fn function does this for us, taking care of train/test splitting, fitting a pipeline in the training set and computing the accuracy on the test set. fitness_fn = movie_reviews . make_fn ( examples = 100 ) The RandomSearch strategy simply calls grammar.sample() a bunch of times and stores the best performing pipeline. It has no intelligence whatsoever, but it serves as a good baseline implementation. We will run it for a total of 1000 fitness evaluations, or equivalently, a total of 1000 different random pipelines. random_search = RandomSearch ( grammar , fitness_fn , random_state = 0 ) best , score = random_search . run ( 1000 ) Note For reproducibility purposes we can pass a fixed random seed in random_state .","title":"Finding the best pipeline"},{"location":"guide/tests.guide.cfg/#final-remarks","text":"We only used scikit-learn here for illustrative purposes, but you can apply this strategy to any problem whose solution consists of exploring a large space of complex class instances interrelated with each other. Also, in this example we have manually written wrappers for scikit-learn classes to provide the necessary annotations. However, specifically for scikit-learn , we already provide a bunch of wrappers with suitable annotations in autogoal.contrib.sklearn . We also only use RandomSearch in this example because the focus is on defining the pipelines. However, the autogoal.search namespace contains other search strategies that perform much better than plain random sampling.","title":"Final remarks"},{"location":"guide/tests.guide.functional/","text":"Functional API \u00b6 AutoGOAL's functional API allows you to transform any Python callable (e.g., a method) into an optimizable target. In contrast with the class-based and graph-based APIs, the functional API does not require to know before-hand the structure of the space you want to optimize. This enables very flexible use cases, in which you can iterate quickly, experiment, and transform deterministic code to solve one particular task into optimizable software that seems to magically solve the problem for you in the best possible way. Let's start with a toy example just to show the basic usage of the API. from autogoal.sampling import Sampler def generate ( sampler : Sampler ): x1 = sampler . continuous ( 0 , 1 , \"x1\" ) x2 = sampler . continuous ( 0 , 1 , \"x2\" ) if x1 > x2 : return ( x1 , x2 ) return ( 0 , 0 ) def fn ( t ): x1 , x2 = t return x1 * x2 search = PESearch ( generate , fn ) best , y = search . run ( 1000 ) print ( search . _model ) print ( best , y )","title":"Functional API"},{"location":"guide/tests.guide.functional/#functional-api","text":"AutoGOAL's functional API allows you to transform any Python callable (e.g., a method) into an optimizable target. In contrast with the class-based and graph-based APIs, the functional API does not require to know before-hand the structure of the space you want to optimize. This enables very flexible use cases, in which you can iterate quickly, experiment, and transform deterministic code to solve one particular task into optimizable software that seems to magically solve the problem for you in the best possible way. Let's start with a toy example just to show the basic usage of the API. from autogoal.sampling import Sampler def generate ( sampler : Sampler ): x1 = sampler . continuous ( 0 , 1 , \"x1\" ) x2 = sampler . continuous ( 0 , 1 , \"x2\" ) if x1 > x2 : return ( x1 , x2 ) return ( 0 , 0 ) def fn ( t ): x1 , x2 = t return x1 * x2 search = PESearch ( generate , fn ) best , y = search . run ( 1000 ) print ( search . _model ) print ( best , y )","title":"Functional API"},{"location":"guide/tests.guide.graph/","text":"Graph-based API \u00b6 Note This section is under construction.","title":"Graph-based API"},{"location":"guide/tests.guide.graph/#graph-based-api","text":"Note This section is under construction.","title":"Graph-based API"},{"location":"guide/tests.guide.include_algorithm/","text":"\u00bfQu\u00e9 podemos hacer cuando queremos incluir un nuevo algoritmo a AutoGOAL y que este se integre de forma natural con todos los otros para poder formar parte de posibles pipelines. Primero tenemos que importar la clase de la que debemos heredar para que se reaproveche todo el sistema de conectar pipelines de AutoGOAL from autogoal.kb import AlgorithmBase from autogoal.grammar import BooleanValue , DiscreteValue from autogoal.kb import * Tenemos que crear una clase que representa a nuestro nuevo algoritmo Supongamos que queremos un Algoritmo que puede conviertir o no a min\u00fasculas y eliminar las palabras muy cortas. Al definirlas de esta forma le estamos pidiendo a AutoGOAL que pruebe a hacerlas o no en diferentes pipelines, teniendo e cuenta adem\u00e1s distintos valores. class NewAlgorithm ( AlgorithmBase ): En el constructor tenemos que poner los hiperpar\u00e1mtros que son optimizables def __init__ ( self , min_length tomar\u00e1 valores entre cero y cinco de forma autom\u00e1tica para diferentes pipelines. Este par\u00e1metro est\u00e1 permitiendo buscar distintos tama\u00f1os de palabra y probar cual de ellos ser\u00e1 mejor min_length : DiscreteValue ( min = 0 , max = 5 ), lower es un par\u00e1metro que en algunos casos ser\u00e1 True y en otros False. Podemos utilizarlo para llevar o no a min\u00fasculas el texto. lower : BooleanValue (), ): self . min_length = min_length self . lower = lower El m\u00e9todo run es el m\u00e9todo que tienen en com\u00fan todo los algoritmos incluidos en AutoGOAL es el que permite a la biblioteca ejecutar el m\u00e9todo. Este m\u00e9todo define tanto la entrada como la salida de tu algoritmo, utilizando anotaciones de los tipo sem\u00e1nticos de AutoGOAL. Adem\u00e1s es quien contiene el funcionamiento real de nuestro algoritmo, es el c\u00f3digo que se ejecutar\u00e1. Digamos que recibe una oraci\u00f3n (Sentence) y devuelve una lista de palabras (Seq[Word]) def run ( self , input : Sentence ) -> Seq [ Word ]: Como podemos ver la implementaci\u00f3n del m\u00e9todo se realiza bastante independiente de la biblioteca if self . lower : input = input . lower () result = [] for i in input . split (): if len ( i ) > self . min_length : result . append ( i ) return result Una vez que tenemos listo nuestro algoritmo solo nos queda indicarle a la clase AutoML que lo utilice en la b\u00fasqueda Estos son algunos import que nos hacen falta m\u00e1s adelante from autogoal.ml import AutoML from autogoal.contrib import find_classes Probemos con HAHA from autogoal.datasets import haha Cargando los datos X_train , y_train , X_test , y_test = haha . load () Creando la instancia de AutoML con nuestra clase automl = AutoML ( input = ( Seq [ Sentence ], Supervised [ VectorCategorical ]), # **tipos de entrada** output = VectorCategorical , # **tipo de salida** Agregando nuestra clase y todo el resto de algortimos de AutoGOAL registry = [ NewAlgorithm ] + find_classes (), ) Ahora sencillamente tenemos que ejecutar AutoML y ya nuestro algoritmo aparecer\u00e1 en algunos pipelines. Debemos tener en cuenta que esto no garantiza qeu aparezca en el mejor pipeline encontrado, sino que se conectar\u00e1 con el resto de los algoritmo como si fuera nativo de AutoGOAL. automl . fit ( X_train , y_train ) score = automl . score ( X_test , y_test ) print ( score )","title":"Using custom algorithms"},{"location":"guide/tests.guide.predefined/","text":"Auto GOAL permite a los investigadores y profesionales desarrollar ra\u0301pidamente algoritmos de referencia optimizados en diversos problemas de aprendizaje automa\u0301tico. Para utilizar AutoGOAL de la forma m\u00e1s sencilla posible es necesario definir 3 componentes: - Entrada - Salida - M\u00e9trica a optimizar (Funci\u00f3n objetivo) Que se le definen a la clase AutoML El primer paso es importar la clase AutoML from autogoal.ml import AutoML El segundo paso es tener un dataset representado en alguno de los tipos definidos en AutoGOAL. Por ejemplo en este caso utilizaremos HAHA un corpus de mensajes de Twitter en espa\u00f1ol que queremos clasificar en humor\u00edsticos o no. from autogoal.datasets import haha Cargando los datos X_train , y_train , X_test , y_test = haha . load () Como tenemos que representarlo seg\u00fan los tipos definidos en AutoGOAL Vamos a importar los tipos que nos hacen falta para HAHA En este caso podemos verlo como un listado (que se representa con el tipo Seq de Sequence) de oraciones (que se representa con el tipo Sentence) , ya que los mensajes son muy cortos. De las que conocemos (al menos para una parte para modelar como supervisado con el tipo Supervised) la categor\u00eda de humor (esta categor\u00eda podemos representarla con el tipo VectorCategorical). Definir el tipo de entrada como las oraciones + las clases supervisadas de las mismas, le deja claro a AutoGOAL que queremos resolver el problema de forma supervisada con un entrenamiento. from autogoal.kb import Seq , Sentence , VectorCategorical , Supervised \u00bfC\u00f3mo utilizamos esto en la clase AutoML? automl = AutoML ( input = ( Seq [ Sentence ], Supervised [ VectorCategorical ]), # **tipos de entrada** output = VectorCategorical , # **tipo de salida** tenemos el par\u00e1metro score_metric para definir la funci\u00f3n objetivo, que si no le fijamos un valor utiliza por defecto la funci\u00f3n autogoal.ml.metrics.accuracy . ) Ya hasta aqu\u00ed hemos definido el problema que queremos resolver ahora solo nos resta ejecutar nuestro algoritmo, llamando al m\u00e9todo fit . Para monitorear el estado del proceso de AutoML, podemos pasar un logger al m\u00e9todo fit . from autogoal.search import RichLogger Entrenando... automl . fit ( X_train , y_train , logger = RichLogger ()) Conociemdo que tan bueno es nuestro algoritmo score = automl . score ( X_test , y_test ) print ( f \"Score: { score : 0.3f } \" ) Esto significa que nuestro algoritmo el mejor pipeline que encontr\u00f3 report\u00f3 un accuracy \"result\" Tambi\u00e9n puede llamarse al m\u00e9todo predict que nos hace la predicci\u00f3n para un conjunto de ejemplos Prediciendo... predictions = automl . predict ( X_test ) for sentence , real , predicted in zip ( X_test [: 10 ], y_test , predictions ): print ( sentence , \"-->\" , real , \"vs\" , predicted )","title":"Predefined pipelines"}]}